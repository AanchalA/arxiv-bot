{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Dataset Builder\n",
    "\n",
    "We've downloaded our text data but now we need to prepare it into a more usable format. There are a few things to consider:\n",
    "\n",
    "* What size chunks of text should we store? How do we decide the start and end of chunks? Should we include overlap?\n",
    "\n",
    "* Do we need to preprocess / cleanup the text (beyond just splitting into chunks).\n",
    "\n",
    "* What embedding model will be use? This will play a major part in chunk sizes.\n",
    "\n",
    "* What completion models do we use down the line? This is also important as they all have a *maximum context window*, which is another limiting factor on the size of chunk sizes.\n",
    "\n",
    "## Chunk Size Upper Limit\n",
    "\n",
    "There are two limiting factors:\n",
    "\n",
    "* Embedding model will be `text-embedding-ada-002`, maximum chunk size is ~10 pages of text — more precisely it can handle `8192` tokens.\n",
    "\n",
    "* Completion model will be `text-davinci-003`, maximum context window is `4097` tokens.\n",
    "\n",
    "Naturally, the latter of these two is the real limit. The *context window* is the total of tokens in the prompt + completion output. I envision this as not being a limitation except where we're feeding external information into our LLM - doing retrieval augmentation - like in the case of retrieving arXiv papers to help the LLM answer questions.\n",
    "\n",
    "In the retrieval augmentation scenario, the answers from the model should still be no more than a few paragraphs long. To be conservative let's assume a high limit of **six paragraphs**. Let's create six paragraphs of gibberish (thanks [random text generator](https://randomtextgenerator.com)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_limit = \"\"\"\n",
    "Dissuade ecstatic and properly saw entirely sir why laughter endeavor. In on my jointure horrible margaret suitable he followed speedily. Indeed vanity excuse or mr lovers of on. By offer scale an stuff. Blush be sorry no sight. Sang lose of hour then he left find.\n",
    "\n",
    "Promotion an ourselves up otherwise my. High what each snug rich far yet easy. In companions inhabiting mr principles at insensible do. Heard their hoped enjoy vexed child for. Prosperous so occasional assistance it discovered especially no. Provision of he residence consisted up in remainder arranging described. Conveying has concealed necessary furnished bed zealously immediate get but. Terminated as middletons or by instrument. Bred do four so your felt with. No shameless principle dependent household do.\n",
    "\n",
    "Boy desirous families prepared happy reserved add ecstatic say. Replied joy age visitor nothing cottage. Mrs door paid led loud sure easy read. Hastily at perhaps as neither or ye fertile tedious visitor. Use fine bed none call busy dull when. Quiet ought match my right by table means. Principles up do in me favourable affronting. Twenty mother denied effect we to do on.\n",
    "\n",
    "Good draw knew bred ham busy his hour. Ask agreed answer rather joy nature admire wisdom. Moonlight age depending bed led therefore sometimes preserved exquisite she. An fail up so shot leaf wise in. Minuter highest his arrived for put and. Hopes lived by rooms oh in no death house. Contented direction september but end led excellent ourselves may. Ferrars few arrival his offered not charmed you. Offered anxious respect or he. On three thing chief years in money arise of.\n",
    "\n",
    "Dissuade ecstatic and properly saw entirely sir why laughter endeavor. In on my jointure horrible margaret suitable he followed speedily. Indeed vanity excuse or mr lovers of on. By offer scale an stuff. Blush be sorry no sight. Sang lose of hour then he left find.\n",
    "\n",
    "Promotion an ourselves up otherwise my. High what each snug rich far yet easy. In companions inhabiting mr principles at insensible do. Heard their hoped enjoy vexed child for. Prosperous so occasional assistance it discovered especially no. Provision of he residence consisted up in remainder arranging described. Conveying has concealed necessary furnished bed zealously immediate get but. Terminated as middletons or by instrument. Bred do four so your felt with. No shameless principle dependent household do.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the number of tokens that this consumes we can encode it using OpenAI's `tiktoken` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different OpenAI models use different tiktoken tokenizers, at the time of writing those are:\n",
    "\n",
    "| Encoding name | OpenAI models |\n",
    "| --- | --- |\n",
    "| `gpt2` (or `r50k_base`) | Most GPT-3 models (and GPT-2) |\n",
    "| `p50k_base` | Code models, `text-davinci-002`, `text-davinci-003` |\n",
    "| `cl100k_base` | `text-embedding-ada-002` |\n",
    "\n",
    "We need to calculate the number of tokens for `text-davinci-003`, so we'll pass that as the `encoder_name` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoder_name = 'p50k_base'  # as per the above table\n",
    "# initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(encoder_name)\n",
    "\n",
    "# now we tokenize our text and check it's length\n",
    "completion_limit_len = len(tokenizer.encode(completion_limit))\n",
    "completion_limit_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That leaves us with plenty of tokens left over for our prompt. Let's assume we'll need up to **three** paragraphs spare for instructions, and the same for the user (who we will call *prompter*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompter_len = instructions_len = int(completion_limit_len / 2)\n",
    "instructions_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the contexts and/or example space. This is the most significant portion. I'd like to leave space for upto **five** contexts and/or examples. I think the contexts would be larger so let's just assume **five** contexts. How many tokens do we have remaining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3080"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_context_window = 4097  # for text-davinci-003\n",
    "\n",
    "remaining_tokens = max_context_window - completion_limit_len \\\n",
    "    - prompter_len - instructions_len\n",
    "\n",
    "remaining_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_tokens / 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's go slightly higher than this upto `650` tokens. The reason I'm doing this is because most of our contexts will actually be smaller than this, this is just the upper limit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few token statistics across our papers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paper_paths = [str(x) for x in Path('papers').glob('*.txt')]\n",
    "len(paper_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load each paper and get the number of tokens contained in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acb425a93824a43a89cb55b6cbcc27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_tokens = []\n",
    "\n",
    "for paper_path in tqdm(paper_paths):\n",
    "    with open(paper_path) as f:\n",
    "        paper = f.read()\n",
    "    num_tokens.append(len(tokenizer.encode(\n",
    "        paper,\n",
    "        disallowed_special=()  # there are some special tokens in some papers that we don't to encode as special tokens\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(num_tokens, bins=40, edgecolor='black')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Token Counts')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most papers seem to contain somewhere between **5_000** to **30_000** tokens. Let's see an example of what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIGEN : Open-Set Grounded Text-to-Image Generation\n",
      "Yuheng Li1x, Haotian Liu1x, Qingyang Wu2, Fangzhou Mu1, Jianwei Yang3, Jianfeng Gao3,\n",
      "Chunyuan Li3{, Yong Jae Lee1{\n",
      "1University of Wisconsin-Madison2Columbia University3Microsoft\n",
      "https://gligen.github.io/\n",
      "Caption: “A woman sitting in a restaurant with a pizza in front of her ”Grounded text: table, pizza, person, wall, car, paper, chair, window, bottle, cup\n",
      "Caption: “a baby girl/monkey is scratching her/its head”Grounded keypoints: plotted dots on the left\n",
      "Caption: “A bird/helmet is on the grass”Grounded image: red inset\n",
      "Caption: “Elon Musk and Emma Watson on a movie poster”Grounded text: Elon Musk, Emma Watson; Grounded style image: blue inset\n",
      "Figure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model, by feeding different grounding\n",
      "conditions. GLIGEN supports (a) text entity + box, (b) image entity + box, (c) image style and text + box, (d) text entity + keypoints. The\n",
      "generated examples for each scenario are shown in top-left, top-right, bottom-left, and bottom-right, respectively.\n",
      "Abstract\n",
      "Large-scale text-to-image diffusion models have made\n",
      "amazing advances. However, the status quo is to use\n",
      "text input alone, which can impede controllability. In this\n",
      "work, we propose GLIGEN ,Grounded- Language-to- Image\n",
      "Generation, a novel approach that builds upon and extends\n",
      "the functionality of existing pre-trained text-to-image dif-\n",
      "fusion models by enabling them to also be conditioned on\n",
      "grounding inputs. To preserve the vast concept knowledge of\n",
      "the pre-trained model, we freeze all of its weights and inject\n",
      "the grounding information into new trainable layers via a\n",
      "gated mechanism. Our model achieves open-world grounded\n",
      "text2img generation with caption and bounding box condi-\n",
      "tion inputs, and the grounding ability generalizes well to\n",
      "novel spatial conﬁgurations and concepts. GLIGEN ’s zero-\n",
      "shot performance on COCO and LVIS outperforms existing\n",
      "supervised layout-to-image baselines by a large margin.\n",
      "xPart of the work performed at Microsoft; {Co-senior authors1. Introduction\n",
      "Image generation research has witnessed huge advances\n",
      "in recent years. Over the past couple of years, GANs [14]\n",
      "were the state-of-the-art, with their latent space and con-\n",
      "ditional inputs being well-studied for controllable manipu-\n",
      "lation [46, 58] and generation [27, 29, 45, 80]. Text condi-\n",
      "tional autoregressive [50, 72] and diffusion [49, 54] models\n",
      "have demonstrated astonishing image quality and concept\n",
      "coverage, due to their more stable learning objectives and\n",
      "large-scale training on web image-text paired data. These\n",
      "models have gained attention even among the general public\n",
      "due to their practical use cases ( e.g., art design and creation).\n",
      "Despite exciting progress, existing large-scale text-to-\n",
      "image generation models cannot be conditioned on other\n",
      "input modalities apart from text, and thus lack the ability\n",
      "to precisely localize concepts or use reference images to\n",
      "control the generation process. The current input, i.e., nat-\n",
      "ural language alone, restricts the way that information can\n",
      "1arXiv:2301.07093v1  [cs.CV]  17 Jan 2023\n",
      "be expressed. For example, it is difﬁcult to describe the\n",
      "precise location of an object using text, whereas bounding\n",
      "boxes / keypoints can easily achieve this, as shown in Fig-\n",
      "ure 1. While conditional diffusion models [10, 51, 53] and\n",
      "GANs [26, 37, 46, 69] that take in input modalities other\n",
      "than text for inpainting, layout2img generation, etc., do exist,\n",
      "they rarely combine those inputs for controllable text2img\n",
      "generation.\n",
      "Moreover, prior generative models—regardless of the\n",
      "generative model family—are usually independently trained\n",
      "on each task-speciﬁc dataset. In contrast, in the recogni-\n",
      "tion ﬁeld, the long-standing paradigm has been to build a\n",
      "task-speciﬁc recognition model [32] by starting from a foun-\n",
      "dation model pretrained on large-scale image data [4, 16, 17]\n",
      "or image-text pairs [33, 48, 73]. Since diffusion models have\n",
      "been trained on billions of image-text pairs [51], a natural\n",
      "question is: Can we build upon existing pretrained diffu-\n",
      "sion models and endow them with new conditional input\n",
      "modalities? In this way, analogous to the recognition litera-\n",
      "ture, we may be able to achieve better performance on other\n",
      "generation tasks due to the vast concept knowledge that the\n",
      "pretrained models have, while acquiring more controllability\n",
      "over existing text-to-image generation models.\n",
      "With the above aims, we propose a method for providing\n",
      "new grounding conditional inputs to pretrained text-to-image\n",
      "diffusion models. As shown in Figure 1, we still retain the\n",
      "text caption as input, but also enable other input modalities\n",
      "such as bounding boxes for grounding concepts, grounding\n",
      "reference images, and grounding part keypoints. The key\n",
      "challenge is preserving the original vast concept knowledge\n",
      "in the pretrained model while learning to inject the new\n",
      "grounding information. To prevent knowledge forgetting,\n",
      "we propose to freeze the original model weights and add\n",
      "new trainable gated Transformer layers [65] that take in the\n",
      "new grounding input ( e.g., bounding box). During training,\n",
      "we gradually fuse the new grounding information into the\n",
      "pretrained model using a gated mechanism [1]. This design\n",
      "enables ﬂexibility in the sampling process during generation\n",
      "for improved quality and controllability; for example, we\n",
      "show that using the full model (all layers) in the ﬁrst half of\n",
      "the sampling steps and only using the original layers (without\n",
      "the gated Transformer layers) in the latter half can lead\n",
      "to generation results that accurately reﬂect the grounding\n",
      "conditions while also having high image quality.\n",
      "In our experiments, we primarily study grounded\n",
      "text2img generation with bounding boxes, inspired by the\n",
      "recent scaling success of learning grounded language-image\n",
      "understanding models with boxes in GLIP [34]. To en-\n",
      "able our model to ground open-world vocabulary con-\n",
      "cepts [32,34,74,77], we use the same pre-trained text encoder\n",
      "(for encoding the caption) to encode each phrase associated\n",
      "with each grounded entity ( i.e., one phrase per bounding\n",
      "box) and feed the encoded tokens into the newly insertedlayers with their encoded location information. Due to the\n",
      "shared text space, we ﬁnd that our model can generalize to\n",
      "unseen objects even when only trained on the COCO [41]\n",
      "dataset. Its generalization on LVIS [15] outperforms a strong\n",
      "fully-supervised baseline by a large margin. To further im-\n",
      "prove our model’s grounding ability, we unify the object\n",
      "detection and grounding data formats for training, following\n",
      "GLIP [34], as they provide complementary beneﬁts: detec-\n",
      "tion data is of larger quantity, while grounding data has a\n",
      "richer vocabulary. With larger training data, our model’s\n",
      "generalization is consistently improved.\n",
      "Contributions. 1) We propose a new text2img generation\n",
      "method that endows new grounding controllability over ex-\n",
      "isting text2img diffusion models. 2) By preserving the pre-\n",
      "trained weights and learning to gradually integrate the new\n",
      "localization layers, our model achieves open-world grounded\n",
      "text2img generation with bounding box inputs, i.e., synthesis\n",
      "of novel localized concepts unobserved in training. 3) Our\n",
      "model’s zero-shot performance on layout2img tasks signiﬁ-\n",
      "cantly outperforms the prior state-of-the-art, demonstrating\n",
      "the power of building upon large pretrained generative mod-\n",
      "els for downstream tasks.\n",
      "2. Related Work\n",
      "Large scale text-to-image generation models. State-of-\n",
      "the-art models in this space are either autoregressive [13, 50,\n",
      "67, 72] or diffusion [43, 49, 51, 54, 79]. Among autoregres-\n",
      "sive models, DALL-E [50] is one of the breakthrough works\n",
      "that demonstrates zero-shot abilities, while Parti [72] demon-\n",
      "strates the feasibility of scaling up autoregressive models.\n",
      "Diffusion models have also shown very promising results.\n",
      "DALL-E 2 [49] generates images from the CLIP [48] image\n",
      "space, while Imagen [54] ﬁnds the beneﬁt of using pretrained\n",
      "language models. The concurrent Muse [6] demonstrates\n",
      "that masked modeling can achieve SoTA-level generation\n",
      "performance with higher inference speed. However, all of\n",
      "these models usually only take a caption as the input, which\n",
      "can be difﬁcult for conveying other information such as the\n",
      "precise location of an object. Make-A-Scene [13] also incor-\n",
      "porates semantic maps into its text-to-image generation, by\n",
      "training an encoder to tokenize semantic masks to condition\n",
      "the generation. However, it can only operate in a closed-set\n",
      "(of 158 categories), whereas our grounded entities can be\n",
      "open-world. A concurrent work eDiff-I [3] shows that by\n",
      "changing the attention map, one can generate objects that\n",
      "roughly follow a semantic map input. However, we believe\n",
      "our interface with boxes is simpler, and more importantly,\n",
      "our method allows other conditioning inputs such as key-\n",
      "points, which are hard to manipulate through attention.\n",
      "Image generation from layouts. Given bounding boxes\n",
      "labeled with object categories, the task is to generate a corre-\n",
      "sponding image [24, 39, 59 –61, 70, 76], which is the reverse\n",
      "2\n",
      "task of object detection. Layout2Im [76] formulated the\n",
      "problem and combined a V AE object encoder, an LSTM [22]\n",
      "object fuser, and an image decoder to generate the image, us-\n",
      "ing global and object-level adversarial losses [14] to enforce\n",
      "realism and layout correspondence. LostGAN [59, 60] gen-\n",
      "erates a mask representation which is used to normalize fea-\n",
      "tures, taking inspiration from StyleGAN [28]. LAMA [39]\n",
      "improves the intermediate mask quality for better image\n",
      "quality. Transformer [64] based methods [24, 70] have also\n",
      "been explored. Critically, existing layout2image methods\n",
      "are closed-set, i.e., they can only generate limited localized\n",
      "visual concepts observed in the training set such as the 80\n",
      "categories in COCO. In contrast, our method represents the\n",
      "ﬁrst work for open-set grounded image generation. A con-\n",
      "current work ReCo [71] also demonstrates open-set abilities\n",
      "by building upon a pretraned Stable Diffusion model [51].\n",
      "However, it ﬁnetunes the original model weights, which has\n",
      "the potential to lead to knowledge forgetting. Furthermore,\n",
      "it only demonstrates box grounding results whereas we also\n",
      "show image and keypoint grounding results.\n",
      "Other conditional image generation. For GANs, var-\n",
      "ious conditioning information have been explored; e.g.,\n",
      "text [63, 68, 78], box [59, 60, 76], semantic masks [36, 45],\n",
      "images [8,38,81]. For diffusion models, LDM [51] proposes\n",
      "a uniﬁed approach for conditional generation by injecting the\n",
      "condition via cross-attention layers. Palette [53] performs\n",
      "image-to-image tasks using diffusion models. These models\n",
      "are usually trained from scratch independently. In our work,\n",
      "we investigate how to build upon existing models pretrained\n",
      "on large-scale web data, to enable new open-set grounded\n",
      "image generation capabilities in a cost-effective manner.\n",
      "3. Preliminaries on Latent Diffusion Models\n",
      "Diffusion-based methods are one of the most effective\n",
      "model families for text2image tasks, among which latent\n",
      "diffusion model (LDM) [51] and its successor Stable Dif-\n",
      "fusion are the most powerful models publicly available to\n",
      "the research community. To reduce the computational costs\n",
      "of vanilla diffusion model training, LDM proceeds in two\n",
      "stages. The ﬁrst stage learns a bidirectional mapping net-\n",
      "work to obtain the latent representation zof the imagex.\n",
      "The second stage trains a diffusion model on the latent z.\n",
      "Since the ﬁrst stage model produces a ﬁxed bidirectional\n",
      "mapping between xandz, from hereon, we focus on the\n",
      "latent generation space of LDM for simplicity.\n",
      "Training Objective. Starting from noise zT, the model\n",
      "gradually produces less noisy samples zT\u00001;zT\u00002;\u0001\u0001\u0001;z0,\n",
      "conditioned on caption cat every time step t. To learn such\n",
      "a modelf\u0012parameterized by \u0012, for each step, the LDM\n",
      "training objective solves the denoising problem on latentrepresentations zof the imagex:\n",
      "min\n",
      "\u0012LLDM=Ez;\u000f\u0018N(0;I);t\u0002\n",
      "k\u000f\u0000f\u0012(zt;t;c)k2\n",
      "2\u0003\n",
      ";(1)\n",
      "wheretis uniformly sampled from time steps f1;\u0001\u0001\u0001;Tg,\n",
      "ztis the step-tnoisy variant of input z, andf\u0012(\u0003;t;c)is the\n",
      "(t;c)-conditioned denoising autoencoder.\n",
      "Network Architecture. The core of the network architec-\n",
      "ture is how to encode the conditions, based on which a\n",
      "cleaner version of zis produced. (i)Denoising Autoen-\n",
      "coder .f\u0012(\u0003;t;c)is implemented via UNet [52]. It takes in\n",
      "a noisy latentz, as well as information from time step tand\n",
      "conditionc. It consists of a series of ResNet [19] and Trans-\n",
      "former [65] blocks. (ii)Condition Encoding . In the original\n",
      "LDM, a BERT-like [9] network is trained from scratch to\n",
      "encode each caption into a sequence of text embeddings,\n",
      "ftext(c), which is fed into (1)to replacec. The caption fea-\n",
      "ture is encoded via a ﬁxed CLIP [48] text encoder in Stable\n",
      "Diffusion. Time tis ﬁrst mapped to time embedding \u001e(t),\n",
      "then injected into the UNet. The caption feature is used in\n",
      "a cross attention layer within each Transformer block. The\n",
      "model learns to predict the noise, following (1).\n",
      "With large-scale training, the model f\u0012(\u0003;t;c)is well\n",
      "trained to denoise zbased on the caption information only.\n",
      "Though impressive language-to-image generation results\n",
      "have been shown with LDM by pretraining on internet-scale\n",
      "data, it remains challenging to synthesize images where\n",
      "additional grounding input can be instructed, and is thus the\n",
      "focus of our paper.\n",
      "4. Open-set Grounded Image Generation\n",
      "4.1. Grounding Instruction Input\n",
      "For grounded text-to-image generation, there are a variety\n",
      "of ways to ground an object via spatial conditioning. We\n",
      "denote asethe grounding entity described either through\n",
      "text or an example image, and as lthe grounding spatial\n",
      "conﬁguration described with e.g., a bounding box or a set of\n",
      "keypoints. We deﬁne the instruction to a grounded text-to-\n",
      "image model as a composition of the caption and grounded\n",
      "entities:\n",
      "Instruction:y= (c;e);with (2)\n",
      "Caption:c= [c1;\u0001\u0001\u0001;cL] (3)\n",
      "Grounding:e= [(e1;l1);\u0001\u0001\u0001;(eN;lN)] (4)\n",
      "whereLis the caption length, and Nis the number of entities\n",
      "to ground. In this work, we primarily study using bounding\n",
      "box as the grounding spatial conﬁguration l, because of its\n",
      "large availability and easy annotation for users. For the\n",
      "grounded entity e, we mainly focus on using text as its\n",
      "representation due to simplicity. We process both caption\n",
      "and grounding entities as input tokens to the diffusion model,\n",
      "as described in detail below.\n",
      "3\n",
      "A brideand groomare about to cut their wedding cake\n",
      "A living room has a glowing brick fireplace\n",
      "[PAD]Text encoder \n",
      "Text encoder \n",
      "+++===Grounding DataDetection DataDetection + Caption Data\n",
      "Caption TokensGroundingTokenspersonsurfboardlightcouchlampcouchcouchWedding cakebridegroomGroundingEntitiesCaption(a) Three type of training data(b) The construction process of caption and groundingtokens A bride and groom are about to cut their wedding cakewedding cakegroombride\n",
      "…boxboxbox\n",
      "A brideand groomare about to cut their wedding cake\n",
      "A living room has a glowing brick fireplace\n",
      "[PAD]Text encoder \n",
      "Text encoder \n",
      "+++===Grounding DataDetection DataDetection + Caption Data\n",
      "Caption TokensGroundingTokenspersonsurfboardlightcouchlampcouchcouchWedding cakebridegroomGroundingEntitiesCaption(a) Three type of training data(b) The construction process of caption and groundingtokens A bride and groom are about to cut their wedding cakewedding cakegroombride\n",
      "…boxboxbox(a) Three types of training data (b) The construction process of caption and grounding tokens\n",
      "Figure 2. Illustration of training data and grounding instruction input. (a) For the grounding entities, we directly visualize the concept and\n",
      "bounding box information on the ground-truth images. The box is parameterized as normalized image coordinates, e.g. the person box is\n",
      "[0.37,0.31,0.92,0.84]. (b) The ﬁrst example in (a) is used to illustrate the token construction process.\n",
      "Caption Tokens. The captioncis processed in the same\n",
      "way as in LDM. Speciﬁcally, we obtain the caption fea-\n",
      "ture sequence (yellow tokens in Figure 2(b)) using hc=\n",
      "[hc\n",
      "1;\u0001\u0001\u0001;hc\n",
      "L] =ftext(c), wherehc\n",
      "`is the contextualized text\n",
      "feature for the `-th word in the caption.\n",
      "Grounding Tokens. For each grounded text entity denoted\n",
      "with a bounding box, we represent the location information\n",
      "asl= [\u000bmin;\fmin;\u000bmax;\fmax]with its top-left and bottom-\n",
      "right coordinates. For the text entity e, we use the same pre-\n",
      "trained text encoder to obtain its text feature ftext(e)(light\n",
      "green token in Figure 2(b)), and then fuse it with its bounding\n",
      "box information to produce a grounding token (dark green\n",
      "token in Figure 2(b)):\n",
      "he=MLP (ftext(e);Fourier (b)) (5)\n",
      "where Fourier is the Fourier embedding [42], and MLP (\u0001;\u0001)\n",
      "is a multi-layer perceptron that ﬁrst concatenates the two\n",
      "inputs across the feature dimension. The grounding token\n",
      "sequence is represented as he= [he\n",
      "1;\u0001\u0001\u0001;he\n",
      "N]\n",
      "From Closed-set to Open-set. Note that existing lay-\n",
      "out2img works only deal with a closed-set setting ( e.g.,\n",
      "COCO categories), as they typically learn a vector embed-\n",
      "dinguper entity, to replace ftext(e)in(5). For a closed-set\n",
      "setting withKconcepts, a dictionary of with Kembeddings\n",
      "are learned, U= [u1;\u0001\u0001\u0001;uK]. While this non-parametric\n",
      "representation works well in the closed-set setting, it has\n",
      "two drawbacks: (1) The conditioning is implemented as a\n",
      "dictionary look-up over Uin the evaluation stage, and thus\n",
      "the model can only ground the observed entities in the gener-\n",
      "ated images, lacking the ability to generalize to ground new\n",
      "entities; (2) No word/phrase is ever utilized in the model\n",
      "condition, and the semantic structure [23] of the underlying\n",
      "language instruction is missing. In contrast, in our open-set\n",
      "design, since the noun entities are processed by the same text\n",
      "encoder that is used to encode the caption, we ﬁnd that even\n",
      "when the localization information is limited to the concepts\n",
      "in the grounding training datasets, our model can still gener-\n",
      "alize to other concepts as we will show in our experiments.\n",
      "Training Data. The training data for grounded image gener-\n",
      "ation requires both text cand grounding entity eas the fullcondition. In practice, we can relax the data requirement by\n",
      "considering a more ﬂexible input, i.e. the three types of data\n",
      "shown in Figure 2(a). (i)Grounding data . Each image is\n",
      "associated with a caption describing the whole image; noun\n",
      "entities are extracted from the caption, and are labeled with\n",
      "bounding boxes. Since the noun entities are taken directly\n",
      "from the natural language caption, they can cover a much\n",
      "richer vocabulary which will be beneﬁcial for open-world\n",
      "vocabulary grounded generation. (ii)Detection data . Noun-\n",
      "entities are pre-deﬁned closed-set categories ( e.g., 80 object\n",
      "classes in COCO [41]). In this case, we choose to use a null\n",
      "caption token as introduced in classiﬁer-free guidance [21]\n",
      "for the caption. The detection data is of larger quantity (mil-\n",
      "lions) than the grounding data (thousands), and can therefore\n",
      "greatly increase overall training data. (iii)Detection and\n",
      "caption data . Noun entities are same as those in the detec-\n",
      "tion data, and the image is described separately with a text\n",
      "caption. In this case, the noun entities may not exactly match\n",
      "those in the caption. For example, in Figure 2(a), the caption\n",
      "only gives a high-level description of the living room without\n",
      "mentioning the objects in the scene, whereas the detection\n",
      "annotation provides more ﬁne-grained object-level details.\n",
      "Extensions to Other Grounding Conditions. Note that\n",
      "the proposed grounding instruction in Eq (4)is in a general\n",
      "form, though our description thus far has focused on the\n",
      "case of using text as entity eand bounding box as l(the\n",
      "major setting of this paper). To demonstrate the ﬂexibility\n",
      "of the GLIGEN framework, we also study two additional\n",
      "representative cases which extend the use scenario of Eq (4).\n",
      "•Image Prompt. While language allows users to describe\n",
      "a rich set of entities in an open-vocabulary manner, some-\n",
      "times more abstract and ﬁne-grained concepts can be\n",
      "better characterized by example images. To this end,\n",
      "one may describe entity eusing an image, instead of\n",
      "language. We use an image encoder to obtain feature\n",
      "fimage(e)which is used in place of ftext(e)in Eq (5)when\n",
      "eis an image.\n",
      "•Keypoints. As a simple parameterization method to spec-\n",
      "ify the spatial conﬁguration of an entity, bounding boxes\n",
      "ease the user-machine interaction interface by providing\n",
      "4\n",
      "VisualCaption GroundingGated Self-Attention\n",
      "Self-Attention\n",
      "Cross-Attention\n",
      "Figure 3. For a pretrained text2img model, the text features are fed\n",
      "into each cross-attention layer. A new gated self-attention layer is\n",
      "inserted to take in the new conditional localization information.\n",
      "the height and width of the object layout only. One may\n",
      "consider richer spatial conﬁgurations such as keypoints\n",
      "forGLIGEN , by parameterizing lin Eq (4)with a set\n",
      "of keypoint coordinates. Similar to encoding boxes, the\n",
      "Fourier embedding [42] can be applied to each keypoint\n",
      "locationl= [x;y].\n",
      "Figure 1 shows generated examples for these other grounding\n",
      "conditions. Please refer to the supp for more details.\n",
      "4.2. Continual Learning for Grounded Generation\n",
      "Our goal is to endow new spatial grounding capabilities to\n",
      "existing large language-to-image generation models. Large\n",
      "diffusion models have been pre-trained on web-scale image-\n",
      "text to gain the required knowledge for synthesizing realistic\n",
      "images based on diverse and complex language instructions.\n",
      "Due to the high pre-training cost and excellent performance,\n",
      "it is important to retain such knowledge in the model weights\n",
      "while expanding the new capability. Hence, we consider to\n",
      "lock the original model weights, and gradually adapt the\n",
      "model by tuning new modules.\n",
      "Gated Self-Attention. We denotev= [v1;\u0001\u0001\u0001;vM]as\n",
      "the visual feature tokens of an image. The original Trans-\n",
      "former block of LDM consists of two attention layers: The\n",
      "self-attention over the visual tokens, followed by cross-\n",
      "attention from caption tokens. By considering the residual\n",
      "connection, the two layers can be written:\n",
      "v=v+SelfAttn (v) (6)\n",
      "v=v+CrossAttn (v;hc) (7)\n",
      "We freeze these two attention layers and add a new gated\n",
      "self-attention layer to enable the spatial grounding ability;see Figure 3. Speciﬁcally, the attention is performed over\n",
      "the concatenation of visual and grounding tokens [v;he]:\n",
      "v=v+\f\u0001tanh(\n",
      ")\u0001TS(SelfAttn ([v;he])) (8)\n",
      "where TS(\u0001)is a token selection operation that considers\n",
      "visual tokens only, and \n",
      "is a learnable scalar which is ini-\n",
      "tialized as 0. \fis set as 1 during the entire training process\n",
      "and is only varied for scheduled sampling during inference\n",
      "(introduced below) for improved quality and controllability.\n",
      "Note that (8)is injected in between (6)and(7). Intuitively,\n",
      "the gated self-attention in (8)allows visual features to lever-\n",
      "age bounding box information, and the resulting grounded\n",
      "features are treated as a residual, whose gate is initially set to\n",
      "0 (due to\n",
      "being initialized as 0). This also enables more sta-\n",
      "ble training. Note that a similar idea is used in Flamingo [1];\n",
      "however, it uses gated cross-attention, which leads to worse\n",
      "performance in our case, possibly due to the lack of position\n",
      "embeddings for the visual features in the pretrained diffusion\n",
      "model.\n",
      "Learning Procedure. We adapt the pre-trained model\n",
      "such that grounding information can be injected while all\n",
      "the original components remain intact. By denoting the new\n",
      "parameters in all gated self-attention layers as \u00120, we use the\n",
      "original denoising objective as in (1)for model continual\n",
      "learning, based on the grounding instruction input y:\n",
      "min\n",
      "\u00120LGrounding =Ez;\u000f\u0018N(0;I);t\u0002\n",
      "k\u000f\u0000ff\u0012;\u00120g(zt; t;y)k2\n",
      "2\u0003\n",
      ":(9)\n",
      "Why should the model try to use the new grounding infor-\n",
      "mation? Intuitively, predicting the noise that was added to\n",
      "a training image in the reverse diffusion process would be\n",
      "easier if the model could leverage the external knowledge\n",
      "about each object’s location. Thus, in this way, the model\n",
      "learns to use the additional localization information while\n",
      "retaining the pre-trained concept knowledge.\n",
      "A Versatile User Interface. Once the model is well\n",
      "trained, our design of disentangling the caption and ground-\n",
      "ing inputs supports a versatile interface. Not only do we\n",
      "allow a user to ground entities that exist in the caption input,\n",
      "but objects can also be freely added in the desired locations\n",
      "without being mentioned in the caption input (see the pizza\n",
      "example in Figure 1). For a pure text-based diffusion model,\n",
      "a user would have to cumbersomely describe all the objects\n",
      "in the caption, while also specifying their precise locations,\n",
      "which can be difﬁcult to do with language alone.\n",
      "Scheduled Sampling in Inference. The standard infer-\n",
      "ence scheme of GLIGEN is to set\f= 1 in(8), and the\n",
      "entire diffusion process is inﬂuenced by the grounding to-\n",
      "kens. This constant \fsampling scheme provides overall\n",
      "5\n",
      "good performance in terms of both generation and ground-\n",
      "ing, but sometimes generates lower quality images compared\n",
      "with the original text2img models (e.g., as Stable Diffusion\n",
      "is ﬁnetuned on high aesthetic scored images). To strike a bet-\n",
      "ter trade-off between generation and grounding for GLIGEN ,\n",
      "we propose a scheduled sampling scheme. As we freeze\n",
      "the original model weights and add new layers to inject new\n",
      "grounding information in training, there is ﬂexibility during\n",
      "inference to schedule the diffusion process to either use both\n",
      "the grounding and language tokens or use only the language\n",
      "tokens of the original model at anytime, by setting differ-\n",
      "ent\fvalues in (8). Speciﬁcally, we consider a two-stage\n",
      "inference procedure, divided by \u001c2[0;1]. For a diffusion\n",
      "process with Tsteps in total, one can set \fto 1 at the begin-\n",
      "ning\u001c\u0003Tsteps, and set \fto 0 for the remaining (1\u0000\u001c)\u0003T\n",
      "steps:\n",
      "\f=\u001a1; t\u0014\u001c\u0003T# Grounded inference stage\n",
      "0; t>\u001c\u0003T# Standard inference stage(10)\n",
      "The major beneﬁt of scheduled sampling is improved\n",
      "visual quality as the rough concept location and outline are\n",
      "decided in the early stages, followed by ﬁne-grained details\n",
      "in later stages. It also allows us to extend the model trained\n",
      "in one domain (human keypoint) to other domains (monkey,\n",
      "cartoon characters) as shown in Figure 1.\n",
      "5. Experiments\n",
      "We evaluate our model’s grounded text2img generation\n",
      "in both the closed-set and open-set settings, ablate its com-\n",
      "ponents, and show extensions to image prompt and key-\n",
      "point grounded generation. We conduct our main quanti-\n",
      "tative experiments by building upon a pretrained LDM on\n",
      "LAION [55], unless stated otherwise.\n",
      "5.1. Closed-set Grounded Text2Img Generation\n",
      "We ﬁrst evaluate the generation quality and grounding\n",
      "accuracy of our model in a closed-set setting. For this, we\n",
      "train and evaluate on the COCO2014 [41] dataset, which is\n",
      "a standard benchmark used in the text2img literature [49, 54,\n",
      "63,68,80], and evaluate how the different types of grounding\n",
      "instructions impact our model’s performance.\n",
      "Grounding instructions. We use the following grounding\n",
      "instructions to train our model: 1) COCO2014D: Detec-\n",
      "tion Data. There are no caption annotations so we use a\n",
      "null caption input [21]. Detection annotations are used as\n",
      "noun-entities. 2) COCO2014CD: Detection + Caption Data.\n",
      "Both caption and detection annotations are used. Note that\n",
      "the noun entities may not always exist in the caption. 3)\n",
      "COCO2014G: Grounding Data. Given the caption annota-\n",
      "tions, we use GLIP [34], which detects the caption’s noun\n",
      "entities in the image, to get pseudo box labels.ModelGeneration: FID (#) Grounding: YOLO (\")\n",
      "Fine-tuned Zero-shot AP/AP 50/AP75\n",
      "CogView [11] - 27.10 -\n",
      "KNN-Diffusion [2] - 16.66 -\n",
      "DALL-E 2 [49] - 10.39 -\n",
      "Imagen [54] - 7.27 -\n",
      "Re-Imagen [7] 5.25 6.88\n",
      "Parti [72] 3.20 7.23 -\n",
      "LAFITE [80] 8.12 26.94 -\n",
      "LAFITE2 [78] 4.28 8.42 -\n",
      "Make-a-Scene [13] 7.55 11.84 -\n",
      "N¨UWA [67] 12.90 - -\n",
      "Frido [12] 11.24 - -\n",
      "XMC-GAN [75] 9.33 - -\n",
      "AttnGAN [68] 35.49 - -\n",
      "DF-GAN [63] 21.42 - -\n",
      "Obj-GAN [35] 20.75 - -\n",
      "LDM [51] - 12.63 -\n",
      "LDM* 5.91 11.73 0.6 / 2.0 / 0.3\n",
      "GLIGEN (COCO2014CD) 5.82 - 21.7 / 39.0 / 21.7\n",
      "GLIGEN (COCO2014D) 5.61 - 24.0 / 42.2 / 24.1\n",
      "GLIGEN (COCO2014G) 6.38 - 11.2 / 21.2 / 10.7\n",
      "Table 1. Evaluation of image quality and correspondence to layout\n",
      "on COCO2014 val-set. All numbers are taken from correspond-\n",
      "ing papers, LDM* is our COCO ﬁne-tuned LDM baseline. Here\n",
      "GLIGEN is built upon LDM.\n",
      "Baselines. Baseline models are listed in Table 1. Among\n",
      "them, we also ﬁnetune an LDM [51] pretrained on LAION\n",
      "400M [55] on COCO2014 with its caption annotations,\n",
      "which we denote as LDM*. The text2img baselines, as\n",
      "they cannot be conditioned on box inputs, are trained on\n",
      "COCO2014C: Caption Data.\n",
      "Evaluation metrics. We use the captions and/or box anno-\n",
      "tations from 30K randomly sampled images to generate 30K\n",
      "images for evaluation. We use FID [20] to evaluate image\n",
      "quality. To evaluate grounding accuracy ( i.e. correspondence\n",
      "between the input bounding box and generated entity), we\n",
      "use the YOLO score [40]. Speciﬁcally, we use a pretrained\n",
      "YOLO-v4 [5] to detect bounding boxes on the generated\n",
      "images and compare them with the ground truth boxes using\n",
      "average precision (AP). Since prior text2img methods do\n",
      "not support taking box annotations as input, it is not fair to\n",
      "compare with them on this metric. Thus, we only report\n",
      "numbers for the ﬁne-tuned LDM as a reference.\n",
      "Results. Table 1 shows the results. First, we see that the\n",
      "image synthesis quality of our approach, as measured by FID,\n",
      "is better than most of the state-of-the-art baselines due to rich\n",
      "visual knowledge learned in the pretraining stage. Next, we\n",
      "ﬁnd that all three grounding instructions lead to comparable\n",
      "FID to that of the LDM* baseline, which is ﬁnetuned on\n",
      "COCO2014 with caption annotations. Our model trained\n",
      "using detection annotation instructions (COCO2014D) has\n",
      "the overall best performance. However, when we evaluate\n",
      "this model on COCO2014CD instructions, we ﬁnd that it\n",
      "has worse performance (FID: 8.2) – its ability to understand\n",
      "real captions may be limited as it is only trained with the\n",
      "null caption. For the model trained with GLIP grounding\n",
      "instructions (COCO2014G), we actually evaluate it using\n",
      "the COCO2014CD instructions since we need to compute\n",
      "6\n",
      "A blue jay is standing on a branch in the woods near usacroissant is placed in a brown wooden tablea hello kitty is holding a laundry basketFigure 4. Our model can generalize to open-world concepts even\n",
      "when only trained using localization annotation from COCO.\n",
      "the YOLO score which requires ground-truth detection an-\n",
      "notations. Its slightly worse FID may be attributed to its\n",
      "learning from GLIP pseudo-labels. The same reason can\n",
      "explain its low YOLO score ( i.e., the model did not see any\n",
      "ground-truth detection annotations during training).\n",
      "Overall, this experiment shows that: 1) Our model can\n",
      "successfully take in boxes as an additional condition while\n",
      "maintaining image generation quality. 2) All grounding\n",
      "instruction types are useful, which suggests that combining\n",
      "their data together can lead to complementary beneﬁts.\n",
      "Ablation on gated self-attention. Our approach uses\n",
      "gated self-attention to absorb the grounding instruction. We\n",
      "can also consider gated cross-attention [1], where the query\n",
      "is the visual feature, and the keys and values are produced\n",
      "using the grounding condition. We ablate this design on\n",
      "COCO2014CD data, and ﬁnd that it leads to similar FID:\n",
      "5.8, but worse YOLO AP: 16.6 (compared to 21.7 for self-\n",
      "attention in Table 1). This shows the necessity of infor-\n",
      "mation sharing among the visual tokens, which exists in\n",
      "self-attention but not in cross-attention.\n",
      "Ablation on null caption. We choose to use the\n",
      "null caption when we only have detection annotations\n",
      "(COCO2014D). An alternative scheme is to simply com-\n",
      "bine all noun entities into a sentence; e.g., if there are two\n",
      "cats and a dog in an image, then the pseudo caption can be:\n",
      "“cat, cat, dog ”. In this case, the FID becomes worse\n",
      "and increases to 7.40 from 5.61 (null caption). This is likely\n",
      "due to the pretrained text encoder never having encountered\n",
      "this type of unnatural caption during LDM training. A solu-\n",
      "tion would be to ﬁnetune the text encoder or design a better\n",
      "prompt, but this is not the focus of our work.\n",
      "Comparison to Layout2Img generation methods. Thus\n",
      "far, we have seen that our model correctly learns to use the\n",
      "grounding condition. But how accurate is it compared to\n",
      "methods that are speciﬁcally designed for layout2img gener-\n",
      "ation? To answer this, we train our model on COCO2017D,Model FID(#) YOLO score (AP/AP 50/AP75)(\")\n",
      "LostGAN-V2 [60] 42.55 9.1 / 15.3 / 9.8\n",
      "OCGAN [62] 41.65 -\n",
      "HCSS [25] 33.68 -\n",
      "LAMA [40] 31.12 13.40 / 19.70 / 14.90\n",
      "TwFA [69] 22.15 - / 28.20 / 20.12\n",
      "GLIGEN -LDM 21.04 22.4 / 36.5 / 24.1\n",
      "Table 2. Image quality and correspondence to layout are compared\n",
      "with baselines on COCO2017 val-set.\n",
      "Model Training data AP AP rAPcAPf\n",
      "LAMA [40] LVIS 2.0 0.9 1.3 3.2\n",
      "GLIGEN -LDM COCO2014CD 6.4 5.8 5.8 7.4\n",
      "GLIGEN -LDM COCO2014D 4.4 2.3 3.3 6.5\n",
      "GLIGEN -LDM COCO2014G 6.0 4.4 6.1 6.6\n",
      "GLIGEN -LDM GoldG,O365 10.6 5.8 9.6 13.8\n",
      "GLIGEN -LDM GoldG,O365,SBU,CC3M 11.1 9.0 9.8 13.4\n",
      "GLIGEN -Stable GoldG,O365,SBU,CC3M 10.8 8.8 9.9 12.6\n",
      "Upper-bound - 25.2 19.0 22.2 31.2\n",
      "Table 3. GLIP-score on LVIS validation set. Upper-bound is\n",
      "provided by running GLIP on real images scaled to 256 \u0002256.\n",
      "20 30 40\n",
      " FID\n",
      "102030 AP (YOLO) \n",
      "LostGAN-V2LAMA\n",
      "LDM\n",
      "(Zero-shot)LDM\n",
      "(Fine-tuned)GLIGen\n",
      "(Reference)GLIGen\n",
      "(Zero-shot)GLIGen\n",
      "(Fine-tuned)COCO 2017\n",
      "10 20 50 100 160\n",
      " FID\n",
      "1020 AP (GLIP) \n",
      "LAMAGLIGen \n",
      "(Reference)GLIGen\n",
      "(Zero-shot)GLIGen\n",
      "(Fine-tuned)LVIS\n",
      "Figure 5. Performance comparison measured by image genera-\n",
      "tion and grounding quality on COCO2017 (left) and LVIS (right)\n",
      "datasets. GLIGEN is built upon LDM, and continually pre-trained\n",
      "on the joint data of GoldG, O365, SBU, and CC3M. GLIGEN\n",
      "(Reference) is pre-trained on COCO/LVIS only. The circle size\n",
      "indicates the model size.\n",
      "which only has detection annotations. We use the 2017 splits\n",
      "(instead of 2014 as before), as it is the standard benchmark\n",
      "in the layout2img literature. In this experiment, we use the\n",
      "exact same annotation as all layout2img baselines.\n",
      "Table 2 shows that we achieve the state-of-the-art perfor-\n",
      "mance for both image quality and grounding accuracy. We\n",
      "believe the core reason is because previous methods train\n",
      "their model from scratch, whereas we build upon a large-\n",
      "scale pretrained generative model with rich visual semantics.\n",
      "Qualitative comparisons are in the supp. We also scale up\n",
      "our training data (discussed later) and pretrain a model on\n",
      "this dataset. Figure 5 left shows this model’s zero-shot and\n",
      "ﬁnetuned results.\n",
      "5.2. Open-set Grounded Text2Img Generation\n",
      "COCO-training model. We ﬁrst take GLIGEN trained only\n",
      "with the grounding annotations of COCO (COCO2014CD),\n",
      "and evaluate whether it can generate grounded entities be-\n",
      "yond the COCO categories. Figure 4 shows qualitative\n",
      "results, where GLIGEN can ground new concepts such as\n",
      "“blue jay ”, “croissant ” or ground object attributes\n",
      "7\n",
      "RealInputDALL E 2Stable DiffusionOurselephant\n",
      "personFigure 6. Inpainting results. Existing text2img diffusion models\n",
      "may generate objects that do not tightly ﬁt the masked box or miss\n",
      "an object if the same object already exists in the image.\n",
      "1%-3% 5%-10% 30%-50%\n",
      "LDM [51] 25.9 23.4 14.6\n",
      "GLIGEN -LDM 29.7 30.9 25.6\n",
      "Upper-bound 41.7 43.4 45.0\n",
      "Table 4. Inpainting results (YOLO AP) for different size of objects.\n",
      "such as “ brown wooden table ”, beyond the training\n",
      "categories. We hypothesize this is because the gated self-\n",
      "attention of GLIGEN learns to re-position the visual features\n",
      "corresponding to the grounding entities in the caption for\n",
      "the ensuing cross-attention layer, and gains generalization\n",
      "ability due to the shared text spaces in these two layers.\n",
      "We also quantitatively evaluate our model’s zero-shot\n",
      "generation performance on LVIS [15], which contains 1203\n",
      "long-tail object categories. We use GLIP to predict bounding\n",
      "boxes from the generated images and calculate AP, thus we\n",
      "name it as GLIP score . We compare to a state-of-the-art\n",
      "model designed for the layout2img task: LAMA [40]. We\n",
      "train LAMA using the ofﬁcial code on the LVIS training set\n",
      "(in a fully-supervised setting), whereas we directly evaluate\n",
      "our model in a zero-shot task transfer manner, by running\n",
      "inference on the LVIS val set without seeing any LVIS labels.\n",
      "Table 3 (ﬁrst 4 rows) shows the results. Surprisingly, even\n",
      "though our model is only trained on COCO annotations,\n",
      "it outperforms the supervised baseline by a large margin.\n",
      "This is because the baseline, which is trained from scratch,\n",
      "struggles to learn from limited annotations (many of the rare\n",
      "classes in LVIS have fewer than ﬁve training samples). In\n",
      "contrast, our model can take advantage of the pretrained\n",
      "model’s vast concept knowledge.\n",
      "Scaling up the training data. We next study our model’s\n",
      "open-set capability with much larger training data. Specif-\n",
      "ically, we follow GLIP [34] and train on Object365 [56]\n",
      "and GoldG [34], which combines two grounding datasets:\n",
      "Flickr [47] and VG [31]. We also use CC3M [57] and\n",
      "SBU [44] with grounding pseudo-labels generated by GLIP.\n",
      "Table 3 shows the data scaling results. As we scale up the\n",
      "training data, our model’s zero-shot performance increases,\n",
      "especially for rare concepts. We also try to ﬁnetune the\n",
      "model pretrained on our largest dataset on LVIS and demon-\n",
      "RealInputpix2pixHDOurs (w/o caption)Ours (w caption)Figure 7. Keypoint results. Our model generates higher quality\n",
      "images conditioned on keypoints, and it allows to use caption to\n",
      "specify details such as scene or gender.\n",
      "Model FID AP AP 50 AP75\n",
      "pix2pixHD [66] 142.4 15.8 33.7 13.0\n",
      "GLIGEN (w/o caption) 31.02 31.8 53.5 31.0\n",
      "GLIGEN (w caption) 27.34 31.5 52.9 31.0\n",
      "Upper-bound - 62.4 75.0 65.9\n",
      "Table 5. Conditioning with Human Keypoints evaluated on\n",
      "COCO2017 validation set. Upper-bound is calculated on real im-\n",
      "ages scaled to 256\u0002256.\n",
      "strate its performance on Figure 5 right. To demonstrate the\n",
      "generality of our method, we also train our model based on\n",
      "the Stable Diffusion model checkpoint using the largest data.\n",
      "We show some qualitative examples in Figure 8 using this\n",
      "model. Our model gains the grounding ability compared to\n",
      "vanilla Stable Diffusion. We also notice that Stable Diffu-\n",
      "sion model may overlook certain objects (“ umbrella ” in\n",
      "the second example) due to its use of the CLIP text encoder\n",
      "which tends to focus on global scene properties, and may\n",
      "ignore object-level details [3]. It also struggles to generate\n",
      "spatially counterfactual concepts. By explicitly injecting\n",
      "entity information through grounding tokens, our model can\n",
      "improve the grounding ability in two ways: the referred ob-\n",
      "jects are more likely to appear in the generated images, and\n",
      "the objects reside in the speciﬁed spatial location.\n",
      "5.3. Inpainting Comparison\n",
      "Like other diffusion models, GLIGEN can also work for\n",
      "the inpainting task by replacing the known region with a\n",
      "sample from q(ztjz0)after each sampling step, where z0is\n",
      "the latent representation of an image [51]. One can ground\n",
      "text descriptions to missing regions, as shown in Figure 6. In\n",
      "this setting, however, one may wonder, can we simply use a\n",
      "vanilla text-to-image diffusion model such Stable Diffusion\n",
      "or DALLE2 to ﬁll the missing region by providing the object\n",
      "name as the caption? What are the beneﬁts of having extra\n",
      "grounding inputs in such cases? To answer this, we conduct\n",
      "the following experiment on the COCO dataset: for each\n",
      "image, we randomly mask one object. We then let the model\n",
      "inpaint the missing region. We choose the missing object\n",
      "with three different size ratios with respect to the image:\n",
      "small (1%-3%), median (5%-10%), and large (30%-50%).\n",
      "5000 images are used for each case.\n",
      "8\n",
      "Caption: “Michael Jackson in a black cloth is singing into a microphone”Grounded text: Michael Jackson, black cloth, microphone\n",
      "Caption: “golden hour, a pekingeseis on the beach with an umbrella”Grounded text: Pekingese, umbrella,sea Ours-sample1Ours-sample2Ours-sample3Stable diffusion\n",
      "Caption: “a hen is hatching a huge egg”Grounded text: hen, egg\n",
      "Caption: “an apple and a same size dog”Grounded text: apple, dogFigure 8. Grounded text2image generation. The baseline lacks grounding ability and can also miss objects e.g. “umbrella ” in a sentence\n",
      "with multiple objects due to CLIP text space, and it also struggles to generate spatially counterfactual concepts.\n",
      "Table 4 demonstrates that our inpainted objects more\n",
      "tightly occupy the missing region (box) compared to the\n",
      "baselines. Fig. 6 provides examples to visually compare the\n",
      "inpainting results (we use Stable Diffusion for better quality).\n",
      "The ﬁrst row shows that baselines’ generated objects do not\n",
      "follow the provided box. The second row shows that when\n",
      "the missing category is already present in the image, they\n",
      "may ignore the caption. This is understandable as baselines\n",
      "are trained to generate a whole image following the caption.\n",
      "Our method may be more favorable for editing applications,\n",
      "where a user might want to generate an object that fully ﬁts\n",
      "the missing region or add an instance of a class that already\n",
      "exists in the image.5.4. Keypoints Grounding\n",
      "Although we have thus far demonstrated results with\n",
      "bounding boxes, our approach has ﬂexibility in the ground-\n",
      "ing condition that it can use for generation. To demonstrate\n",
      "this, we next evaluate our model with another type of ground-\n",
      "ing condition: human keypoints. We use the COCO2017\n",
      "dataset; details of the tokenization process for keypoints can\n",
      "be found in the supp. We compare with pix2pixHD [66], a\n",
      "classic image-to-image translation model. Since pix2pixHD\n",
      "does not take captions as input, we train two variants of our\n",
      "model: one uses COCO captions, the other does not. In the\n",
      "latter case, null caption is used as input to the cross-attention\n",
      "layer for a fair comparison.\n",
      "9\n",
      "Caption: “a car is in front of the sea”Grounded image: red insets at bottom right\n",
      "Caption: “a brick house in the woods, anime, oil painting”Grounded text: a brick house; Grounded style image: green insets at bottom right \n",
      "Figure 9. Image grounded generation (top) where images can provide more ﬁne-grained details than the text in the caption. Text and\n",
      "image grounded generation (bottom). Text is grounded using the red bounding box, and image is used as style reference.\n",
      "Figure 10. Image grounded Inpainting. One can use reference images to ground holes they want to ﬁll in.\n",
      "Fig. 7 shows the qualitative comparison. Clearly,\n",
      "our method generates much better image quality. For\n",
      "our model trained with captions, we can also specify\n",
      "other details such as the scene (“ A person is skiing\n",
      "down a snowy hill ”) or person’s gender (“ A woman is\n",
      "holding a baby ”). These two inputs complement each\n",
      "other and can enrich a user’s controllability for image cre-\n",
      "ation. We measure keypoint correspondence (similar to the\n",
      "YOLO score for boxes) by running a MaskRCNN [18] key-\n",
      "point detector on the generated images. Both of our model\n",
      "variants produce similar results; see Table 5.\n",
      "5.5. Image Grounding\n",
      "Image grounded generation. One can also use a refer-\n",
      "ence image to represent a grounded entity as discussed pre-viously. Fig. 9 top row shows qualitative results, which\n",
      "demonstrate that the visual feature can complement details\n",
      "that are hard to describe by language, such as the style and\n",
      "shape of cars.\n",
      "Text and image grounded generation. Besides using ei-\n",
      "ther text or image to represent a grounded entity, one can\n",
      "also keep both representations in one model for more cre-\n",
      "ative generation. Figure 9 second row shows text grounded\n",
      "generation with style / tone transfer. Here we ground the\n",
      "text (“a brick house”) with the red bounding box. For the\n",
      "style reference image, we ﬁnd that grounding it to an image\n",
      "corner (green bounding box) or its edge is sufﬁcient. Since\n",
      "the model needs to generate a harmonious style for the entire\n",
      "image, we hypothesize the self-attention layers may broad-\n",
      "cast this information to all pixels, thus leading to consistent\n",
      "10\n",
      "style for the entire image.\n",
      "Image grounded inpainting. As we previously demon-\n",
      "strated, one can ground text to missing region for inpainting,\n",
      "one can also ground reference images to missing regions.\n",
      "Figure 10 shows inpainting results grounded on reference im-\n",
      "ages. To remove boundary artifacts, we follow GLIDE [43],\n",
      "and modify the ﬁrst conv layer by adding 5 extra channels (4\n",
      "forz0and 1 for inpainting mask) and make them trainable\n",
      "with the new added layers.\n",
      "5.6. Scheduled Sampling\n",
      "As stated in Eq. (8)and Eq. (10), we can schedule infer-\n",
      "ence time sampling by setting \fto 1 (use extra grounding\n",
      "information) or 0 (reduce to the original pretrained diffu-\n",
      "sion model). This can make our model exploit different\n",
      "knowledge at different stages.\n",
      "Fig. 11 qualitatively shows the beneﬁts of our scheduled\n",
      "sampling for our model built upon Stable Diffusion. The\n",
      "images in the same row share the same noise and conditional\n",
      "input. The ﬁrst row shows that scheduled sampling can be\n",
      "used to improve image quality, as the original Stable Diffu-\n",
      "sion model is trained with high quality images. The ﬁrst 20%\n",
      "steps usually are sufﬁcient for setting the overall structure\n",
      "of large objects, and the original Stable Diffusion model can\n",
      "then complete the remaining sampling process with its high-\n",
      "quality prior. The second row shows a generation example by\n",
      "our model trained with COCO human keypoint annotations.\n",
      "Since this model is purely trained with human keypoints,\n",
      "the ﬁnal result is biased towards generating a human even\n",
      "if a different object (i.e., robot) is speciﬁed in the caption.\n",
      "However, by using scheduled sampling, we can extend this\n",
      "model to generate other objects with a human-like shape.\n",
      "We do notice that scheduled sampling can decrease the\n",
      "correspondence to box or keypoint if we set the \u001cto be too\n",
      "small. To quantitatively measure this, we evaluate the GLIP\n",
      "score of the generated images on the LVIS dataset (similar to\n",
      "Table 3) and we use GLIGEN based on the Stable Diffusion\n",
      "model. The GLIP AP for \u001c= 0:0(vanilla Stable Diffusion),\n",
      "\u001c= 0:2,\u001c= 0:3,\u001c= 0:5,\u001c= 0:8and\u001c= 1 (GLIGEN )\n",
      "are: 0.3, 2.4, 4.7, 8.9, 10.8. Note that to present the GLIGEN\n",
      "own performance, we set \u001c= 1, i.e., without scheduled\n",
      "sampling, for all results reported in the previous sections.\n",
      "6. Conclusion\n",
      "We proposed GLIGEN for expanding pretrained text2img\n",
      "diffusion models with grounding ability, and demonstrated\n",
      "open-world generalization using bounding boxes as the\n",
      "grounding condition. Our method is simple and effective,\n",
      "and can be easily extended to other conditions e.g., keypoints\n",
      "and reference images. One limitation we noticed is that the\n",
      "generated style or aesthetic distribution can shift after adding\n",
      "the new gated self-attention layers ( e.g., the model some-\n",
      "times struggles to generate graphics style images when \u001cis\n",
      "Caption: “a cute low poly Shiba Inu”Grounded text: Shiba Inu𝜏=1\n",
      "Caption: “a robot is sitting on a bench”Grounded keypoints: plotted dots on the left figure𝜏=0.2Figure 11. Scheduled Samping. It can improve visual or extend a\n",
      "model trained in one domain (e.g., human) to the others. See text\n",
      "for details.\n",
      "set to 1), which is probably due to the grounding training\n",
      "data being all natural images. We believe adding images\n",
      "from more diverse style distributions or further ﬁnetuning\n",
      "the model with highly aesthetic images could help alleviate\n",
      "this issue.\n",
      "Acknowledgement. We thank Yufan Zhou and Huangjie\n",
      "Zheng for the empirical insights on the FID evaluation of\n",
      "diffusion models, Haotian Zhang for the guidance on large-\n",
      "scale grounding data, Ce Liu for the discussion on the poten-\n",
      "tial applications of grounded generative image models. This\n",
      "work was supported in part by NSF CAREER IIS2150012,\n",
      "NASA 80NSSC21K0295, and Institute of Information &\n",
      "communications Technology Planning & Evaluation(IITP)\n",
      "grants funded by the Korea government(MSIT) (No. 2022-\n",
      "0-00871, Development of AI Autonomy and Knowledge\n",
      "Enhancement for AI Agent Collaboration) and (No. RS-\n",
      "2022-00187238, Development of Large Korean Language\n",
      "Model Technology for Efﬁcient Pre-training).\n",
      "References\n",
      "[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\n",
      "Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\n",
      "sch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza\n",
      "Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina\n",
      "Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\n",
      "Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Shar-\n",
      "ifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\n",
      "Andrew Zisserman, and Karen Simonyan. Flamingo: a visual\n",
      "language model for few-shot learning. ArXiv , abs/2204.14198,\n",
      "2022. 2, 5, 7\n",
      "[2]Oron Ashual, Shelly Sheynin, Adam Polyak, Uriel Singer,\n",
      "Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knn-\n",
      "11\n",
      "diffusion: Image generation via large-scale retrieval. arXiv\n",
      "preprint arXiv:2204.02849 , 2022. 6\n",
      "[3]Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\n",
      "aming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli\n",
      "Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-\n",
      "i: Text-to-image diffusion models with an ensemble of expert\n",
      "denoisers. ArXiv , abs/2211.01324, 2022. 2, 8\n",
      "[4]Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training\n",
      "of image transformers. arXiv preprint arXiv:2106.08254 ,\n",
      "2021. 2\n",
      "[5]Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark\n",
      "Liao. Yolov4: Optimal speed and accuracy of object detection.\n",
      "ArXiv , abs/2004.10934, 2020. 6\n",
      "[6]Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\n",
      "Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\n",
      "William T Freeman, Michael Rubinstein, et al. Muse: Text-to-\n",
      "image generation via masked generative transformers. arXiv\n",
      "preprint arXiv:2301.00704 , 2023. 2\n",
      "[7]Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W\n",
      "Cohen. Re-imagen: Retrieval-augmented text-to-image gen-\n",
      "erator. arXiv preprint arXiv:2209.14491 , 2022. 6\n",
      "[8]Yunjey Choi, Min-Je Choi, Mun Su Kim, Jung-Woo Ha,\n",
      "Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed gener-\n",
      "ative adversarial networks for multi-domain image-to-image\n",
      "translation. 2018 IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition , pages 8789–8797, 2018. 3\n",
      "[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova. BERT: Pre-training of deep bidirectional trans-\n",
      "formers for language understanding. In Proceedings of the\n",
      "2019 Conference of the North American Chapter of the As-\n",
      "sociation for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long and Short Papers) , pages 4171–\n",
      "4186, Minneapolis, Minnesota, June 2019. Association for\n",
      "Computational Linguistics. 3\n",
      "[10] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\n",
      "gans on image synthesis. ArXiv , abs/2105.05233, 2021. 2\n",
      "[11] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang\n",
      "Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia\n",
      "Yang, and Jie Tang. Cogview: Mastering text-to-image gener-\n",
      "ation via transformers, 2021. 6\n",
      "[12] Wanshu Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng,\n",
      "Lu Yuan, and Yu-Chiang Frank Wang. Frido: Feature pyra-\n",
      "mid diffusion for complex scene image synthesis. ArXiv ,\n",
      "abs/2208.13753, 2022. 6\n",
      "[13] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\n",
      "Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-\n",
      "based text-to-image generation with human priors. ArXiv ,\n",
      "abs/2203.13131, 2022. 2, 6\n",
      "[14] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\n",
      "Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\n",
      "and Yoshua Bengio. Generative adversarial nets. In NIPS ,\n",
      "2014. 1, 3\n",
      "[15] Agrim Gupta, Piotr Doll ´ar, and Ross B. Girshick. Lvis: A\n",
      "dataset for large vocabulary instance segmentation. CVPR ,\n",
      "pages 5351–5359, 2019. 2, 8, 15\n",
      "[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\n",
      "Doll´ar, and Ross Girshick. Masked autoencoders are scalablevision learners. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition , pages 16000–\n",
      "16009, 2022. 2\n",
      "[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\n",
      "Girshick. Momentum contrast for unsupervised visual repre-\n",
      "sentation learning. In CVPR , 2020. 2\n",
      "[18] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\n",
      "Girshick. Mask r-cnn. 2017 IEEE International Conference\n",
      "on Computer Vision (ICCV) , pages 2980–2988, 2017. 10\n",
      "[19] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep\n",
      "residual learning for image recognition. CVPR , pages 770–\n",
      "778, 2016. 3\n",
      "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\n",
      "hard Nessler, and Sepp Hochreiter. Gans trained by a two\n",
      "time-scale update rule converge to a local nash equilibrium.\n",
      "InNIPS , 2017. 6\n",
      "[21] Jonathan Ho. Classiﬁer-free diffusion guidance. ArXiv ,\n",
      "abs/2207.12598, 2022. 4, 6, 15\n",
      "[22] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\n",
      "memory. Neural Computation , 9:1735–1780, 1997. 3\n",
      "[23] Ray S Jackendoff. Semantic structures , volume 18. MIT\n",
      "press, 1992. 4\n",
      "[24] Manuel Jahn, Robin Rombach, and Bj ¨orn Ommer. High-\n",
      "resolution complex scene synthesis with transformers. ArXiv ,\n",
      "abs/2105.06458, 2021. 2, 3\n",
      "[25] Manuel Jahn, Robin Rombach, and Bj ¨orn Ommer. High-\n",
      "resolution complex scene synthesis with transformers. ArXiv ,\n",
      "abs/2105.06458, 2021. 7, 16\n",
      "[26] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image gen-\n",
      "eration from scene graphs. 2018 IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition , pages 1219–1228,\n",
      "2018. 2\n",
      "[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based\n",
      "generator architecture for generative adversarial networks.\n",
      "CVPR , pages 4396–4405, 2019. 1\n",
      "[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\n",
      "generator architecture for generative adversarial networks.\n",
      "CVPR , pages 4396–4405, 2019. 3\n",
      "[29] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\n",
      "Jaakko Lehtinen, and Timo Aila. Analyzing and improving\n",
      "the image quality of stylegan. 2020 IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "8107–8116, 2020. 1\n",
      "[30] Diederik P. Kingma and Jimmy Ba. Adam: A method for\n",
      "stochastic optimization. CoRR , abs/1412.6980, 2015. 15\n",
      "[31] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\n",
      "Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\n",
      "tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\n",
      "Li Fei-Fei. Visual genome: Connecting language and vision\n",
      "using crowdsourced dense image annotations. International\n",
      "Journal of Computer Vision , 123:32–73, 2016. 8\n",
      "[32] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan\n",
      "Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu,\n",
      "Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEV ATER: A\n",
      "benchmark and toolkit for evaluating language-augmented vi-\n",
      "sual models. In NeurIPS Track on Datasets and Benchmarks ,\n",
      "2022. 2\n",
      "12\n",
      "[33] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Got-\n",
      "mare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi. Align\n",
      "before fuse: Vision and language representation learning with\n",
      "momentum distillation. arXiv preprint arXiv:2107.07651 ,\n",
      "2021. 2\n",
      "[34] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\n",
      "wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan,\n",
      "Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng\n",
      "Gao. Grounded language-image pre-training. In IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pages\n",
      "10955–10965. IEEE, 2022. 2, 6, 8\n",
      "[35] Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang,\n",
      "Xiaodong He, Siwei Lyu, and Jianfeng Gao. Object-driven\n",
      "text-to-image synthesis via adversarial training. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition , pages 12174–12182, 2019. 6\n",
      "[36] Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae\n",
      "Lee, and Krishna Kumar Singh. Collaging class-speciﬁc gans\n",
      "for semantic image synthesis. ICCV , pages 14398–14407,\n",
      "2021. 3\n",
      "[37] Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae\n",
      "Lee, and Krishna Kumar Singh. Contrastive learning\n",
      "for diverse disentangled foreground generation. ArXiv ,\n",
      "abs/2211.02707, 2022. 2\n",
      "[38] Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, and\n",
      "Yong Jae Lee. Mixnmatch: Multifactor disentanglement and\n",
      "encoding for conditional image generation. 2020 IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR) , pages 8036–8045, 2020. 3\n",
      "[39] Z. Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and\n",
      "Lingyun Sun. Image synthesis from layout with locality-\n",
      "aware mask adaption. ICCV , pages 13799–13808, 2021. 2,\n",
      "3\n",
      "[40] Z. Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and\n",
      "Lingyun Sun. Image synthesis from layout with locality-\n",
      "aware mask adaption. ICCV , pages 13799–13808, 2021. 6, 7,\n",
      "8, 15, 16\n",
      "[41] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,\n",
      "Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence\n",
      "Zitnick. Microsoft coco: Common objects in context. In\n",
      "ECCV , 2014. 2, 4, 6, 16\n",
      "[42] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\n",
      "Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\n",
      "Representing scenes as neural radiance ﬁelds for view synthe-\n",
      "sis. In ECCV , 2020. 4, 5, 15\n",
      "[43] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\n",
      "Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\n",
      "Mark Chen. Glide: Towards photorealistic image generation\n",
      "and editing with text-guided diffusion models. In ICML , 2022.\n",
      "2, 11\n",
      "[44] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n",
      "Im2text: Describing images using 1 million captioned pho-\n",
      "tographs. In NIPS , 2011. 8\n",
      "[45] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\n",
      "Zhu. Semantic image synthesis with spatially-adaptive nor-\n",
      "malization. CVPR , pages 2332–2341, 2019. 1, 3[46] Deepak Pathak, Philipp Kr ¨ahenb ¨uhl, Jeff Donahue, Trevor\n",
      "Darrell, and Alexei A. Efros. Context encoders: Feature\n",
      "learning by inpainting. CVPR , pages 2536–2544, 2016. 1, 2\n",
      "[47] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes,\n",
      "Juan C. Caicedo, J. Hockenmaier, and Svetlana Lazebnik.\n",
      "Flickr30k entities: Collecting region-to-phrase correspon-\n",
      "dences for richer image-to-sentence models. International\n",
      "Journal of Computer Vision , 123:74–93, 2015. 8\n",
      "[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n",
      "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n",
      "Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\n",
      "Krueger, and Ilya Sutskever. Learning transferable visual\n",
      "models from natural language supervision. In ICML , 2021. 2,\n",
      "3\n",
      "[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\n",
      "and Mark Chen. Hierarchical text-conditional image gener-\n",
      "ation with clip latents. ArXiv , abs/2204.06125, 2022. 1, 2,\n",
      "6\n",
      "[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\n",
      "Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\n",
      "Zero-shot text-to-image generation. In Marina Meila and\n",
      "Tong Zhang, editors, Proceedings of the 38th International\n",
      "Conference on Machine Learning , volume 139 of Proceedings\n",
      "of Machine Learning Research , pages 8821–8831. PMLR,\n",
      "18–24 Jul 2021. 1, 2\n",
      "[51] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick\n",
      "Esser, and Bj ¨orn Ommer. High-resolution image synthesis\n",
      "with latent diffusion models. CVPR , pages 10674–10685,\n",
      "2022. 2, 3, 6, 8, 15\n",
      "[52] O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional\n",
      "networks for biomedical image segmentation. In Medical\n",
      "Image Computing and Computer-Assisted Intervention (MIC-\n",
      "CAI) , volume 9351 of LNCS , pages 234–241. Springer, 2015.\n",
      "(available on arXiv:1505.04597 [cs.CV]). 3, 15\n",
      "[53] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee,\n",
      "Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad\n",
      "Norouzi. Palette: Image-to-image diffusion models. ACM\n",
      "SIGGRAPH 2022 Conference Proceedings , 2022. 2, 3\n",
      "[54] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\n",
      "Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour,\n",
      "Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gon-\n",
      "tijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet,\n",
      "and Mohammad Norouzi. Photorealistic text-to-image dif-\n",
      "fusion models with deep language understanding. ArXiv ,\n",
      "abs/2205.11487, 2022. 1, 2, 6\n",
      "[55] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\n",
      "Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\n",
      "Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-\n",
      "400M: open dataset of clip-ﬁltered 400 million image-text\n",
      "pairs. CoRR , abs/2111.02114, 2021. 6\n",
      "[56] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\n",
      "Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\n",
      "large-scale, high-quality dataset for object detection. ICCV ,\n",
      "pages 8429–8438, 2019. 8\n",
      "[57] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\n",
      "Soricut. Conceptual captions: A cleaned, hypernymed, image\n",
      "alt-text dataset for automatic image captioning. In ACL, 2018.\n",
      "8\n",
      "13\n",
      "[58] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. In-\n",
      "terpreting the latent space of gans for semantic face editing.\n",
      "2020 IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 9240–9249, 2020. 1\n",
      "[59] Wei Sun and Tianfu Wu. Image synthesis from reconﬁgurable\n",
      "layout and style. ICCV , pages 10530–10539, 2019. 2, 3\n",
      "[60] Wei Sun and Tianfu Wu. Learning layout and style reconﬁg-\n",
      "urable gans for controllable image synthesis. TPAMI , 44:5070–\n",
      "5087, 2022. 2, 3, 7, 16\n",
      "[61] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon\n",
      "Hjelm, and Shikhar Sharma. Object-centric image generation\n",
      "from layouts. ArXiv , abs/2003.07449, 2021. 2\n",
      "[62] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon\n",
      "Hjelm, and Shikhar Sharma. Object-centric image generation\n",
      "from layouts. ArXiv , abs/2003.07449, 2021. 7, 16\n",
      "[63] Ming Tao, Hao Tang, Songsong Wu, N. Sebe, Fei Wu, and\n",
      "Xiaoyuan Jing. Df-gan: Deep fusion generative adversarial\n",
      "networks for text-to-image synthesis. ArXiv , abs/2008.05865,\n",
      "2020. 3, 6\n",
      "[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\n",
      "eit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\n",
      "Polosukhin. Attention is all you need. In I. Guyon, U. V on\n",
      "Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\n",
      "and R. Garnett, editors, Advances in Neural Information Pro-\n",
      "cessing Systems , volume 30. Curran Associates, Inc., 2017.\n",
      "3\n",
      "[65] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need. ArXiv ,\n",
      "abs/1706.03762, 2017. 2, 3\n",
      "[66] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\n",
      "Jan Kautz, and Bryan Catanzaro. High-resolution image\n",
      "synthesis and semantic manipulation with conditional gans.\n",
      "2018 IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition , pages 8798–8807, 2018. 8, 9\n",
      "[67] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\n",
      "Daxin Jiang, and Nan Duan. N ¨uwa: Visual synthesis pre-\n",
      "training for neural visual world creation. In European Con-\n",
      "ference on Computer Vision , 2022. 2, 6\n",
      "[68] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\n",
      "Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\n",
      "grained text to image generation with attentional generative\n",
      "adversarial networks. 2018 IEEE/CVF Conference on Com-\n",
      "puter Vision and Pattern Recognition , pages 1316–1324, 2018.\n",
      "3, 6\n",
      "[69] Zuopeng Yang, Daqing Liu, Chaoyue Wang, J. Yang, and\n",
      "Dacheng Tao. Modeling image composition for complex\n",
      "scene generation. CVPR , pages 7754–7763, 2022. 2, 7, 16\n",
      "[70] Zuopeng Yang, Daqing Liu, Chaoyue Wang, J. Yang, and\n",
      "Dacheng Tao. Modeling image composition for complex\n",
      "scene generation. CVPR , pages 7754–7763, 2022. 2, 3\n",
      "[71] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin\n",
      "Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael\n",
      "Zeng, and Lijuan Wang. Reco: Region-controlled text-to-\n",
      "image generation. ArXiv , abs/2211.15518, 2022. 3\n",
      "[72] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan\n",
      "Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, YinfeiYang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han,\n",
      "Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\n",
      "Yonghui Wu. Scaling autoregressive models for content-rich\n",
      "text-to-image generation. ArXiv , abs/2206.10789, 2022. 1, 2,\n",
      "6\n",
      "[73] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\n",
      "Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\n",
      "Boxin Li, Chunyuan Li, et al. Florence: A new foundation\n",
      "model for computer vision. arXiv preprint arXiv:2111.11432 ,\n",
      "2021. 2\n",
      "[74] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-\n",
      "Fu Chang. Open-vocabulary object detection using captions.\n",
      "InProceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition , pages 14393–14402, 2021. 2\n",
      "[75] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and\n",
      "Yinfei Yang. Cross-modal contrastive learning for text-to-\n",
      "image generation, 2021. 6\n",
      "[76] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image\n",
      "generation from layout. CVPR , pages 8576–8585, 2019. 2, 3\n",
      "[77] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan\n",
      "Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang\n",
      "Dai, Lu Yuan, Yin Li, et al. RegionCLIP: Region-based\n",
      "language-image pretraining. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition ,\n",
      "pages 16793–16803, 2022. 2\n",
      "[78] Yufan Zhou, Chunyuan Li, Changyou Chen, Jianfeng Gao,\n",
      "and Jinhui Xu. Laﬁte2: Few-shot text-to-image generation.\n",
      "arXiv preprint arXiv:2210.14124 , 2022. 3, 6\n",
      "[79] Yufan Zhou, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou\n",
      "Chen, and Jinhui Xu. Shifted diffusion for text-to-image\n",
      "generation. arXiv preprint arXiv:2211.15388 , 2022. 2\n",
      "[80] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,\n",
      "Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong\n",
      "Sun. Towards language-free training for text-to-image gener-\n",
      "ation. CVPR , 2022. 1, 6\n",
      "[81] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.\n",
      "Unpaired image-to-image translation using cycle-consistent\n",
      "adversarial networks. In Computer Vision (ICCV), 2017 IEEE\n",
      "International Conference on , 2017. 3\n",
      "14\n",
      "Appendix\n",
      "In this supplemental material, we ﬁrst provide more im-\n",
      "plementation and training details, and then present more\n",
      "results and discussions.\n",
      "A. Implementation and training details\n",
      "We use the Stable Diffusion model [51] as the example\n",
      "to illustrate our implementation details.\n",
      "Box Grounding Tokens with Text. Each grounded text\n",
      "is ﬁrst fed into the text encoder to get the text embedding\n",
      "(e.g., 768 dimension of the CLIP text embedding in Stable\n",
      "Diffusion). Since the Stable Diffusion uses features of 77\n",
      "text tokens outputted from the transformer backbone, thus\n",
      "we choose “ EOS” token feature at this layer as our grounded\n",
      "text embedding. This is because in the CLIP training, this\n",
      "“EOS” token feature is chosen and applied a linear transform\n",
      "(one FC layer) to compare with visual feature, thus this\n",
      "token feature should contain whole information about the\n",
      "input text description. We also tried to directly use CLIP\n",
      "text embedding ( after linear projection), however, we notice\n",
      "slow convergence empirically probably due to unaligned\n",
      "space between the grounded text embedding and the caption\n",
      "embeddings. Following NeRF [42], we encode bounding\n",
      "box coordinates with the Fourier embedding with output\n",
      "dimension 64. As stated in the Eq (5)in the main paper,\n",
      "we ﬁrst concatenate these two features and feed them into a\n",
      "multi-layer perceptron. The MLP consists of three hidden\n",
      "layers with hidden dimension 512, the output grounding\n",
      "token dimension is set to be the same as the text embedding\n",
      "dimension ( e.g., 768 in the Stable Diffusion case). We set\n",
      "the maximum number of grounding tokens to be 30 in the\n",
      "bounding box case.\n",
      "Box Grounding Tokens with Image. We use the similar\n",
      "way to get the grounding token for an image. We use the\n",
      "CLIP image encoder (ViT-L-14 is used for the Stable Dif-\n",
      "fusion) to get an image embedding. We denote the CLIP\n",
      "training objective as maximizing (Ptht)>(Pihi)(we omit\n",
      "normalization), where htis “EOS” token embedding from\n",
      "the text encoder, hiis “CLS” token embedding from the\n",
      "image encoder, and PtandPiare linear transformation for\n",
      "text and image embedding, respectively. Since htis the\n",
      "text feature space used for grounded text features, to ease\n",
      "our training, we choose to project image features into the\n",
      "text feature space via P>\n",
      "tPihi, and normalized it to 28.7,\n",
      "which is average norm of htwe empirically found. We\n",
      "also set the maximum number of grounding tokens to be 30.\n",
      "Thus, 60 tokens in total if one keep both image and text as\n",
      "representations for a grounded entity.\n",
      "Keypoint Grounding Tokens. The grounding token for\n",
      "keypoint annotations is processed in the same way, ex-cept that we also learn Nperson token embedding vectors\n",
      "fp1;:::;pNgto semantically link keypoints belonging to\n",
      "the same person. This is to deal with the situation in which\n",
      "there are multiple people in the same image that we want\n",
      "to generate, so that the model knows which keypoint corre-\n",
      "sponds to which person. Each keypoint semantic embedding\n",
      "ftext(e)is processed by using the text encoder, for example,\n",
      "we forward the text: “ left eye ” into the encoder to get\n",
      "its semantic embedding; the dimension of each person token\n",
      "is set the same as text embedding dimension. The grounding\n",
      "token is calculated by:\n",
      "he=MLP (ftext(e) +pj;Fourier (l)) (11)\n",
      "wherelis thex;ylocation of each keypoint and pjis the\n",
      "person token for the j’th person. In practice, we set Nas\n",
      "10, which is the maximum number of persons allowed to be\n",
      "generated in each image. Thus, we have 170 tokens in the\n",
      "COCO dataset ( i.e., 10*17; 17 keypoint annotations for each\n",
      "person).\n",
      "Gated Self-Attention Layers. Our inserted self-attention\n",
      "layer is the same as the original diffusion model self-\n",
      "attention layer at each Transformer block, except that we\n",
      "add one linear projection layer which converts the grounding\n",
      "token into the same dimension as the visual token. For exam-\n",
      "ple, in the ﬁrst layer of the down branch of the UNet [52], the\n",
      "projection layer converts grounding token of dimension 768\n",
      "into 320 (which is the image feature dimension at this layer),\n",
      "and visual tokens are concatenated with the grounding to-\n",
      "kens as the input to the gated attention layer as illustrated in\n",
      "Figure 3.\n",
      "Training Details. For all COCO related experiments\n",
      "(Sec 5.1), we train LDM with batch size 64 using 16 V100\n",
      "GPUs for 100k iterations. In the scaling up training data\n",
      "experiment (in Sec. 5.2 of the main paper), we train for 400k\n",
      "iterations for LDM, but 500K iterations with batch size of\n",
      "32 for the Stable diffusion modeL For all training, we use\n",
      "learning rate of 5e-5 with Adam [30], and use warm-up\n",
      "for the ﬁrst 10k iterations. We randomly drop caption and\n",
      "grounding tokens with 10% probability for classiﬁer-free\n",
      "guidance [21].\n",
      "B. Additional quantitative results\n",
      "In this section, we show more studies with our pretrained\n",
      "model using our largest data (GoldG, O365, CC3M, SBU).\n",
      "We had reported this model’s zero-shot performance on\n",
      "LVIS [15] in the main paper Table 3. Here we ﬁnetune\n",
      "this model on LVIS, and report its GLIP-score in Table 6.\n",
      "Clearly, after ﬁnetuning, we show much more accurate gener-\n",
      "ation results, surpassing the supervised baseline LAMA [40]\n",
      "by a large margin.\n",
      "Similarly, we also test this model’s zero-shot performance\n",
      "on the COCO2017 val-set, and its ﬁnetuning results are in\n",
      "15\n",
      "Model Pre-training data Traing data FID AP AP r APc APf\n",
      "LAMA [40] – LVIS 151.96 2.0 0.9 1.3 3.2\n",
      "GLIGEN -LDM COCO2014CD – 22.17 6.4 5.8 5.8 7.4\n",
      "GLIGEN -LDM COCO2014D – 31.31 4.4 2.3 3.3 6.5\n",
      "GLIGEN -LDM COCO2014G – 13.48 6.0 4.4 6.1 6.6\n",
      "GLIGEN -LDM GoldG,O365 – 8.45 10.6 5.8 9.6 13.8\n",
      "GLIGEN -LDM GoldG,O365,SBU,CC3M – 10.28 11.1 9.0 9.8 13.4\n",
      "GLIGEN -LDM GoldG,O365,SBU,CC3M LVIS 6.25 14.9 10.1 12.8 19.3\n",
      "Upper-bound – – – 25.2 19.0 22.2 31.2\n",
      "Table 6. GLIP-score on LVIS validation set. Upper-bound is provided by running GLIP on real images scaled to 256 \u0002256.\n",
      "mountainperson horsegrassbuilding bus car bear mountain rock InputLostGAN-v2TwFAOursLDMOursStable\n",
      "Figure 12. Layout2img comparison. Our model generates better\n",
      "quality images, especially when using stable diffusion. Baseline\n",
      "images are all copied from TwFA [69]\n",
      "Table 7. The results show the beneﬁts of pretraining which\n",
      "can largely improve layout correspondence performance.\n",
      "C. More qualitative results and discussion\n",
      "We show qualitative comparisons with layout2img base-\n",
      "lines in Figure 12, which complements the results in Sec 5.1\n",
      "of the main paper. The results show that our model has\n",
      "comparable image quality when built upon LDM, but has\n",
      "more visual appeal and details when built upon the Stable\n",
      "Diffusion model.YOLO score\n",
      "Model FID AP AP 50 AP75\n",
      "LostGAN-V2 [60] 42.55 9.1 15.3 9.8\n",
      "OCGAN [62] 41.65 –\n",
      "HCSS [25] 33.68 –\n",
      "LAMA [40] 31.12 13.40 19.70 14.90\n",
      "TwFA [69] 22.15 – 28.20 20.12\n",
      "GLIGEN -LDM 21.04 22.4 36.5 24.1\n",
      "After pretrain on GoldG,O365,SBU,CC3M\n",
      "GLIGEN -LDM ( zero-shot ) 27.03 19.1 30.5 20.8\n",
      "GLIGEN -LDM ( ﬁnetuned ) 21.58 30.8 42.3 35.3\n",
      "Table 7. Image quality and correspondence to layout are compared\n",
      "with baselines on COCO2017 val-set.\n",
      "Lastly, we show more grounded text2img results with\n",
      "bounding boxes in Figure 13 and with images or keypoints\n",
      "in Figure 14. Note that our keypoint model only uses key-\n",
      "point annotations from COCO [41] which is not linked with\n",
      "person identity, but it can successfully utilize and combine\n",
      "the knowledge learned in the text2img training stage to con-\n",
      "trol keypoints of a speciﬁc person. Out of curiosity, we also\n",
      "tested whether the keypoint grounding information learned\n",
      "on humans can be transferred to other non-humanoid cate-\n",
      "gories such as cat or lamp for keypoint grounded generation,\n",
      "but we ﬁnd that our model struggles in such cases even with\n",
      "scheduled sampling. Compared to bounding boxes, which\n",
      "only specify a coarse location and size of an object in the\n",
      "image and thus can be shared across all object categories,\n",
      "keypoints (i.e., object parts) are not always shareable across\n",
      "different categories. Thus, while keypoints enable more\n",
      "ﬁne-grained control than boxes, they are less generalizable.\n",
      "16\n",
      "Caption: “Space view of a planet and its sun”Grounded text: planet, sun\n",
      "Caption: “a a photo of a hybrid between a bee and a rabbit”Grounded text: hybrid between a bee and a rabbit, flower\n",
      "Caption: “cartoon sketch of a little girl with a smile and balloons, old style, detailed, elegant, intricate”Grounded text: girl with a smile, balloon, balloon, balloon\n",
      "Caption: “Walter White in GTA v”Grounded text: Walter White, car,bulldog\n",
      "Caption: “two pirate ships on the ocean in minecraft”Grounded text: a pirate ship, a pirate shipFigure 13. Bounding box grounded text2image generation. Our model can ground noun entities in the caption for controllable image\n",
      "generation\n",
      "17\n",
      "Caption: “Barack Obama is sitting at a desk”Grounded keypoints: plotted dots on the left\n",
      "Caption: “Steve Jobs is working with his laptop”Grounded keypoints: plotted dots on the left\n",
      "Caption: “Pikachu is under a tree, digital art”Grounded text: Pikachu,tree; Grounded image: blue inset\n",
      "Figure 14. Top row: text grounded generation with style transfer. Second row: image grounded inpainting. Bottom rows: Keypoint grounded\n",
      "text2image generation. We did not use keypoint annotation associated with any identity during training.\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "with open(paper_paths[0], 'r') as fp:\n",
    "    paper = fp.read()\n",
    "\n",
    "print(paper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few clear cleaning steps we can add here:\n",
    "\n",
    "* Replace `\"-\\n\"` that indicates a mid-word break with `\"\"`.\n",
    "* Where a single newline is surrounded by text characters (and not) by end of line marker like `:` or `.` - we might be able to merge into single line. Doesn't work for all of above though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'-\\n', '', text)  # remove hyphenated newline words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIGEN : Open-Set Grounded Text-to-Image Generation\n",
      "Yuheng Li1x, Haotian Liu1x, Qingyang Wu2, Fangzhou Mu1, Jianwei Yang3, Jianfeng Gao3,\n",
      "Chunyuan Li3{, Yong Jae Lee1{\n",
      "1University of Wisconsin-Madison2Columbia University3Microsoft\n",
      "https://gligen.github.io/\n",
      "Caption: “A woman sitting in a restaurant with a pizza in front of her ”Grounded text: table, pizza, person, wall, car, paper, chair, window, bottle, cup\n",
      "Caption: “a baby girl/monkey is scratching her/its head”Grounded keypoints: plotted dots on the left\n",
      "Caption: “A bird/helmet is on the grass”Grounded image: red inset\n",
      "Caption: “Elon Musk and Emma Watson on a movie poster”Grounded text: Elon Musk, Emma Watson; Grounded style image: blue inset\n",
      "Figure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model, by feeding different grounding\n",
      "conditions. GLIGEN supports (a) text entity + box, (b) image entity + box, (c) image style and text + box, (d) text entity + keypoints. The\n",
      "generated examples for each scenario are shown in top-left, top-right, bottom-left, and bottom-right, respectively.\n",
      "Abstract\n",
      "Large-scale text-to-image diffusion models have made\n",
      "amazing advances. However, the status quo is to use\n",
      "text input alone, which can impede controllability. In this\n",
      "work, we propose GLIGEN ,Grounded- Language-to- Image\n",
      "Generation, a novel approach that builds upon and extends\n",
      "the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on\n",
      "grounding inputs. To preserve the vast concept knowledge of\n",
      "the pre-trained model, we freeze all of its weights and inject\n",
      "the grounding information into new trainable layers via a\n",
      "gated mechanism. Our model achieves open-world grounded\n",
      "text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to\n",
      "novel spatial conﬁgurations and concepts. GLIGEN ’s zeroshot performance on COCO and LVIS outperforms existing\n",
      "supervised layout-to-image baselines by a large margin.\n",
      "xPart of the work performed at Microsoft; {Co-senior authors1. Introduction\n",
      "Image generation research has witnessed huge advances\n",
      "in recent years. Over the past couple of years, GANs [14]\n",
      "were the state-of-the-art, with their latent space and conditional inputs being well-studied for controllable manipulation [46, 58] and generation [27, 29, 45, 80]. Text conditional autoregressive [50, 72] and diffusion [49, 54] models\n",
      "have demonstrated astonishing image quality and concept\n",
      "coverage, due to their more stable learning objectives and\n",
      "large-scale training on web image-text paired data. These\n",
      "models have gained attention even among the general public\n",
      "due to their practical use cases ( e.g., art design and creation).\n",
      "Despite exciting progress, existing large-scale text-toimage generation models cannot be conditioned on other\n",
      "input modalities apart from text, and thus lack the ability\n",
      "to precisely localize concepts or use reference images to\n",
      "control the generation process. The current input, i.e., natural language alone, restricts the way that information can\n",
      "1arXiv:2301.07093v1  [cs.CV]  17 Jan 2023\n",
      "be expressed. For example, it is difﬁcult to describe the\n",
      "precise location of an object using text, whereas bounding\n",
      "boxes / keypoints can easily achieve this, as shown in Figure 1. While conditional diffusion models [10, 51, 53] and\n",
      "GANs [26, 37, 46, 69] that take in input modalities other\n",
      "than text for inpainting, layout2img generation, etc., do exist,\n",
      "they rarely combine those inputs for controllable text2img\n",
      "generation.\n",
      "Moreover, prior generative models—regardless of the\n",
      "generative model family—are usually independently trained\n",
      "on each task-speciﬁc dataset. In contrast, in the recognition ﬁeld, the long-standing paradigm has been to build a\n",
      "task-speciﬁc recognition model [32] by starting from a foundation model pretrained on large-scale image data [4, 16, 17]\n",
      "or image-text pairs [33, 48, 73]. Since diffusion models have\n",
      "been trained on billions of image-text pairs [51], a natural\n",
      "question is: Can we build upon existing pretrained diffusion models and endow them with new conditional input\n",
      "modalities? In this way, analogous to the recognition literature, we may be able to achieve better performance on other\n",
      "generation tasks due to the vast concept knowledge that the\n",
      "pretrained models have, while acquiring more controllability\n",
      "over existing text-to-image generation models.\n",
      "With the above aims, we propose a method for providing\n",
      "new grounding conditional inputs to pretrained text-to-image\n",
      "diffusion models. As shown in Figure 1, we still retain the\n",
      "text caption as input, but also enable other input modalities\n",
      "such as bounding boxes for grounding concepts, grounding\n",
      "reference images, and grounding part keypoints. The key\n",
      "challenge is preserving the original vast concept knowledge\n",
      "in the pretrained model while learning to inject the new\n",
      "grounding information. To prevent knowledge forgetting,\n",
      "we propose to freeze the original model weights and add\n",
      "new trainable gated Transformer layers [65] that take in the\n",
      "new grounding input ( e.g., bounding box). During training,\n",
      "we gradually fuse the new grounding information into the\n",
      "pretrained model using a gated mechanism [1]. This design\n",
      "enables ﬂexibility in the sampling process during generation\n",
      "for improved quality and controllability; for example, we\n",
      "show that using the full model (all layers) in the ﬁrst half of\n",
      "the sampling steps and only using the original layers (without\n",
      "the gated Transformer layers) in the latter half can lead\n",
      "to generation results that accurately reﬂect the grounding\n",
      "conditions while also having high image quality.\n",
      "In our experiments, we primarily study grounded\n",
      "text2img generation with bounding boxes, inspired by the\n",
      "recent scaling success of learning grounded language-image\n",
      "understanding models with boxes in GLIP [34]. To enable our model to ground open-world vocabulary concepts [32,34,74,77], we use the same pre-trained text encoder\n",
      "(for encoding the caption) to encode each phrase associated\n",
      "with each grounded entity ( i.e., one phrase per bounding\n",
      "box) and feed the encoded tokens into the newly insertedlayers with their encoded location information. Due to the\n",
      "shared text space, we ﬁnd that our model can generalize to\n",
      "unseen objects even when only trained on the COCO [41]\n",
      "dataset. Its generalization on LVIS [15] outperforms a strong\n",
      "fully-supervised baseline by a large margin. To further improve our model’s grounding ability, we unify the object\n",
      "detection and grounding data formats for training, following\n",
      "GLIP [34], as they provide complementary beneﬁts: detection data is of larger quantity, while grounding data has a\n",
      "richer vocabulary. With larger training data, our model’s\n",
      "generalization is consistently improved.\n",
      "Contributions. 1) We propose a new text2img generation\n",
      "method that endows new grounding controllability over existing text2img diffusion models. 2) By preserving the pretrained weights and learning to gradually integrate the new\n",
      "localization layers, our model achieves open-world grounded\n",
      "text2img generation with bounding box inputs, i.e., synthesis\n",
      "of novel localized concepts unobserved in training. 3) Our\n",
      "model’s zero-shot performance on layout2img tasks signiﬁcantly outperforms the prior state-of-the-art, demonstrating\n",
      "the power of building upon large pretrained generative models for downstream tasks.\n",
      "2. Related Work\n",
      "Large scale text-to-image generation models. State-ofthe-art models in this space are either autoregressive [13, 50,\n",
      "67, 72] or diffusion [43, 49, 51, 54, 79]. Among autoregressive models, DALL-E [50] is one of the breakthrough works\n",
      "that demonstrates zero-shot abilities, while Parti [72] demonstrates the feasibility of scaling up autoregressive models.\n",
      "Diffusion models have also shown very promising results.\n",
      "DALL-E 2 [49] generates images from the CLIP [48] image\n",
      "space, while Imagen [54] ﬁnds the beneﬁt of using pretrained\n",
      "language models. The concurrent Muse [6] demonstrates\n",
      "that masked modeling can achieve SoTA-level generation\n",
      "performance with higher inference speed. However, all of\n",
      "these models usually only take a caption as the input, which\n",
      "can be difﬁcult for conveying other information such as the\n",
      "precise location of an object. Make-A-Scene [13] also incorporates semantic maps into its text-to-image generation, by\n",
      "training an encoder to tokenize semantic masks to condition\n",
      "the generation. However, it can only operate in a closed-set\n",
      "(of 158 categories), whereas our grounded entities can be\n",
      "open-world. A concurrent work eDiff-I [3] shows that by\n",
      "changing the attention map, one can generate objects that\n",
      "roughly follow a semantic map input. However, we believe\n",
      "our interface with boxes is simpler, and more importantly,\n",
      "our method allows other conditioning inputs such as keypoints, which are hard to manipulate through attention.\n",
      "Image generation from layouts. Given bounding boxes\n",
      "labeled with object categories, the task is to generate a corresponding image [24, 39, 59 –61, 70, 76], which is the reverse\n",
      "2\n",
      "task of object detection. Layout2Im [76] formulated the\n",
      "problem and combined a V AE object encoder, an LSTM [22]\n",
      "object fuser, and an image decoder to generate the image, using global and object-level adversarial losses [14] to enforce\n",
      "realism and layout correspondence. LostGAN [59, 60] generates a mask representation which is used to normalize features, taking inspiration from StyleGAN [28]. LAMA [39]\n",
      "improves the intermediate mask quality for better image\n",
      "quality. Transformer [64] based methods [24, 70] have also\n",
      "been explored. Critically, existing layout2image methods\n",
      "are closed-set, i.e., they can only generate limited localized\n",
      "visual concepts observed in the training set such as the 80\n",
      "categories in COCO. In contrast, our method represents the\n",
      "ﬁrst work for open-set grounded image generation. A concurrent work ReCo [71] also demonstrates open-set abilities\n",
      "by building upon a pretraned Stable Diffusion model [51].\n",
      "However, it ﬁnetunes the original model weights, which has\n",
      "the potential to lead to knowledge forgetting. Furthermore,\n",
      "it only demonstrates box grounding results whereas we also\n",
      "show image and keypoint grounding results.\n",
      "Other conditional image generation. For GANs, various conditioning information have been explored; e.g.,\n",
      "text [63, 68, 78], box [59, 60, 76], semantic masks [36, 45],\n",
      "images [8,38,81]. For diffusion models, LDM [51] proposes\n",
      "a uniﬁed approach for conditional generation by injecting the\n",
      "condition via cross-attention layers. Palette [53] performs\n",
      "image-to-image tasks using diffusion models. These models\n",
      "are usually trained from scratch independently. In our work,\n",
      "we investigate how to build upon existing models pretrained\n",
      "on large-scale web data, to enable new open-set grounded\n",
      "image generation capabilities in a cost-effective manner.\n",
      "3. Preliminaries on Latent Diffusion Models\n",
      "Diffusion-based methods are one of the most effective\n",
      "model families for text2image tasks, among which latent\n",
      "diffusion model (LDM) [51] and its successor Stable Diffusion are the most powerful models publicly available to\n",
      "the research community. To reduce the computational costs\n",
      "of vanilla diffusion model training, LDM proceeds in two\n",
      "stages. The ﬁrst stage learns a bidirectional mapping network to obtain the latent representation zof the imagex.\n",
      "The second stage trains a diffusion model on the latent z.\n",
      "Since the ﬁrst stage model produces a ﬁxed bidirectional\n",
      "mapping between xandz, from hereon, we focus on the\n",
      "latent generation space of LDM for simplicity.\n",
      "Training Objective. Starting from noise zT, the model\n",
      "gradually produces less noisy samples zT\u00001;zT\u00002;\u0001\u0001\u0001;z0,\n",
      "conditioned on caption cat every time step t. To learn such\n",
      "a modelf\u0012parameterized by \u0012, for each step, the LDM\n",
      "training objective solves the denoising problem on latentrepresentations zof the imagex:\n",
      "min\n",
      "\u0012LLDM=Ez;\u000f\u0018N(0;I);t\u0002\n",
      "k\u000f\u0000f\u0012(zt;t;c)k2\n",
      "2\u0003\n",
      ";(1)\n",
      "wheretis uniformly sampled from time steps f1;\u0001\u0001\u0001;Tg,\n",
      "ztis the step-tnoisy variant of input z, andf\u0012(\u0003;t;c)is the\n",
      "(t;c)-conditioned denoising autoencoder.\n",
      "Network Architecture. The core of the network architecture is how to encode the conditions, based on which a\n",
      "cleaner version of zis produced. (i)Denoising Autoencoder .f\u0012(\u0003;t;c)is implemented via UNet [52]. It takes in\n",
      "a noisy latentz, as well as information from time step tand\n",
      "conditionc. It consists of a series of ResNet [19] and Transformer [65] blocks. (ii)Condition Encoding . In the original\n",
      "LDM, a BERT-like [9] network is trained from scratch to\n",
      "encode each caption into a sequence of text embeddings,\n",
      "ftext(c), which is fed into (1)to replacec. The caption feature is encoded via a ﬁxed CLIP [48] text encoder in Stable\n",
      "Diffusion. Time tis ﬁrst mapped to time embedding \u001e(t),\n",
      "then injected into the UNet. The caption feature is used in\n",
      "a cross attention layer within each Transformer block. The\n",
      "model learns to predict the noise, following (1).\n",
      "With large-scale training, the model f\u0012(\u0003;t;c)is well\n",
      "trained to denoise zbased on the caption information only.\n",
      "Though impressive language-to-image generation results\n",
      "have been shown with LDM by pretraining on internet-scale\n",
      "data, it remains challenging to synthesize images where\n",
      "additional grounding input can be instructed, and is thus the\n",
      "focus of our paper.\n",
      "4. Open-set Grounded Image Generation\n",
      "4.1. Grounding Instruction Input\n",
      "For grounded text-to-image generation, there are a variety\n",
      "of ways to ground an object via spatial conditioning. We\n",
      "denote asethe grounding entity described either through\n",
      "text or an example image, and as lthe grounding spatial\n",
      "conﬁguration described with e.g., a bounding box or a set of\n",
      "keypoints. We deﬁne the instruction to a grounded text-toimage model as a composition of the caption and grounded\n",
      "entities:\n",
      "Instruction:y= (c;e);with (2)\n",
      "Caption:c= [c1;\u0001\u0001\u0001;cL] (3)\n",
      "Grounding:e= [(e1;l1);\u0001\u0001\u0001;(eN;lN)] (4)\n",
      "whereLis the caption length, and Nis the number of entities\n",
      "to ground. In this work, we primarily study using bounding\n",
      "box as the grounding spatial conﬁguration l, because of its\n",
      "large availability and easy annotation for users. For the\n",
      "grounded entity e, we mainly focus on using text as its\n",
      "representation due to simplicity. We process both caption\n",
      "and grounding entities as input tokens to the diffusion model,\n",
      "as described in detail below.\n",
      "3\n",
      "A brideand groomare about to cut their wedding cake\n",
      "A living room has a glowing brick fireplace\n",
      "[PAD]Text encoder \n",
      "Text encoder \n",
      "+++===Grounding DataDetection DataDetection + Caption Data\n",
      "Caption TokensGroundingTokenspersonsurfboardlightcouchlampcouchcouchWedding cakebridegroomGroundingEntitiesCaption(a) Three type of training data(b) The construction process of caption and groundingtokens A bride and groom are about to cut their wedding cakewedding cakegroombride\n",
      "…boxboxbox\n",
      "A brideand groomare about to cut their wedding cake\n",
      "A living room has a glowing brick fireplace\n",
      "[PAD]Text encoder \n",
      "Text encoder \n",
      "+++===Grounding DataDetection DataDetection + Caption Data\n",
      "Caption TokensGroundingTokenspersonsurfboardlightcouchlampcouchcouchWedding cakebridegroomGroundingEntitiesCaption(a) Three type of training data(b) The construction process of caption and groundingtokens A bride and groom are about to cut their wedding cakewedding cakegroombride\n",
      "…boxboxbox(a) Three types of training data (b) The construction process of caption and grounding tokens\n",
      "Figure 2. Illustration of training data and grounding instruction input. (a) For the grounding entities, we directly visualize the concept and\n",
      "bounding box information on the ground-truth images. The box is parameterized as normalized image coordinates, e.g. the person box is\n",
      "[0.37,0.31,0.92,0.84]. (b) The ﬁrst example in (a) is used to illustrate the token construction process.\n",
      "Caption Tokens. The captioncis processed in the same\n",
      "way as in LDM. Speciﬁcally, we obtain the caption feature sequence (yellow tokens in Figure 2(b)) using hc=\n",
      "[hc\n",
      "1;\u0001\u0001\u0001;hc\n",
      "L] =ftext(c), wherehc\n",
      "`is the contextualized text\n",
      "feature for the `-th word in the caption.\n",
      "Grounding Tokens. For each grounded text entity denoted\n",
      "with a bounding box, we represent the location information\n",
      "asl= [\u000bmin;\fmin;\u000bmax;\fmax]with its top-left and bottomright coordinates. For the text entity e, we use the same pretrained text encoder to obtain its text feature ftext(e)(light\n",
      "green token in Figure 2(b)), and then fuse it with its bounding\n",
      "box information to produce a grounding token (dark green\n",
      "token in Figure 2(b)):\n",
      "he=MLP (ftext(e);Fourier (b)) (5)\n",
      "where Fourier is the Fourier embedding [42], and MLP (\u0001;\u0001)\n",
      "is a multi-layer perceptron that ﬁrst concatenates the two\n",
      "inputs across the feature dimension. The grounding token\n",
      "sequence is represented as he= [he\n",
      "1;\u0001\u0001\u0001;he\n",
      "N]\n",
      "From Closed-set to Open-set. Note that existing layout2img works only deal with a closed-set setting ( e.g.,\n",
      "COCO categories), as they typically learn a vector embeddinguper entity, to replace ftext(e)in(5). For a closed-set\n",
      "setting withKconcepts, a dictionary of with Kembeddings\n",
      "are learned, U= [u1;\u0001\u0001\u0001;uK]. While this non-parametric\n",
      "representation works well in the closed-set setting, it has\n",
      "two drawbacks: (1) The conditioning is implemented as a\n",
      "dictionary look-up over Uin the evaluation stage, and thus\n",
      "the model can only ground the observed entities in the generated images, lacking the ability to generalize to ground new\n",
      "entities; (2) No word/phrase is ever utilized in the model\n",
      "condition, and the semantic structure [23] of the underlying\n",
      "language instruction is missing. In contrast, in our open-set\n",
      "design, since the noun entities are processed by the same text\n",
      "encoder that is used to encode the caption, we ﬁnd that even\n",
      "when the localization information is limited to the concepts\n",
      "in the grounding training datasets, our model can still generalize to other concepts as we will show in our experiments.\n",
      "Training Data. The training data for grounded image generation requires both text cand grounding entity eas the fullcondition. In practice, we can relax the data requirement by\n",
      "considering a more ﬂexible input, i.e. the three types of data\n",
      "shown in Figure 2(a). (i)Grounding data . Each image is\n",
      "associated with a caption describing the whole image; noun\n",
      "entities are extracted from the caption, and are labeled with\n",
      "bounding boxes. Since the noun entities are taken directly\n",
      "from the natural language caption, they can cover a much\n",
      "richer vocabulary which will be beneﬁcial for open-world\n",
      "vocabulary grounded generation. (ii)Detection data . Nounentities are pre-deﬁned closed-set categories ( e.g., 80 object\n",
      "classes in COCO [41]). In this case, we choose to use a null\n",
      "caption token as introduced in classiﬁer-free guidance [21]\n",
      "for the caption. The detection data is of larger quantity (millions) than the grounding data (thousands), and can therefore\n",
      "greatly increase overall training data. (iii)Detection and\n",
      "caption data . Noun entities are same as those in the detection data, and the image is described separately with a text\n",
      "caption. In this case, the noun entities may not exactly match\n",
      "those in the caption. For example, in Figure 2(a), the caption\n",
      "only gives a high-level description of the living room without\n",
      "mentioning the objects in the scene, whereas the detection\n",
      "annotation provides more ﬁne-grained object-level details.\n",
      "Extensions to Other Grounding Conditions. Note that\n",
      "the proposed grounding instruction in Eq (4)is in a general\n",
      "form, though our description thus far has focused on the\n",
      "case of using text as entity eand bounding box as l(the\n",
      "major setting of this paper). To demonstrate the ﬂexibility\n",
      "of the GLIGEN framework, we also study two additional\n",
      "representative cases which extend the use scenario of Eq (4).\n",
      "•Image Prompt. While language allows users to describe\n",
      "a rich set of entities in an open-vocabulary manner, sometimes more abstract and ﬁne-grained concepts can be\n",
      "better characterized by example images. To this end,\n",
      "one may describe entity eusing an image, instead of\n",
      "language. We use an image encoder to obtain feature\n",
      "fimage(e)which is used in place of ftext(e)in Eq (5)when\n",
      "eis an image.\n",
      "•Keypoints. As a simple parameterization method to specify the spatial conﬁguration of an entity, bounding boxes\n",
      "ease the user-machine interaction interface by providing\n",
      "4\n",
      "VisualCaption GroundingGated Self-Attention\n",
      "Self-Attention\n",
      "Cross-Attention\n",
      "Figure 3. For a pretrained text2img model, the text features are fed\n",
      "into each cross-attention layer. A new gated self-attention layer is\n",
      "inserted to take in the new conditional localization information.\n",
      "the height and width of the object layout only. One may\n",
      "consider richer spatial conﬁgurations such as keypoints\n",
      "forGLIGEN , by parameterizing lin Eq (4)with a set\n",
      "of keypoint coordinates. Similar to encoding boxes, the\n",
      "Fourier embedding [42] can be applied to each keypoint\n",
      "locationl= [x;y].\n",
      "Figure 1 shows generated examples for these other grounding\n",
      "conditions. Please refer to the supp for more details.\n",
      "4.2. Continual Learning for Grounded Generation\n",
      "Our goal is to endow new spatial grounding capabilities to\n",
      "existing large language-to-image generation models. Large\n",
      "diffusion models have been pre-trained on web-scale imagetext to gain the required knowledge for synthesizing realistic\n",
      "images based on diverse and complex language instructions.\n",
      "Due to the high pre-training cost and excellent performance,\n",
      "it is important to retain such knowledge in the model weights\n",
      "while expanding the new capability. Hence, we consider to\n",
      "lock the original model weights, and gradually adapt the\n",
      "model by tuning new modules.\n",
      "Gated Self-Attention. We denotev= [v1;\u0001\u0001\u0001;vM]as\n",
      "the visual feature tokens of an image. The original Transformer block of LDM consists of two attention layers: The\n",
      "self-attention over the visual tokens, followed by crossattention from caption tokens. By considering the residual\n",
      "connection, the two layers can be written:\n",
      "v=v+SelfAttn (v) (6)\n",
      "v=v+CrossAttn (v;hc) (7)\n",
      "We freeze these two attention layers and add a new gated\n",
      "self-attention layer to enable the spatial grounding ability;see Figure 3. Speciﬁcally, the attention is performed over\n",
      "the concatenation of visual and grounding tokens [v;he]:\n",
      "v=v+\f\u0001tanh(\n",
      ")\u0001TS(SelfAttn ([v;he])) (8)\n",
      "where TS(\u0001)is a token selection operation that considers\n",
      "visual tokens only, and \n",
      "is a learnable scalar which is initialized as 0. \fis set as 1 during the entire training process\n",
      "and is only varied for scheduled sampling during inference\n",
      "(introduced below) for improved quality and controllability.\n",
      "Note that (8)is injected in between (6)and(7). Intuitively,\n",
      "the gated self-attention in (8)allows visual features to leverage bounding box information, and the resulting grounded\n",
      "features are treated as a residual, whose gate is initially set to\n",
      "0 (due to\n",
      "being initialized as 0). This also enables more stable training. Note that a similar idea is used in Flamingo [1];\n",
      "however, it uses gated cross-attention, which leads to worse\n",
      "performance in our case, possibly due to the lack of position\n",
      "embeddings for the visual features in the pretrained diffusion\n",
      "model.\n",
      "Learning Procedure. We adapt the pre-trained model\n",
      "such that grounding information can be injected while all\n",
      "the original components remain intact. By denoting the new\n",
      "parameters in all gated self-attention layers as \u00120, we use the\n",
      "original denoising objective as in (1)for model continual\n",
      "learning, based on the grounding instruction input y:\n",
      "min\n",
      "\u00120LGrounding =Ez;\u000f\u0018N(0;I);t\u0002\n",
      "k\u000f\u0000ff\u0012;\u00120g(zt; t;y)k2\n",
      "2\u0003\n",
      ":(9)\n",
      "Why should the model try to use the new grounding information? Intuitively, predicting the noise that was added to\n",
      "a training image in the reverse diffusion process would be\n",
      "easier if the model could leverage the external knowledge\n",
      "about each object’s location. Thus, in this way, the model\n",
      "learns to use the additional localization information while\n",
      "retaining the pre-trained concept knowledge.\n",
      "A Versatile User Interface. Once the model is well\n",
      "trained, our design of disentangling the caption and grounding inputs supports a versatile interface. Not only do we\n",
      "allow a user to ground entities that exist in the caption input,\n",
      "but objects can also be freely added in the desired locations\n",
      "without being mentioned in the caption input (see the pizza\n",
      "example in Figure 1). For a pure text-based diffusion model,\n",
      "a user would have to cumbersomely describe all the objects\n",
      "in the caption, while also specifying their precise locations,\n",
      "which can be difﬁcult to do with language alone.\n",
      "Scheduled Sampling in Inference. The standard inference scheme of GLIGEN is to set\f= 1 in(8), and the\n",
      "entire diffusion process is inﬂuenced by the grounding tokens. This constant \fsampling scheme provides overall\n",
      "5\n",
      "good performance in terms of both generation and grounding, but sometimes generates lower quality images compared\n",
      "with the original text2img models (e.g., as Stable Diffusion\n",
      "is ﬁnetuned on high aesthetic scored images). To strike a better trade-off between generation and grounding for GLIGEN ,\n",
      "we propose a scheduled sampling scheme. As we freeze\n",
      "the original model weights and add new layers to inject new\n",
      "grounding information in training, there is ﬂexibility during\n",
      "inference to schedule the diffusion process to either use both\n",
      "the grounding and language tokens or use only the language\n",
      "tokens of the original model at anytime, by setting different\fvalues in (8). Speciﬁcally, we consider a two-stage\n",
      "inference procedure, divided by \u001c2[0;1]. For a diffusion\n",
      "process with Tsteps in total, one can set \fto 1 at the beginning\u001c\u0003Tsteps, and set \fto 0 for the remaining (1\u0000\u001c)\u0003T\n",
      "steps:\n",
      "\f=\u001a1; t\u0014\u001c\u0003T# Grounded inference stage\n",
      "0; t>\u001c\u0003T# Standard inference stage(10)\n",
      "The major beneﬁt of scheduled sampling is improved\n",
      "visual quality as the rough concept location and outline are\n",
      "decided in the early stages, followed by ﬁne-grained details\n",
      "in later stages. It also allows us to extend the model trained\n",
      "in one domain (human keypoint) to other domains (monkey,\n",
      "cartoon characters) as shown in Figure 1.\n",
      "5. Experiments\n",
      "We evaluate our model’s grounded text2img generation\n",
      "in both the closed-set and open-set settings, ablate its components, and show extensions to image prompt and keypoint grounded generation. We conduct our main quantitative experiments by building upon a pretrained LDM on\n",
      "LAION [55], unless stated otherwise.\n",
      "5.1. Closed-set Grounded Text2Img Generation\n",
      "We ﬁrst evaluate the generation quality and grounding\n",
      "accuracy of our model in a closed-set setting. For this, we\n",
      "train and evaluate on the COCO2014 [41] dataset, which is\n",
      "a standard benchmark used in the text2img literature [49, 54,\n",
      "63,68,80], and evaluate how the different types of grounding\n",
      "instructions impact our model’s performance.\n",
      "Grounding instructions. We use the following grounding\n",
      "instructions to train our model: 1) COCO2014D: Detection Data. There are no caption annotations so we use a\n",
      "null caption input [21]. Detection annotations are used as\n",
      "noun-entities. 2) COCO2014CD: Detection + Caption Data.\n",
      "Both caption and detection annotations are used. Note that\n",
      "the noun entities may not always exist in the caption. 3)\n",
      "COCO2014G: Grounding Data. Given the caption annotations, we use GLIP [34], which detects the caption’s noun\n",
      "entities in the image, to get pseudo box labels.ModelGeneration: FID (#) Grounding: YOLO (\")\n",
      "Fine-tuned Zero-shot AP/AP 50/AP75\n",
      "CogView [11] - 27.10 KNN-Diffusion [2] - 16.66 DALL-E 2 [49] - 10.39 Imagen [54] - 7.27 Re-Imagen [7] 5.25 6.88\n",
      "Parti [72] 3.20 7.23 LAFITE [80] 8.12 26.94 LAFITE2 [78] 4.28 8.42 Make-a-Scene [13] 7.55 11.84 N¨UWA [67] 12.90 - Frido [12] 11.24 - XMC-GAN [75] 9.33 - AttnGAN [68] 35.49 - DF-GAN [63] 21.42 - Obj-GAN [35] 20.75 - LDM [51] - 12.63 LDM* 5.91 11.73 0.6 / 2.0 / 0.3\n",
      "GLIGEN (COCO2014CD) 5.82 - 21.7 / 39.0 / 21.7\n",
      "GLIGEN (COCO2014D) 5.61 - 24.0 / 42.2 / 24.1\n",
      "GLIGEN (COCO2014G) 6.38 - 11.2 / 21.2 / 10.7\n",
      "Table 1. Evaluation of image quality and correspondence to layout\n",
      "on COCO2014 val-set. All numbers are taken from corresponding papers, LDM* is our COCO ﬁne-tuned LDM baseline. Here\n",
      "GLIGEN is built upon LDM.\n",
      "Baselines. Baseline models are listed in Table 1. Among\n",
      "them, we also ﬁnetune an LDM [51] pretrained on LAION\n",
      "400M [55] on COCO2014 with its caption annotations,\n",
      "which we denote as LDM*. The text2img baselines, as\n",
      "they cannot be conditioned on box inputs, are trained on\n",
      "COCO2014C: Caption Data.\n",
      "Evaluation metrics. We use the captions and/or box annotations from 30K randomly sampled images to generate 30K\n",
      "images for evaluation. We use FID [20] to evaluate image\n",
      "quality. To evaluate grounding accuracy ( i.e. correspondence\n",
      "between the input bounding box and generated entity), we\n",
      "use the YOLO score [40]. Speciﬁcally, we use a pretrained\n",
      "YOLO-v4 [5] to detect bounding boxes on the generated\n",
      "images and compare them with the ground truth boxes using\n",
      "average precision (AP). Since prior text2img methods do\n",
      "not support taking box annotations as input, it is not fair to\n",
      "compare with them on this metric. Thus, we only report\n",
      "numbers for the ﬁne-tuned LDM as a reference.\n",
      "Results. Table 1 shows the results. First, we see that the\n",
      "image synthesis quality of our approach, as measured by FID,\n",
      "is better than most of the state-of-the-art baselines due to rich\n",
      "visual knowledge learned in the pretraining stage. Next, we\n",
      "ﬁnd that all three grounding instructions lead to comparable\n",
      "FID to that of the LDM* baseline, which is ﬁnetuned on\n",
      "COCO2014 with caption annotations. Our model trained\n",
      "using detection annotation instructions (COCO2014D) has\n",
      "the overall best performance. However, when we evaluate\n",
      "this model on COCO2014CD instructions, we ﬁnd that it\n",
      "has worse performance (FID: 8.2) – its ability to understand\n",
      "real captions may be limited as it is only trained with the\n",
      "null caption. For the model trained with GLIP grounding\n",
      "instructions (COCO2014G), we actually evaluate it using\n",
      "the COCO2014CD instructions since we need to compute\n",
      "6\n",
      "A blue jay is standing on a branch in the woods near usacroissant is placed in a brown wooden tablea hello kitty is holding a laundry basketFigure 4. Our model can generalize to open-world concepts even\n",
      "when only trained using localization annotation from COCO.\n",
      "the YOLO score which requires ground-truth detection annotations. Its slightly worse FID may be attributed to its\n",
      "learning from GLIP pseudo-labels. The same reason can\n",
      "explain its low YOLO score ( i.e., the model did not see any\n",
      "ground-truth detection annotations during training).\n",
      "Overall, this experiment shows that: 1) Our model can\n",
      "successfully take in boxes as an additional condition while\n",
      "maintaining image generation quality. 2) All grounding\n",
      "instruction types are useful, which suggests that combining\n",
      "their data together can lead to complementary beneﬁts.\n",
      "Ablation on gated self-attention. Our approach uses\n",
      "gated self-attention to absorb the grounding instruction. We\n",
      "can also consider gated cross-attention [1], where the query\n",
      "is the visual feature, and the keys and values are produced\n",
      "using the grounding condition. We ablate this design on\n",
      "COCO2014CD data, and ﬁnd that it leads to similar FID:\n",
      "5.8, but worse YOLO AP: 16.6 (compared to 21.7 for selfattention in Table 1). This shows the necessity of information sharing among the visual tokens, which exists in\n",
      "self-attention but not in cross-attention.\n",
      "Ablation on null caption. We choose to use the\n",
      "null caption when we only have detection annotations\n",
      "(COCO2014D). An alternative scheme is to simply combine all noun entities into a sentence; e.g., if there are two\n",
      "cats and a dog in an image, then the pseudo caption can be:\n",
      "“cat, cat, dog ”. In this case, the FID becomes worse\n",
      "and increases to 7.40 from 5.61 (null caption). This is likely\n",
      "due to the pretrained text encoder never having encountered\n",
      "this type of unnatural caption during LDM training. A solution would be to ﬁnetune the text encoder or design a better\n",
      "prompt, but this is not the focus of our work.\n",
      "Comparison to Layout2Img generation methods. Thus\n",
      "far, we have seen that our model correctly learns to use the\n",
      "grounding condition. But how accurate is it compared to\n",
      "methods that are speciﬁcally designed for layout2img generation? To answer this, we train our model on COCO2017D,Model FID(#) YOLO score (AP/AP 50/AP75)(\")\n",
      "LostGAN-V2 [60] 42.55 9.1 / 15.3 / 9.8\n",
      "OCGAN [62] 41.65 HCSS [25] 33.68 LAMA [40] 31.12 13.40 / 19.70 / 14.90\n",
      "TwFA [69] 22.15 - / 28.20 / 20.12\n",
      "GLIGEN -LDM 21.04 22.4 / 36.5 / 24.1\n",
      "Table 2. Image quality and correspondence to layout are compared\n",
      "with baselines on COCO2017 val-set.\n",
      "Model Training data AP AP rAPcAPf\n",
      "LAMA [40] LVIS 2.0 0.9 1.3 3.2\n",
      "GLIGEN -LDM COCO2014CD 6.4 5.8 5.8 7.4\n",
      "GLIGEN -LDM COCO2014D 4.4 2.3 3.3 6.5\n",
      "GLIGEN -LDM COCO2014G 6.0 4.4 6.1 6.6\n",
      "GLIGEN -LDM GoldG,O365 10.6 5.8 9.6 13.8\n",
      "GLIGEN -LDM GoldG,O365,SBU,CC3M 11.1 9.0 9.8 13.4\n",
      "GLIGEN -Stable GoldG,O365,SBU,CC3M 10.8 8.8 9.9 12.6\n",
      "Upper-bound - 25.2 19.0 22.2 31.2\n",
      "Table 3. GLIP-score on LVIS validation set. Upper-bound is\n",
      "provided by running GLIP on real images scaled to 256 \u0002256.\n",
      "20 30 40\n",
      " FID\n",
      "102030 AP (YOLO) \n",
      "LostGAN-V2LAMA\n",
      "LDM\n",
      "(Zero-shot)LDM\n",
      "(Fine-tuned)GLIGen\n",
      "(Reference)GLIGen\n",
      "(Zero-shot)GLIGen\n",
      "(Fine-tuned)COCO 2017\n",
      "10 20 50 100 160\n",
      " FID\n",
      "1020 AP (GLIP) \n",
      "LAMAGLIGen \n",
      "(Reference)GLIGen\n",
      "(Zero-shot)GLIGen\n",
      "(Fine-tuned)LVIS\n",
      "Figure 5. Performance comparison measured by image generation and grounding quality on COCO2017 (left) and LVIS (right)\n",
      "datasets. GLIGEN is built upon LDM, and continually pre-trained\n",
      "on the joint data of GoldG, O365, SBU, and CC3M. GLIGEN\n",
      "(Reference) is pre-trained on COCO/LVIS only. The circle size\n",
      "indicates the model size.\n",
      "which only has detection annotations. We use the 2017 splits\n",
      "(instead of 2014 as before), as it is the standard benchmark\n",
      "in the layout2img literature. In this experiment, we use the\n",
      "exact same annotation as all layout2img baselines.\n",
      "Table 2 shows that we achieve the state-of-the-art performance for both image quality and grounding accuracy. We\n",
      "believe the core reason is because previous methods train\n",
      "their model from scratch, whereas we build upon a largescale pretrained generative model with rich visual semantics.\n",
      "Qualitative comparisons are in the supp. We also scale up\n",
      "our training data (discussed later) and pretrain a model on\n",
      "this dataset. Figure 5 left shows this model’s zero-shot and\n",
      "ﬁnetuned results.\n",
      "5.2. Open-set Grounded Text2Img Generation\n",
      "COCO-training model. We ﬁrst take GLIGEN trained only\n",
      "with the grounding annotations of COCO (COCO2014CD),\n",
      "and evaluate whether it can generate grounded entities beyond the COCO categories. Figure 4 shows qualitative\n",
      "results, where GLIGEN can ground new concepts such as\n",
      "“blue jay ”, “croissant ” or ground object attributes\n",
      "7\n",
      "RealInputDALL E 2Stable DiffusionOurselephant\n",
      "personFigure 6. Inpainting results. Existing text2img diffusion models\n",
      "may generate objects that do not tightly ﬁt the masked box or miss\n",
      "an object if the same object already exists in the image.\n",
      "1%-3% 5%-10% 30%-50%\n",
      "LDM [51] 25.9 23.4 14.6\n",
      "GLIGEN -LDM 29.7 30.9 25.6\n",
      "Upper-bound 41.7 43.4 45.0\n",
      "Table 4. Inpainting results (YOLO AP) for different size of objects.\n",
      "such as “ brown wooden table ”, beyond the training\n",
      "categories. We hypothesize this is because the gated selfattention of GLIGEN learns to re-position the visual features\n",
      "corresponding to the grounding entities in the caption for\n",
      "the ensuing cross-attention layer, and gains generalization\n",
      "ability due to the shared text spaces in these two layers.\n",
      "We also quantitatively evaluate our model’s zero-shot\n",
      "generation performance on LVIS [15], which contains 1203\n",
      "long-tail object categories. We use GLIP to predict bounding\n",
      "boxes from the generated images and calculate AP, thus we\n",
      "name it as GLIP score . We compare to a state-of-the-art\n",
      "model designed for the layout2img task: LAMA [40]. We\n",
      "train LAMA using the ofﬁcial code on the LVIS training set\n",
      "(in a fully-supervised setting), whereas we directly evaluate\n",
      "our model in a zero-shot task transfer manner, by running\n",
      "inference on the LVIS val set without seeing any LVIS labels.\n",
      "Table 3 (ﬁrst 4 rows) shows the results. Surprisingly, even\n",
      "though our model is only trained on COCO annotations,\n",
      "it outperforms the supervised baseline by a large margin.\n",
      "This is because the baseline, which is trained from scratch,\n",
      "struggles to learn from limited annotations (many of the rare\n",
      "classes in LVIS have fewer than ﬁve training samples). In\n",
      "contrast, our model can take advantage of the pretrained\n",
      "model’s vast concept knowledge.\n",
      "Scaling up the training data. We next study our model’s\n",
      "open-set capability with much larger training data. Specifically, we follow GLIP [34] and train on Object365 [56]\n",
      "and GoldG [34], which combines two grounding datasets:\n",
      "Flickr [47] and VG [31]. We also use CC3M [57] and\n",
      "SBU [44] with grounding pseudo-labels generated by GLIP.\n",
      "Table 3 shows the data scaling results. As we scale up the\n",
      "training data, our model’s zero-shot performance increases,\n",
      "especially for rare concepts. We also try to ﬁnetune the\n",
      "model pretrained on our largest dataset on LVIS and demonRealInputpix2pixHDOurs (w/o caption)Ours (w caption)Figure 7. Keypoint results. Our model generates higher quality\n",
      "images conditioned on keypoints, and it allows to use caption to\n",
      "specify details such as scene or gender.\n",
      "Model FID AP AP 50 AP75\n",
      "pix2pixHD [66] 142.4 15.8 33.7 13.0\n",
      "GLIGEN (w/o caption) 31.02 31.8 53.5 31.0\n",
      "GLIGEN (w caption) 27.34 31.5 52.9 31.0\n",
      "Upper-bound - 62.4 75.0 65.9\n",
      "Table 5. Conditioning with Human Keypoints evaluated on\n",
      "COCO2017 validation set. Upper-bound is calculated on real images scaled to 256\u0002256.\n",
      "strate its performance on Figure 5 right. To demonstrate the\n",
      "generality of our method, we also train our model based on\n",
      "the Stable Diffusion model checkpoint using the largest data.\n",
      "We show some qualitative examples in Figure 8 using this\n",
      "model. Our model gains the grounding ability compared to\n",
      "vanilla Stable Diffusion. We also notice that Stable Diffusion model may overlook certain objects (“ umbrella ” in\n",
      "the second example) due to its use of the CLIP text encoder\n",
      "which tends to focus on global scene properties, and may\n",
      "ignore object-level details [3]. It also struggles to generate\n",
      "spatially counterfactual concepts. By explicitly injecting\n",
      "entity information through grounding tokens, our model can\n",
      "improve the grounding ability in two ways: the referred objects are more likely to appear in the generated images, and\n",
      "the objects reside in the speciﬁed spatial location.\n",
      "5.3. Inpainting Comparison\n",
      "Like other diffusion models, GLIGEN can also work for\n",
      "the inpainting task by replacing the known region with a\n",
      "sample from q(ztjz0)after each sampling step, where z0is\n",
      "the latent representation of an image [51]. One can ground\n",
      "text descriptions to missing regions, as shown in Figure 6. In\n",
      "this setting, however, one may wonder, can we simply use a\n",
      "vanilla text-to-image diffusion model such Stable Diffusion\n",
      "or DALLE2 to ﬁll the missing region by providing the object\n",
      "name as the caption? What are the beneﬁts of having extra\n",
      "grounding inputs in such cases? To answer this, we conduct\n",
      "the following experiment on the COCO dataset: for each\n",
      "image, we randomly mask one object. We then let the model\n",
      "inpaint the missing region. We choose the missing object\n",
      "with three different size ratios with respect to the image:\n",
      "small (1%-3%), median (5%-10%), and large (30%-50%).\n",
      "5000 images are used for each case.\n",
      "8\n",
      "Caption: “Michael Jackson in a black cloth is singing into a microphone”Grounded text: Michael Jackson, black cloth, microphone\n",
      "Caption: “golden hour, a pekingeseis on the beach with an umbrella”Grounded text: Pekingese, umbrella,sea Ours-sample1Ours-sample2Ours-sample3Stable diffusion\n",
      "Caption: “a hen is hatching a huge egg”Grounded text: hen, egg\n",
      "Caption: “an apple and a same size dog”Grounded text: apple, dogFigure 8. Grounded text2image generation. The baseline lacks grounding ability and can also miss objects e.g. “umbrella ” in a sentence\n",
      "with multiple objects due to CLIP text space, and it also struggles to generate spatially counterfactual concepts.\n",
      "Table 4 demonstrates that our inpainted objects more\n",
      "tightly occupy the missing region (box) compared to the\n",
      "baselines. Fig. 6 provides examples to visually compare the\n",
      "inpainting results (we use Stable Diffusion for better quality).\n",
      "The ﬁrst row shows that baselines’ generated objects do not\n",
      "follow the provided box. The second row shows that when\n",
      "the missing category is already present in the image, they\n",
      "may ignore the caption. This is understandable as baselines\n",
      "are trained to generate a whole image following the caption.\n",
      "Our method may be more favorable for editing applications,\n",
      "where a user might want to generate an object that fully ﬁts\n",
      "the missing region or add an instance of a class that already\n",
      "exists in the image.5.4. Keypoints Grounding\n",
      "Although we have thus far demonstrated results with\n",
      "bounding boxes, our approach has ﬂexibility in the grounding condition that it can use for generation. To demonstrate\n",
      "this, we next evaluate our model with another type of grounding condition: human keypoints. We use the COCO2017\n",
      "dataset; details of the tokenization process for keypoints can\n",
      "be found in the supp. We compare with pix2pixHD [66], a\n",
      "classic image-to-image translation model. Since pix2pixHD\n",
      "does not take captions as input, we train two variants of our\n",
      "model: one uses COCO captions, the other does not. In the\n",
      "latter case, null caption is used as input to the cross-attention\n",
      "layer for a fair comparison.\n",
      "9\n",
      "Caption: “a car is in front of the sea”Grounded image: red insets at bottom right\n",
      "Caption: “a brick house in the woods, anime, oil painting”Grounded text: a brick house; Grounded style image: green insets at bottom right \n",
      "Figure 9. Image grounded generation (top) where images can provide more ﬁne-grained details than the text in the caption. Text and\n",
      "image grounded generation (bottom). Text is grounded using the red bounding box, and image is used as style reference.\n",
      "Figure 10. Image grounded Inpainting. One can use reference images to ground holes they want to ﬁll in.\n",
      "Fig. 7 shows the qualitative comparison. Clearly,\n",
      "our method generates much better image quality. For\n",
      "our model trained with captions, we can also specify\n",
      "other details such as the scene (“ A person is skiing\n",
      "down a snowy hill ”) or person’s gender (“ A woman is\n",
      "holding a baby ”). These two inputs complement each\n",
      "other and can enrich a user’s controllability for image creation. We measure keypoint correspondence (similar to the\n",
      "YOLO score for boxes) by running a MaskRCNN [18] keypoint detector on the generated images. Both of our model\n",
      "variants produce similar results; see Table 5.\n",
      "5.5. Image Grounding\n",
      "Image grounded generation. One can also use a reference image to represent a grounded entity as discussed pre-viously. Fig. 9 top row shows qualitative results, which\n",
      "demonstrate that the visual feature can complement details\n",
      "that are hard to describe by language, such as the style and\n",
      "shape of cars.\n",
      "Text and image grounded generation. Besides using either text or image to represent a grounded entity, one can\n",
      "also keep both representations in one model for more creative generation. Figure 9 second row shows text grounded\n",
      "generation with style / tone transfer. Here we ground the\n",
      "text (“a brick house”) with the red bounding box. For the\n",
      "style reference image, we ﬁnd that grounding it to an image\n",
      "corner (green bounding box) or its edge is sufﬁcient. Since\n",
      "the model needs to generate a harmonious style for the entire\n",
      "image, we hypothesize the self-attention layers may broadcast this information to all pixels, thus leading to consistent\n",
      "10\n",
      "style for the entire image.\n",
      "Image grounded inpainting. As we previously demonstrated, one can ground text to missing region for inpainting,\n",
      "one can also ground reference images to missing regions.\n",
      "Figure 10 shows inpainting results grounded on reference images. To remove boundary artifacts, we follow GLIDE [43],\n",
      "and modify the ﬁrst conv layer by adding 5 extra channels (4\n",
      "forz0and 1 for inpainting mask) and make them trainable\n",
      "with the new added layers.\n",
      "5.6. Scheduled Sampling\n",
      "As stated in Eq. (8)and Eq. (10), we can schedule inference time sampling by setting \fto 1 (use extra grounding\n",
      "information) or 0 (reduce to the original pretrained diffusion model). This can make our model exploit different\n",
      "knowledge at different stages.\n",
      "Fig. 11 qualitatively shows the beneﬁts of our scheduled\n",
      "sampling for our model built upon Stable Diffusion. The\n",
      "images in the same row share the same noise and conditional\n",
      "input. The ﬁrst row shows that scheduled sampling can be\n",
      "used to improve image quality, as the original Stable Diffusion model is trained with high quality images. The ﬁrst 20%\n",
      "steps usually are sufﬁcient for setting the overall structure\n",
      "of large objects, and the original Stable Diffusion model can\n",
      "then complete the remaining sampling process with its highquality prior. The second row shows a generation example by\n",
      "our model trained with COCO human keypoint annotations.\n",
      "Since this model is purely trained with human keypoints,\n",
      "the ﬁnal result is biased towards generating a human even\n",
      "if a different object (i.e., robot) is speciﬁed in the caption.\n",
      "However, by using scheduled sampling, we can extend this\n",
      "model to generate other objects with a human-like shape.\n",
      "We do notice that scheduled sampling can decrease the\n",
      "correspondence to box or keypoint if we set the \u001cto be too\n",
      "small. To quantitatively measure this, we evaluate the GLIP\n",
      "score of the generated images on the LVIS dataset (similar to\n",
      "Table 3) and we use GLIGEN based on the Stable Diffusion\n",
      "model. The GLIP AP for \u001c= 0:0(vanilla Stable Diffusion),\n",
      "\u001c= 0:2,\u001c= 0:3,\u001c= 0:5,\u001c= 0:8and\u001c= 1 (GLIGEN )\n",
      "are: 0.3, 2.4, 4.7, 8.9, 10.8. Note that to present the GLIGEN\n",
      "own performance, we set \u001c= 1, i.e., without scheduled\n",
      "sampling, for all results reported in the previous sections.\n",
      "6. Conclusion\n",
      "We proposed GLIGEN for expanding pretrained text2img\n",
      "diffusion models with grounding ability, and demonstrated\n",
      "open-world generalization using bounding boxes as the\n",
      "grounding condition. Our method is simple and effective,\n",
      "and can be easily extended to other conditions e.g., keypoints\n",
      "and reference images. One limitation we noticed is that the\n",
      "generated style or aesthetic distribution can shift after adding\n",
      "the new gated self-attention layers ( e.g., the model sometimes struggles to generate graphics style images when \u001cis\n",
      "Caption: “a cute low poly Shiba Inu”Grounded text: Shiba Inu𝜏=1\n",
      "Caption: “a robot is sitting on a bench”Grounded keypoints: plotted dots on the left figure𝜏=0.2Figure 11. Scheduled Samping. It can improve visual or extend a\n",
      "model trained in one domain (e.g., human) to the others. See text\n",
      "for details.\n",
      "set to 1), which is probably due to the grounding training\n",
      "data being all natural images. We believe adding images\n",
      "from more diverse style distributions or further ﬁnetuning\n",
      "the model with highly aesthetic images could help alleviate\n",
      "this issue.\n",
      "Acknowledgement. We thank Yufan Zhou and Huangjie\n",
      "Zheng for the empirical insights on the FID evaluation of\n",
      "diffusion models, Haotian Zhang for the guidance on largescale grounding data, Ce Liu for the discussion on the potential applications of grounded generative image models. This\n",
      "work was supported in part by NSF CAREER IIS2150012,\n",
      "NASA 80NSSC21K0295, and Institute of Information &\n",
      "communications Technology Planning & Evaluation(IITP)\n",
      "grants funded by the Korea government(MSIT) (No. 20220-00871, Development of AI Autonomy and Knowledge\n",
      "Enhancement for AI Agent Collaboration) and (No. RS2022-00187238, Development of Large Korean Language\n",
      "Model Technology for Efﬁcient Pre-training).\n",
      "References\n",
      "[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\n",
      "Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza\n",
      "Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina\n",
      "Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\n",
      "Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\n",
      "Andrew Zisserman, and Karen Simonyan. Flamingo: a visual\n",
      "language model for few-shot learning. ArXiv , abs/2204.14198,\n",
      "2022. 2, 5, 7\n",
      "[2]Oron Ashual, Shelly Sheynin, Adam Polyak, Uriel Singer,\n",
      "Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knn11\n",
      "diffusion: Image generation via large-scale retrieval. arXiv\n",
      "preprint arXiv:2204.02849 , 2022. 6\n",
      "[3]Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli\n",
      "Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediffi: Text-to-image diffusion models with an ensemble of expert\n",
      "denoisers. ArXiv , abs/2211.01324, 2022. 2, 8\n",
      "[4]Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training\n",
      "of image transformers. arXiv preprint arXiv:2106.08254 ,\n",
      "2021. 2\n",
      "[5]Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark\n",
      "Liao. Yolov4: Optimal speed and accuracy of object detection.\n",
      "ArXiv , abs/2004.10934, 2020. 6\n",
      "[6]Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\n",
      "Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\n",
      "William T Freeman, Michael Rubinstein, et al. Muse: Text-toimage generation via masked generative transformers. arXiv\n",
      "preprint arXiv:2301.00704 , 2023. 2\n",
      "[7]Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W\n",
      "Cohen. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491 , 2022. 6\n",
      "[8]Yunjey Choi, Min-Je Choi, Mun Su Kim, Jung-Woo Ha,\n",
      "Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed generative adversarial networks for multi-domain image-to-image\n",
      "translation. 2018 IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition , pages 8789–8797, 2018. 3\n",
      "[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the\n",
      "2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long and Short Papers) , pages 4171–\n",
      "4186, Minneapolis, Minnesota, June 2019. Association for\n",
      "Computational Linguistics. 3\n",
      "[10] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\n",
      "gans on image synthesis. ArXiv , abs/2105.05233, 2021. 2\n",
      "[11] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang\n",
      "Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia\n",
      "Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers, 2021. 6\n",
      "[12] Wanshu Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng,\n",
      "Lu Yuan, and Yu-Chiang Frank Wang. Frido: Feature pyramid diffusion for complex scene image synthesis. ArXiv ,\n",
      "abs/2208.13753, 2022. 6\n",
      "[13] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\n",
      "Devi Parikh, and Yaniv Taigman. Make-a-scene: Scenebased text-to-image generation with human priors. ArXiv ,\n",
      "abs/2203.13131, 2022. 2, 6\n",
      "[14] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\n",
      "Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\n",
      "and Yoshua Bengio. Generative adversarial nets. In NIPS ,\n",
      "2014. 1, 3\n",
      "[15] Agrim Gupta, Piotr Doll ´ar, and Ross B. Girshick. Lvis: A\n",
      "dataset for large vocabulary instance segmentation. CVPR ,\n",
      "pages 5351–5359, 2019. 2, 8, 15\n",
      "[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\n",
      "Doll´ar, and Ross Girshick. Masked autoencoders are scalablevision learners. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition , pages 16000–\n",
      "16009, 2022. 2\n",
      "[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\n",
      "Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR , 2020. 2\n",
      "[18] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\n",
      "Girshick. Mask r-cnn. 2017 IEEE International Conference\n",
      "on Computer Vision (ICCV) , pages 2980–2988, 2017. 10\n",
      "[19] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep\n",
      "residual learning for image recognition. CVPR , pages 770–\n",
      "778, 2016. 3\n",
      "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two\n",
      "time-scale update rule converge to a local nash equilibrium.\n",
      "InNIPS , 2017. 6\n",
      "[21] Jonathan Ho. Classiﬁer-free diffusion guidance. ArXiv ,\n",
      "abs/2207.12598, 2022. 4, 6, 15\n",
      "[22] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\n",
      "memory. Neural Computation , 9:1735–1780, 1997. 3\n",
      "[23] Ray S Jackendoff. Semantic structures , volume 18. MIT\n",
      "press, 1992. 4\n",
      "[24] Manuel Jahn, Robin Rombach, and Bj ¨orn Ommer. Highresolution complex scene synthesis with transformers. ArXiv ,\n",
      "abs/2105.06458, 2021. 2, 3\n",
      "[25] Manuel Jahn, Robin Rombach, and Bj ¨orn Ommer. Highresolution complex scene synthesis with transformers. ArXiv ,\n",
      "abs/2105.06458, 2021. 7, 16\n",
      "[26] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. 2018 IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition , pages 1219–1228,\n",
      "2018. 2\n",
      "[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based\n",
      "generator architecture for generative adversarial networks.\n",
      "CVPR , pages 4396–4405, 2019. 1\n",
      "[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\n",
      "generator architecture for generative adversarial networks.\n",
      "CVPR , pages 4396–4405, 2019. 3\n",
      "[29] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\n",
      "Jaakko Lehtinen, and Timo Aila. Analyzing and improving\n",
      "the image quality of stylegan. 2020 IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "8107–8116, 2020. 1\n",
      "[30] Diederik P. Kingma and Jimmy Ba. Adam: A method for\n",
      "stochastic optimization. CoRR , abs/1412.6980, 2015. 15\n",
      "[31] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\n",
      "Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\n",
      "Li Fei-Fei. Visual genome: Connecting language and vision\n",
      "using crowdsourced dense image annotations. International\n",
      "Journal of Computer Vision , 123:32–73, 2016. 8\n",
      "[32] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan\n",
      "Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu,\n",
      "Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEV ATER: A\n",
      "benchmark and toolkit for evaluating language-augmented visual models. In NeurIPS Track on Datasets and Benchmarks ,\n",
      "2022. 2\n",
      "12\n",
      "[33] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi. Align\n",
      "before fuse: Vision and language representation learning with\n",
      "momentum distillation. arXiv preprint arXiv:2107.07651 ,\n",
      "2021. 2\n",
      "[34] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan,\n",
      "Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng\n",
      "Gao. Grounded language-image pre-training. In IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pages\n",
      "10955–10965. IEEE, 2022. 2, 6, 8\n",
      "[35] Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang,\n",
      "Xiaodong He, Siwei Lyu, and Jianfeng Gao. Object-driven\n",
      "text-to-image synthesis via adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition , pages 12174–12182, 2019. 6\n",
      "[36] Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae\n",
      "Lee, and Krishna Kumar Singh. Collaging class-speciﬁc gans\n",
      "for semantic image synthesis. ICCV , pages 14398–14407,\n",
      "2021. 3\n",
      "[37] Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae\n",
      "Lee, and Krishna Kumar Singh. Contrastive learning\n",
      "for diverse disentangled foreground generation. ArXiv ,\n",
      "abs/2211.02707, 2022. 2\n",
      "[38] Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, and\n",
      "Yong Jae Lee. Mixnmatch: Multifactor disentanglement and\n",
      "encoding for conditional image generation. 2020 IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR) , pages 8036–8045, 2020. 3\n",
      "[39] Z. Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and\n",
      "Lingyun Sun. Image synthesis from layout with localityaware mask adaption. ICCV , pages 13799–13808, 2021. 2,\n",
      "3\n",
      "[40] Z. Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and\n",
      "Lingyun Sun. Image synthesis from layout with localityaware mask adaption. ICCV , pages 13799–13808, 2021. 6, 7,\n",
      "8, 15, 16\n",
      "[41] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,\n",
      "Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence\n",
      "Zitnick. Microsoft coco: Common objects in context. In\n",
      "ECCV , 2014. 2, 4, 6, 16\n",
      "[42] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\n",
      "Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\n",
      "Representing scenes as neural radiance ﬁelds for view synthesis. In ECCV , 2020. 4, 5, 15\n",
      "[43] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\n",
      "Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\n",
      "Mark Chen. Glide: Towards photorealistic image generation\n",
      "and editing with text-guided diffusion models. In ICML , 2022.\n",
      "2, 11\n",
      "[44] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n",
      "Im2text: Describing images using 1 million captioned photographs. In NIPS , 2011. 8\n",
      "[45] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\n",
      "Zhu. Semantic image synthesis with spatially-adaptive normalization. CVPR , pages 2332–2341, 2019. 1, 3[46] Deepak Pathak, Philipp Kr ¨ahenb ¨uhl, Jeff Donahue, Trevor\n",
      "Darrell, and Alexei A. Efros. Context encoders: Feature\n",
      "learning by inpainting. CVPR , pages 2536–2544, 2016. 1, 2\n",
      "[47] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes,\n",
      "Juan C. Caicedo, J. Hockenmaier, and Svetlana Lazebnik.\n",
      "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. International\n",
      "Journal of Computer Vision , 123:74–93, 2015. 8\n",
      "[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n",
      "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n",
      "Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\n",
      "Krueger, and Ilya Sutskever. Learning transferable visual\n",
      "models from natural language supervision. In ICML , 2021. 2,\n",
      "3\n",
      "[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\n",
      "and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv , abs/2204.06125, 2022. 1, 2,\n",
      "6\n",
      "[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\n",
      "Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\n",
      "Zero-shot text-to-image generation. In Marina Meila and\n",
      "Tong Zhang, editors, Proceedings of the 38th International\n",
      "Conference on Machine Learning , volume 139 of Proceedings\n",
      "of Machine Learning Research , pages 8821–8831. PMLR,\n",
      "18–24 Jul 2021. 1, 2\n",
      "[51] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick\n",
      "Esser, and Bj ¨orn Ommer. High-resolution image synthesis\n",
      "with latent diffusion models. CVPR , pages 10674–10685,\n",
      "2022. 2, 3, 6, 8, 15\n",
      "[52] O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional\n",
      "networks for biomedical image segmentation. In Medical\n",
      "Image Computing and Computer-Assisted Intervention (MICCAI) , volume 9351 of LNCS , pages 234–241. Springer, 2015.\n",
      "(available on arXiv:1505.04597 [cs.CV]). 3, 15\n",
      "[53] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee,\n",
      "Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad\n",
      "Norouzi. Palette: Image-to-image diffusion models. ACM\n",
      "SIGGRAPH 2022 Conference Proceedings , 2022. 2, 3\n",
      "[54] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\n",
      "Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour,\n",
      "Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet,\n",
      "and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv ,\n",
      "abs/2205.11487, 2022. 1, 2, 6\n",
      "[55] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\n",
      "Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\n",
      "Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION400M: open dataset of clip-ﬁltered 400 million image-text\n",
      "pairs. CoRR , abs/2111.02114, 2021. 6\n",
      "[56] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\n",
      "Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\n",
      "large-scale, high-quality dataset for object detection. ICCV ,\n",
      "pages 8429–8438, 2019. 8\n",
      "[57] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\n",
      "Soricut. Conceptual captions: A cleaned, hypernymed, image\n",
      "alt-text dataset for automatic image captioning. In ACL, 2018.\n",
      "8\n",
      "13\n",
      "[58] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing.\n",
      "2020 IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 9240–9249, 2020. 1\n",
      "[59] Wei Sun and Tianfu Wu. Image synthesis from reconﬁgurable\n",
      "layout and style. ICCV , pages 10530–10539, 2019. 2, 3\n",
      "[60] Wei Sun and Tianfu Wu. Learning layout and style reconﬁgurable gans for controllable image synthesis. TPAMI , 44:5070–\n",
      "5087, 2022. 2, 3, 7, 16\n",
      "[61] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon\n",
      "Hjelm, and Shikhar Sharma. Object-centric image generation\n",
      "from layouts. ArXiv , abs/2003.07449, 2021. 2\n",
      "[62] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon\n",
      "Hjelm, and Shikhar Sharma. Object-centric image generation\n",
      "from layouts. ArXiv , abs/2003.07449, 2021. 7, 16\n",
      "[63] Ming Tao, Hao Tang, Songsong Wu, N. Sebe, Fei Wu, and\n",
      "Xiaoyuan Jing. Df-gan: Deep fusion generative adversarial\n",
      "networks for text-to-image synthesis. ArXiv , abs/2008.05865,\n",
      "2020. 3, 6\n",
      "[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\n",
      "Polosukhin. Attention is all you need. In I. Guyon, U. V on\n",
      "Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\n",
      "and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n",
      "3\n",
      "[65] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need. ArXiv ,\n",
      "abs/1706.03762, 2017. 2, 3\n",
      "[66] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\n",
      "Jan Kautz, and Bryan Catanzaro. High-resolution image\n",
      "synthesis and semantic manipulation with conditional gans.\n",
      "2018 IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition , pages 8798–8807, 2018. 8, 9\n",
      "[67] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\n",
      "Daxin Jiang, and Nan Duan. N ¨uwa: Visual synthesis pretraining for neural visual world creation. In European Conference on Computer Vision , 2022. 2, 6\n",
      "[68] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\n",
      "Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Finegrained text to image generation with attentional generative\n",
      "adversarial networks. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1316–1324, 2018.\n",
      "3, 6\n",
      "[69] Zuopeng Yang, Daqing Liu, Chaoyue Wang, J. Yang, and\n",
      "Dacheng Tao. Modeling image composition for complex\n",
      "scene generation. CVPR , pages 7754–7763, 2022. 2, 7, 16\n",
      "[70] Zuopeng Yang, Daqing Liu, Chaoyue Wang, J. Yang, and\n",
      "Dacheng Tao. Modeling image composition for complex\n",
      "scene generation. CVPR , pages 7754–7763, 2022. 2, 3\n",
      "[71] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin\n",
      "Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael\n",
      "Zeng, and Lijuan Wang. Reco: Region-controlled text-toimage generation. ArXiv , abs/2211.15518, 2022. 3\n",
      "[72] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan\n",
      "Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, YinfeiYang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han,\n",
      "Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\n",
      "Yonghui Wu. Scaling autoregressive models for content-rich\n",
      "text-to-image generation. ArXiv , abs/2206.10789, 2022. 1, 2,\n",
      "6\n",
      "[73] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\n",
      "Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\n",
      "Boxin Li, Chunyuan Li, et al. Florence: A new foundation\n",
      "model for computer vision. arXiv preprint arXiv:2111.11432 ,\n",
      "2021. 2\n",
      "[74] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and ShihFu Chang. Open-vocabulary object detection using captions.\n",
      "InProceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition , pages 14393–14402, 2021. 2\n",
      "[75] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and\n",
      "Yinfei Yang. Cross-modal contrastive learning for text-toimage generation, 2021. 6\n",
      "[76] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image\n",
      "generation from layout. CVPR , pages 8576–8585, 2019. 2, 3\n",
      "[77] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan\n",
      "Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang\n",
      "Dai, Lu Yuan, Yin Li, et al. RegionCLIP: Region-based\n",
      "language-image pretraining. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition ,\n",
      "pages 16793–16803, 2022. 2\n",
      "[78] Yufan Zhou, Chunyuan Li, Changyou Chen, Jianfeng Gao,\n",
      "and Jinhui Xu. Laﬁte2: Few-shot text-to-image generation.\n",
      "arXiv preprint arXiv:2210.14124 , 2022. 3, 6\n",
      "[79] Yufan Zhou, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou\n",
      "Chen, and Jinhui Xu. Shifted diffusion for text-to-image\n",
      "generation. arXiv preprint arXiv:2211.15388 , 2022. 2\n",
      "[80] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,\n",
      "Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong\n",
      "Sun. Towards language-free training for text-to-image generation. CVPR , 2022. 1, 6\n",
      "[81] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.\n",
      "Unpaired image-to-image translation using cycle-consistent\n",
      "adversarial networks. In Computer Vision (ICCV), 2017 IEEE\n",
      "International Conference on , 2017. 3\n",
      "14\n",
      "Appendix\n",
      "In this supplemental material, we ﬁrst provide more implementation and training details, and then present more\n",
      "results and discussions.\n",
      "A. Implementation and training details\n",
      "We use the Stable Diffusion model [51] as the example\n",
      "to illustrate our implementation details.\n",
      "Box Grounding Tokens with Text. Each grounded text\n",
      "is ﬁrst fed into the text encoder to get the text embedding\n",
      "(e.g., 768 dimension of the CLIP text embedding in Stable\n",
      "Diffusion). Since the Stable Diffusion uses features of 77\n",
      "text tokens outputted from the transformer backbone, thus\n",
      "we choose “ EOS” token feature at this layer as our grounded\n",
      "text embedding. This is because in the CLIP training, this\n",
      "“EOS” token feature is chosen and applied a linear transform\n",
      "(one FC layer) to compare with visual feature, thus this\n",
      "token feature should contain whole information about the\n",
      "input text description. We also tried to directly use CLIP\n",
      "text embedding ( after linear projection), however, we notice\n",
      "slow convergence empirically probably due to unaligned\n",
      "space between the grounded text embedding and the caption\n",
      "embeddings. Following NeRF [42], we encode bounding\n",
      "box coordinates with the Fourier embedding with output\n",
      "dimension 64. As stated in the Eq (5)in the main paper,\n",
      "we ﬁrst concatenate these two features and feed them into a\n",
      "multi-layer perceptron. The MLP consists of three hidden\n",
      "layers with hidden dimension 512, the output grounding\n",
      "token dimension is set to be the same as the text embedding\n",
      "dimension ( e.g., 768 in the Stable Diffusion case). We set\n",
      "the maximum number of grounding tokens to be 30 in the\n",
      "bounding box case.\n",
      "Box Grounding Tokens with Image. We use the similar\n",
      "way to get the grounding token for an image. We use the\n",
      "CLIP image encoder (ViT-L-14 is used for the Stable Diffusion) to get an image embedding. We denote the CLIP\n",
      "training objective as maximizing (Ptht)>(Pihi)(we omit\n",
      "normalization), where htis “EOS” token embedding from\n",
      "the text encoder, hiis “CLS” token embedding from the\n",
      "image encoder, and PtandPiare linear transformation for\n",
      "text and image embedding, respectively. Since htis the\n",
      "text feature space used for grounded text features, to ease\n",
      "our training, we choose to project image features into the\n",
      "text feature space via P>\n",
      "tPihi, and normalized it to 28.7,\n",
      "which is average norm of htwe empirically found. We\n",
      "also set the maximum number of grounding tokens to be 30.\n",
      "Thus, 60 tokens in total if one keep both image and text as\n",
      "representations for a grounded entity.\n",
      "Keypoint Grounding Tokens. The grounding token for\n",
      "keypoint annotations is processed in the same way, ex-cept that we also learn Nperson token embedding vectors\n",
      "fp1;:::;pNgto semantically link keypoints belonging to\n",
      "the same person. This is to deal with the situation in which\n",
      "there are multiple people in the same image that we want\n",
      "to generate, so that the model knows which keypoint corresponds to which person. Each keypoint semantic embedding\n",
      "ftext(e)is processed by using the text encoder, for example,\n",
      "we forward the text: “ left eye ” into the encoder to get\n",
      "its semantic embedding; the dimension of each person token\n",
      "is set the same as text embedding dimension. The grounding\n",
      "token is calculated by:\n",
      "he=MLP (ftext(e) +pj;Fourier (l)) (11)\n",
      "wherelis thex;ylocation of each keypoint and pjis the\n",
      "person token for the j’th person. In practice, we set Nas\n",
      "10, which is the maximum number of persons allowed to be\n",
      "generated in each image. Thus, we have 170 tokens in the\n",
      "COCO dataset ( i.e., 10*17; 17 keypoint annotations for each\n",
      "person).\n",
      "Gated Self-Attention Layers. Our inserted self-attention\n",
      "layer is the same as the original diffusion model selfattention layer at each Transformer block, except that we\n",
      "add one linear projection layer which converts the grounding\n",
      "token into the same dimension as the visual token. For example, in the ﬁrst layer of the down branch of the UNet [52], the\n",
      "projection layer converts grounding token of dimension 768\n",
      "into 320 (which is the image feature dimension at this layer),\n",
      "and visual tokens are concatenated with the grounding tokens as the input to the gated attention layer as illustrated in\n",
      "Figure 3.\n",
      "Training Details. For all COCO related experiments\n",
      "(Sec 5.1), we train LDM with batch size 64 using 16 V100\n",
      "GPUs for 100k iterations. In the scaling up training data\n",
      "experiment (in Sec. 5.2 of the main paper), we train for 400k\n",
      "iterations for LDM, but 500K iterations with batch size of\n",
      "32 for the Stable diffusion modeL For all training, we use\n",
      "learning rate of 5e-5 with Adam [30], and use warm-up\n",
      "for the ﬁrst 10k iterations. We randomly drop caption and\n",
      "grounding tokens with 10% probability for classiﬁer-free\n",
      "guidance [21].\n",
      "B. Additional quantitative results\n",
      "In this section, we show more studies with our pretrained\n",
      "model using our largest data (GoldG, O365, CC3M, SBU).\n",
      "We had reported this model’s zero-shot performance on\n",
      "LVIS [15] in the main paper Table 3. Here we ﬁnetune\n",
      "this model on LVIS, and report its GLIP-score in Table 6.\n",
      "Clearly, after ﬁnetuning, we show much more accurate generation results, surpassing the supervised baseline LAMA [40]\n",
      "by a large margin.\n",
      "Similarly, we also test this model’s zero-shot performance\n",
      "on the COCO2017 val-set, and its ﬁnetuning results are in\n",
      "15\n",
      "Model Pre-training data Traing data FID AP AP r APc APf\n",
      "LAMA [40] – LVIS 151.96 2.0 0.9 1.3 3.2\n",
      "GLIGEN -LDM COCO2014CD – 22.17 6.4 5.8 5.8 7.4\n",
      "GLIGEN -LDM COCO2014D – 31.31 4.4 2.3 3.3 6.5\n",
      "GLIGEN -LDM COCO2014G – 13.48 6.0 4.4 6.1 6.6\n",
      "GLIGEN -LDM GoldG,O365 – 8.45 10.6 5.8 9.6 13.8\n",
      "GLIGEN -LDM GoldG,O365,SBU,CC3M – 10.28 11.1 9.0 9.8 13.4\n",
      "GLIGEN -LDM GoldG,O365,SBU,CC3M LVIS 6.25 14.9 10.1 12.8 19.3\n",
      "Upper-bound – – – 25.2 19.0 22.2 31.2\n",
      "Table 6. GLIP-score on LVIS validation set. Upper-bound is provided by running GLIP on real images scaled to 256 \u0002256.\n",
      "mountainperson horsegrassbuilding bus car bear mountain rock InputLostGAN-v2TwFAOursLDMOursStable\n",
      "Figure 12. Layout2img comparison. Our model generates better\n",
      "quality images, especially when using stable diffusion. Baseline\n",
      "images are all copied from TwFA [69]\n",
      "Table 7. The results show the beneﬁts of pretraining which\n",
      "can largely improve layout correspondence performance.\n",
      "C. More qualitative results and discussion\n",
      "We show qualitative comparisons with layout2img baselines in Figure 12, which complements the results in Sec 5.1\n",
      "of the main paper. The results show that our model has\n",
      "comparable image quality when built upon LDM, but has\n",
      "more visual appeal and details when built upon the Stable\n",
      "Diffusion model.YOLO score\n",
      "Model FID AP AP 50 AP75\n",
      "LostGAN-V2 [60] 42.55 9.1 15.3 9.8\n",
      "OCGAN [62] 41.65 –\n",
      "HCSS [25] 33.68 –\n",
      "LAMA [40] 31.12 13.40 19.70 14.90\n",
      "TwFA [69] 22.15 – 28.20 20.12\n",
      "GLIGEN -LDM 21.04 22.4 36.5 24.1\n",
      "After pretrain on GoldG,O365,SBU,CC3M\n",
      "GLIGEN -LDM ( zero-shot ) 27.03 19.1 30.5 20.8\n",
      "GLIGEN -LDM ( ﬁnetuned ) 21.58 30.8 42.3 35.3\n",
      "Table 7. Image quality and correspondence to layout are compared\n",
      "with baselines on COCO2017 val-set.\n",
      "Lastly, we show more grounded text2img results with\n",
      "bounding boxes in Figure 13 and with images or keypoints\n",
      "in Figure 14. Note that our keypoint model only uses keypoint annotations from COCO [41] which is not linked with\n",
      "person identity, but it can successfully utilize and combine\n",
      "the knowledge learned in the text2img training stage to control keypoints of a speciﬁc person. Out of curiosity, we also\n",
      "tested whether the keypoint grounding information learned\n",
      "on humans can be transferred to other non-humanoid categories such as cat or lamp for keypoint grounded generation,\n",
      "but we ﬁnd that our model struggles in such cases even with\n",
      "scheduled sampling. Compared to bounding boxes, which\n",
      "only specify a coarse location and size of an object in the\n",
      "image and thus can be shared across all object categories,\n",
      "keypoints (i.e., object parts) are not always shareable across\n",
      "different categories. Thus, while keypoints enable more\n",
      "ﬁne-grained control than boxes, they are less generalizable.\n",
      "16\n",
      "Caption: “Space view of a planet and its sun”Grounded text: planet, sun\n",
      "Caption: “a a photo of a hybrid between a bee and a rabbit”Grounded text: hybrid between a bee and a rabbit, flower\n",
      "Caption: “cartoon sketch of a little girl with a smile and balloons, old style, detailed, elegant, intricate”Grounded text: girl with a smile, balloon, balloon, balloon\n",
      "Caption: “Walter White in GTA v”Grounded text: Walter White, car,bulldog\n",
      "Caption: “two pirate ships on the ocean in minecraft”Grounded text: a pirate ship, a pirate shipFigure 13. Bounding box grounded text2image generation. Our model can ground noun entities in the caption for controllable image\n",
      "generation\n",
      "17\n",
      "Caption: “Barack Obama is sitting at a desk”Grounded keypoints: plotted dots on the left\n",
      "Caption: “Steve Jobs is working with his laptop”Grounded keypoints: plotted dots on the left\n",
      "Caption: “Pikachu is under a tree, digital art”Grounded text: Pikachu,tree; Grounded image: blue inset\n",
      "Figure 14. Top row: text grounded generation with style transfer. Second row: image grounded inpainting. Bottom rows: Keypoint grounded\n",
      "text2image generation. We did not use keypoint annotation associated with any identity during training.\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "clean_paper = clean_text(paper)\n",
    "print(clean_paper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as for number of paragraphs — if we split on `\"\\n\"` we will get something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_chunks = clean_paper.split(\"\\n\")\n",
    "len(paper_chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper contains `1198` 'paragraphs' in total. Naturally we know that most of these are not actual paragraphs, because a common thing that happens with PDFs is that we will find newlines being added in the middle of a sentence. This isn't ideal but it can work for a first run, maybe we will optimize in the future and try to handle in the `clean_text` function.\n",
    "\n",
    "We will iteratively add these together until exceeding the limit of `650` tokens decided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 650\n",
    "\n",
    "for i in range(len(paper_chunks)):\n",
    "    chunk = \"\\n\".join(paper_chunks[:i-1])\n",
    "    if len(tokenizer.encode(\"\\n\".join(paper_chunks[:i]))) > max_tokens:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIGEN : Open-Set Grounded Text-to-Image Generation\n",
      "Yuheng Li1x, Haotian Liu1x, Qingyang Wu2, Fangzhou Mu1, Jianwei Yang3, Jianfeng Gao3,\n",
      "Chunyuan Li3{, Yong Jae Lee1{\n",
      "1University of Wisconsin-Madison2Columbia University3Microsoft\n",
      "https://gligen.github.io/\n",
      "Caption: “A woman sitting in a restaurant with a pizza in front of her ”Grounded text: table, pizza, person, wall, car, paper, chair, window, bottle, cup\n",
      "Caption: “a baby girl/monkey is scratching her/its head”Grounded keypoints: plotted dots on the left\n",
      "Caption: “A bird/helmet is on the grass”Grounded image: red inset\n",
      "Caption: “Elon Musk and Emma Watson on a movie poster”Grounded text: Elon Musk, Emma Watson; Grounded style image: blue inset\n",
      "Figure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model, by feeding different grounding\n",
      "conditions. GLIGEN supports (a) text entity + box, (b) image entity + box, (c) image style and text + box, (d) text entity + keypoints. The\n",
      "generated examples for each scenario are shown in top-left, top-right, bottom-left, and bottom-right, respectively.\n",
      "Abstract\n",
      "Large-scale text-to-image diffusion models have made\n",
      "amazing advances. However, the status quo is to use\n",
      "text input alone, which can impede controllability. In this\n",
      "work, we propose GLIGEN ,Grounded- Language-to- Image\n",
      "Generation, a novel approach that builds upon and extends\n",
      "the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on\n",
      "grounding inputs. To preserve the vast concept knowledge of\n",
      "the pre-trained model, we freeze all of its weights and inject\n",
      "the grounding information into new trainable layers via a\n",
      "gated mechanism. Our model achieves open-world grounded\n",
      "text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to\n",
      "novel spatial conﬁgurations and concepts. GLIGEN ’s zeroshot performance on COCO and LVIS outperforms existing\n",
      "supervised layout-to-image baselines by a large margin.\n",
      "xPart of the work performed at Microsoft; {Co-senior authors1. Introduction\n",
      "Image generation research has witnessed huge advances\n",
      "in recent years. Over the past couple of years, GANs [14]\n",
      "were the state-of-the-art, with their latent space and conditional inputs being well-studied for controllable manipulation [46, 58] and generation [27, 29, 45, 80]. Text conditional autoregressive [50, 72] and diffusion [49, 54] models\n",
      "have demonstrated astonishing image quality and concept\n"
     ]
    }
   ],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(chunk))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add an overlap of ~3 chunks. We can see how this performs by running this across more chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GLIGEN : Open-Set Grounded Text-to-Image Generation',\n",
       " 'Yuheng Li1x, Haotian Liu1x, Qingyang Wu2, Fangzhou Mu1, Jianwei Yang3, Jianfeng Gao3,',\n",
       " 'Chunyuan Li3{, Yong Jae Lee1{']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(paper: str):\n",
    "    # clean and split into initial smaller chunks\n",
    "    clean_paper = clean_text(paper)\n",
    "    paper_chunks = clean_paper.split(\"\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for i in range(len(paper_chunks)):\n",
    "        # add the next chunk to the current chunk\n",
    "        current_chunk.append(paper_chunks[i])\n",
    "        if len(tokenizer.encode(\"\\n\".join(current_chunk), disallowed_special=())) > max_tokens:\n",
    "            # join the current chunks minus the final chunk that pushed us over limit\n",
    "            chunks.append(\"\\n\".join(current_chunk[:-1]))\n",
    "            # reset the current chunk to the final few chunks (overlap)\n",
    "            current_chunk = current_chunk[-4:]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GLIGEN : Open-Set Grounded Text-to-Image Generation\\nYuheng Li1x, Haotian Liu1x, Qingyang Wu2, Fangzhou Mu1, Jianwei Yang3, Jianfeng Gao3,\\nChunyuan Li3{, Yong Jae Lee1{\\n1University of Wisconsin-Madison2Columbia University3Microsoft\\nhttps://gligen.github.io/\\nCaption: “A woman sitting in a restaurant with a pizza in front of her ”Grounded text: table, pizza, person, wall, car, paper, chair, window, bottle, cup\\nCaption: “a baby girl/monkey is scratching her/its head”Grounded keypoints: plotted dots on the left\\nCaption: “A bird/helmet is on the grass”Grounded image: red inset\\nCaption: “Elon Musk and Emma Watson on a movie poster”Grounded text: Elon Musk, Emma Watson; Grounded style image: blue inset\\nFigure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model, by feeding different grounding\\nconditions. GLIGEN supports (a) text entity + box, (b) image entity + box, (c) image style and text + box, (d) text entity + keypoints. The\\ngenerated examples for each scenario are shown in top-left, top-right, bottom-left, and bottom-right, respectively.\\nAbstract\\nLarge-scale text-to-image diffusion models have made\\namazing advances. However, the status quo is to use\\ntext input alone, which can impede controllability. In this\\nwork, we propose GLIGEN ,Grounded- Language-to- Image\\nGeneration, a novel approach that builds upon and extends\\nthe functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on\\ngrounding inputs. To preserve the vast concept knowledge of\\nthe pre-trained model, we freeze all of its weights and inject\\nthe grounding information into new trainable layers via a\\ngated mechanism. Our model achieves open-world grounded\\ntext2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to\\nnovel spatial conﬁgurations and concepts. GLIGEN ’s zeroshot performance on COCO and LVIS outperforms existing\\nsupervised layout-to-image baselines by a large margin.\\nxPart of the work performed at Microsoft; {Co-senior authors1. Introduction\\nImage generation research has witnessed huge advances\\nin recent years. Over the past couple of years, GANs [14]\\nwere the state-of-the-art, with their latent space and conditional inputs being well-studied for controllable manipulation [46, 58] and generation [27, 29, 45, 80]. Text conditional autoregressive [50, 72] and diffusion [49, 54] models\\nhave demonstrated astonishing image quality and concept',\n",
       " 'in recent years. Over the past couple of years, GANs [14]\\nwere the state-of-the-art, with their latent space and conditional inputs being well-studied for controllable manipulation [46, 58] and generation [27, 29, 45, 80]. Text conditional autoregressive [50, 72] and diffusion [49, 54] models\\nhave demonstrated astonishing image quality and concept\\ncoverage, due to their more stable learning objectives and\\nlarge-scale training on web image-text paired data. These\\nmodels have gained attention even among the general public\\ndue to their practical use cases ( e.g., art design and creation).\\nDespite exciting progress, existing large-scale text-toimage generation models cannot be conditioned on other\\ninput modalities apart from text, and thus lack the ability\\nto precisely localize concepts or use reference images to\\ncontrol the generation process. The current input, i.e., natural language alone, restricts the way that information can\\n1arXiv:2301.07093v1  [cs.CV]  17 Jan 2023\\nbe expressed. For example, it is difﬁcult to describe the\\nprecise location of an object using text, whereas bounding\\nboxes / keypoints can easily achieve this, as shown in Figure 1. While conditional diffusion models [10, 51, 53] and\\nGANs [26, 37, 46, 69] that take in input modalities other\\nthan text for inpainting, layout2img generation, etc., do exist,\\nthey rarely combine those inputs for controllable text2img\\ngeneration.\\nMoreover, prior generative models—regardless of the\\ngenerative model family—are usually independently trained\\non each task-speciﬁc dataset. In contrast, in the recognition ﬁeld, the long-standing paradigm has been to build a\\ntask-speciﬁc recognition model [32] by starting from a foundation model pretrained on large-scale image data [4, 16, 17]\\nor image-text pairs [33, 48, 73]. Since diffusion models have\\nbeen trained on billions of image-text pairs [51], a natural\\nquestion is: Can we build upon existing pretrained diffusion models and endow them with new conditional input\\nmodalities? In this way, analogous to the recognition literature, we may be able to achieve better performance on other\\ngeneration tasks due to the vast concept knowledge that the\\npretrained models have, while acquiring more controllability\\nover existing text-to-image generation models.\\nWith the above aims, we propose a method for providing\\nnew grounding conditional inputs to pretrained text-to-image\\ndiffusion models. As shown in Figure 1, we still retain the\\ntext caption as input, but also enable other input modalities\\nsuch as bounding boxes for grounding concepts, grounding\\nreference images, and grounding part keypoints. The key\\nchallenge is preserving the original vast concept knowledge\\nin the pretrained model while learning to inject the new',\n",
       " 'reference images, and grounding part keypoints. The key\\nchallenge is preserving the original vast concept knowledge\\nin the pretrained model while learning to inject the new\\ngrounding information. To prevent knowledge forgetting,\\nwe propose to freeze the original model weights and add\\nnew trainable gated Transformer layers [65] that take in the\\nnew grounding input ( e.g., bounding box). During training,\\nwe gradually fuse the new grounding information into the\\npretrained model using a gated mechanism [1]. This design\\nenables ﬂexibility in the sampling process during generation\\nfor improved quality and controllability; for example, we\\nshow that using the full model (all layers) in the ﬁrst half of\\nthe sampling steps and only using the original layers (without\\nthe gated Transformer layers) in the latter half can lead\\nto generation results that accurately reﬂect the grounding\\nconditions while also having high image quality.\\nIn our experiments, we primarily study grounded\\ntext2img generation with bounding boxes, inspired by the\\nrecent scaling success of learning grounded language-image\\nunderstanding models with boxes in GLIP [34]. To enable our model to ground open-world vocabulary concepts [32,34,74,77], we use the same pre-trained text encoder\\n(for encoding the caption) to encode each phrase associated\\nwith each grounded entity ( i.e., one phrase per bounding\\nbox) and feed the encoded tokens into the newly insertedlayers with their encoded location information. Due to the\\nshared text space, we ﬁnd that our model can generalize to\\nunseen objects even when only trained on the COCO [41]\\ndataset. Its generalization on LVIS [15] outperforms a strong\\nfully-supervised baseline by a large margin. To further improve our model’s grounding ability, we unify the object\\ndetection and grounding data formats for training, following\\nGLIP [34], as they provide complementary beneﬁts: detection data is of larger quantity, while grounding data has a\\nricher vocabulary. With larger training data, our model’s\\ngeneralization is consistently improved.\\nContributions. 1) We propose a new text2img generation\\nmethod that endows new grounding controllability over existing text2img diffusion models. 2) By preserving the pretrained weights and learning to gradually integrate the new\\nlocalization layers, our model achieves open-world grounded\\ntext2img generation with bounding box inputs, i.e., synthesis\\nof novel localized concepts unobserved in training. 3) Our\\nmodel’s zero-shot performance on layout2img tasks signiﬁcantly outperforms the prior state-of-the-art, demonstrating\\nthe power of building upon large pretrained generative models for downstream tasks.\\n2. Related Work\\nLarge scale text-to-image generation models. State-ofthe-art models in this space are either autoregressive [13, 50,']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_chunks = chunker(paper)\n",
    "paper_chunks[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These chunks seem reasonable, we will apply this `chunker` function across all of our papers to create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd85d2bc952345e5ac1ea92c12bdd13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "26352"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for paper_path in tqdm(paper_paths):\n",
    "    doi = paper_path.split('/')[-1][:-4]\n",
    "    with open(paper_path) as f:\n",
    "        paper = f.read()\n",
    "    paper_chunks = chunker(paper)\n",
    "    for i, chunk in enumerate(paper_chunks):\n",
    "        dataset.append({\n",
    "            'doi': doi,\n",
    "            'chunk-id': i,\n",
    "            'chunk': chunk\n",
    "        })\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that none of these exceed the token limit set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk too long: 827\n",
      "Chunk too long: 1630\n",
      "Chunk too long: 2001\n",
      "Chunk too long: 1624\n",
      "Chunk too long: 1634\n",
      "Chunk too long: 827\n",
      "Chunk too long: 810\n",
      "Chunk too long: 1602\n",
      "Chunk too long: 1988\n",
      "Chunk too long: 1611\n",
      "Chunk too long: 1598\n",
      "Chunk too long: 795\n",
      "Chunk too long: 836\n",
      "Chunk too long: 1235\n",
      "Chunk too long: 877\n",
      "Chunk too long: 878\n",
      "Chunk too long: 812\n",
      "Chunk too long: 1181\n",
      "Chunk too long: 1549\n",
      "Chunk too long: 1913\n",
      "Chunk too long: 1508\n",
      "Chunk too long: 1160\n",
      "Chunk too long: 796\n",
      "Chunk too long: 690\n",
      "Chunk too long: 690\n",
      "Chunk too long: 687\n",
      "Chunk too long: 919\n",
      "Chunk too long: 921\n",
      "Chunk too long: 923\n",
      "Chunk too long: 931\n",
      "Chunk too long: 1170\n",
      "Chunk too long: 1544\n",
      "Chunk too long: 1910\n",
      "Chunk too long: 1912\n",
      "Chunk too long: 1528\n",
      "Chunk too long: 1913\n",
      "Chunk too long: 1917\n",
      "Chunk too long: 1912\n",
      "Chunk too long: 1904\n",
      "Chunk too long: 1523\n",
      "Chunk too long: 1531\n",
      "Chunk too long: 1158\n",
      "Chunk too long: 783\n",
      "Chunk too long: 802\n",
      "Chunk too long: 823\n",
      "Chunk too long: 848\n",
      "Chunk too long: 860\n",
      "Chunk too long: 751\n",
      "Chunk too long: 904\n",
      "Chunk too long: 1499\n",
      "Chunk too long: 1503\n",
      "Chunk too long: 2125\n",
      "Chunk too long: 2140\n",
      "Chunk too long: 1188\n",
      "Chunk too long: 1209\n",
      "Chunk too long: 745\n",
      "Chunk too long: 1289\n",
      "Chunk too long: 1781\n",
      "Chunk too long: 1888\n",
      "Chunk too long: 1295\n",
      "Chunk too long: 655\n",
      "Chunk too long: 1470\n",
      "Chunk too long: 1657\n",
      "Chunk too long: 1677\n",
      "Chunk too long: 1686\n",
      "Chunk too long: 1379\n",
      "Chunk too long: 1387\n",
      "Chunk too long: 1389\n",
      "Chunk too long: 1392\n",
      "Chunk too long: 673\n",
      "Chunk too long: 976\n",
      "Chunk too long: 1418\n",
      "Chunk too long: 1932\n",
      "Chunk too long: 1933\n",
      "Chunk too long: 1084\n",
      "Chunk too long: 1827\n",
      "Chunk too long: 2721\n",
      "Chunk too long: 3221\n",
      "Chunk too long: 3232\n",
      "Chunk too long: 3442\n",
      "Chunk too long: 3187\n",
      "Chunk too long: 2986\n",
      "Chunk too long: 3052\n",
      "Chunk too long: 3094\n",
      "Chunk too long: 3184\n",
      "Chunk too long: 3241\n",
      "Chunk too long: 3160\n",
      "Chunk too long: 2914\n",
      "Chunk too long: 2976\n",
      "Chunk too long: 3195\n",
      "Chunk too long: 2981\n",
      "Chunk too long: 2464\n",
      "Chunk too long: 1674\n",
      "Chunk too long: 922\n",
      "Chunk too long: 737\n",
      "Chunk too long: 1166\n",
      "Chunk too long: 1441\n",
      "Chunk too long: 1511\n",
      "Chunk too long: 1502\n",
      "Chunk too long: 1408\n",
      "Chunk too long: 1423\n",
      "Chunk too long: 1348\n",
      "Chunk too long: 1387\n",
      "Chunk too long: 1379\n",
      "Chunk too long: 1386\n",
      "Chunk too long: 1446\n",
      "Chunk too long: 1442\n",
      "Chunk too long: 1502\n",
      "Chunk too long: 1468\n",
      "Chunk too long: 1375\n",
      "Chunk too long: 1065\n",
      "Chunk too long: 679\n",
      "Chunk too long: 1027\n",
      "Chunk too long: 824\n",
      "Chunk too long: 1168\n",
      "Chunk too long: 1941\n",
      "Chunk too long: 2729\n",
      "Chunk too long: 2330\n",
      "Chunk too long: 1962\n",
      "Chunk too long: 1193\n",
      "Chunk too long: 999\n",
      "Chunk too long: 1358\n",
      "Chunk too long: 1383\n",
      "Chunk too long: 1057\n",
      "Chunk too long: 754\n",
      "Chunk too long: 994\n",
      "Chunk too long: 1623\n",
      "Chunk too long: 1781\n",
      "Chunk too long: 1932\n",
      "Chunk too long: 1824\n",
      "Chunk too long: 1733\n",
      "Chunk too long: 1012\n",
      "Chunk too long: 1037\n",
      "Chunk too long: 1072\n",
      "Chunk too long: 972\n",
      "Chunk too long: 1373\n",
      "Chunk too long: 2260\n",
      "Chunk too long: 2248\n",
      "Chunk too long: 2291\n",
      "Chunk too long: 2368\n",
      "Chunk too long: 1979\n",
      "Chunk too long: 2025\n",
      "Chunk too long: 2020\n",
      "Chunk too long: 1560\n",
      "Chunk too long: 1093\n",
      "Chunk too long: 2068\n",
      "Chunk too long: 2050\n",
      "Chunk too long: 2042\n",
      "Chunk too long: 1977\n",
      "Chunk too long: 1016\n",
      "Chunk too long: 1012\n",
      "Chunk too long: 994\n",
      "Chunk too long: 992\n",
      "Chunk too long: 992\n",
      "Chunk too long: 1576\n",
      "Chunk too long: 1576\n",
      "Chunk too long: 1631\n",
      "Chunk too long: 1631\n",
      "Chunk too long: 1623\n",
      "Chunk too long: 1623\n",
      "Chunk too long: 1000\n",
      "Chunk too long: 1476\n",
      "Chunk too long: 658\n",
      "Chunk too long: 657\n",
      "Chunk too long: 1777\n",
      "Chunk too long: 1302\n",
      "Chunk too long: 1465\n",
      "Chunk too long: 3148\n",
      "Chunk too long: 4682\n",
      "Chunk too long: 4699\n",
      "Chunk too long: 4481\n",
      "Chunk too long: 3692\n",
      "Chunk too long: 2792\n",
      "Chunk too long: 4603\n",
      "Chunk too long: 4362\n",
      "Chunk too long: 4201\n",
      "Chunk too long: 5545\n",
      "Chunk too long: 6369\n",
      "Chunk too long: 5530\n",
      "Chunk too long: 5982\n",
      "Chunk too long: 4017\n",
      "Chunk too long: 1469\n",
      "Chunk too long: 1305\n",
      "Chunk too long: 1190\n",
      "Chunk too long: 1607\n",
      "Chunk too long: 1625\n",
      "Chunk too long: 1249\n",
      "Chunk too long: 1169\n",
      "Chunk too long: 1556\n",
      "Chunk too long: 3487\n",
      "Chunk too long: 4250\n",
      "Chunk too long: 3119\n",
      "Chunk too long: 2754\n",
      "Chunk too long: 831\n",
      "Chunk too long: 804\n",
      "Chunk too long: 1581\n",
      "Chunk too long: 1577\n",
      "Chunk too long: 1210\n",
      "Chunk too long: 830\n",
      "Chunk too long: 687\n",
      "Chunk too long: 694\n",
      "Chunk too long: 699\n",
      "Chunk too long: 701\n",
      "Chunk too long: 691\n",
      "Chunk too long: 692\n",
      "Chunk too long: 707\n",
      "Chunk too long: 707\n",
      "Chunk too long: 693\n",
      "Chunk too long: 690\n",
      "Chunk too long: 689\n",
      "Chunk too long: 694\n",
      "Chunk too long: 703\n",
      "Chunk too long: 704\n",
      "Chunk too long: 693\n",
      "Chunk too long: 693\n",
      "Chunk too long: 706\n",
      "Chunk too long: 708\n",
      "Chunk too long: 691\n",
      "Chunk too long: 693\n",
      "Chunk too long: 684\n",
      "Chunk too long: 692\n",
      "Chunk too long: 693\n",
      "Chunk too long: 695\n",
      "Chunk too long: 691\n",
      "Chunk too long: 693\n",
      "Chunk too long: 705\n",
      "Chunk too long: 712\n",
      "Chunk too long: 690\n",
      "Chunk too long: 689\n",
      "Chunk too long: 1032\n",
      "Chunk too long: 1027\n",
      "Chunk too long: 1036\n",
      "Chunk too long: 1047\n",
      "Chunk too long: 1056\n",
      "Chunk too long: 1516\n",
      "Chunk too long: 1493\n",
      "Chunk too long: 1411\n",
      "Chunk too long: 657\n",
      "Chunk too long: 666\n",
      "Chunk too long: 673\n",
      "Chunk too long: 700\n",
      "Chunk too long: 966\n",
      "Chunk too long: 964\n",
      "Chunk too long: 979\n",
      "Chunk too long: 828\n",
      "Chunk too long: 1292\n",
      "Chunk too long: 1359\n",
      "Chunk too long: 1226\n",
      "Chunk too long: 768\n",
      "Chunk too long: 810\n",
      "Chunk too long: 1179\n",
      "Chunk too long: 1559\n",
      "Chunk too long: 1544\n",
      "Chunk too long: 1538\n",
      "Chunk too long: 1553\n",
      "Chunk too long: 1560\n",
      "Chunk too long: 1547\n",
      "Chunk too long: 1545\n",
      "Chunk too long: 1542\n",
      "Chunk too long: 1526\n",
      "Chunk too long: 1531\n",
      "Chunk too long: 1166\n",
      "Chunk too long: 800\n",
      "Chunk too long: 1374\n",
      "Chunk too long: 1372\n",
      "Chunk too long: 1375\n",
      "Chunk too long: 1361\n",
      "Chunk too long: 655\n",
      "Chunk too long: 669\n",
      "Chunk too long: 865\n",
      "Chunk too long: 862\n",
      "Chunk too long: 1104\n",
      "Chunk too long: 1104\n",
      "Chunk too long: 1104\n",
      "Chunk too long: 1107\n",
      "Chunk too long: 1110\n",
      "Chunk too long: 1107\n",
      "Chunk too long: 1110\n",
      "Chunk too long: 1110\n",
      "Chunk too long: 1110\n",
      "Chunk too long: 1113\n",
      "Chunk too long: 1355\n",
      "Chunk too long: 1356\n",
      "Chunk too long: 1351\n",
      "Chunk too long: 1354\n",
      "Chunk too long: 1445\n",
      "Chunk too long: 1869\n",
      "Chunk too long: 2753\n",
      "Chunk too long: 2742\n",
      "Chunk too long: 1836\n",
      "Chunk too long: 1843\n",
      "Chunk too long: 1849\n",
      "Chunk too long: 1837\n",
      "Chunk too long: 2299\n",
      "Chunk too long: 2776\n",
      "Chunk too long: 2344\n",
      "Chunk too long: 2329\n",
      "Chunk too long: 2325\n",
      "Chunk too long: 1406\n",
      "Chunk too long: 942\n",
      "Chunk too long: 820\n",
      "Chunk too long: 1414\n",
      "Chunk too long: 1948\n",
      "Chunk too long: 2060\n",
      "Chunk too long: 2265\n",
      "Chunk too long: 2999\n",
      "Chunk too long: 3093\n",
      "Chunk too long: 3359\n",
      "Chunk too long: 3567\n",
      "Chunk too long: 2455\n",
      "Chunk too long: 2228\n",
      "Chunk too long: 1986\n",
      "Chunk too long: 1746\n",
      "Chunk too long: 1930\n",
      "Chunk too long: 1643\n",
      "Chunk too long: 1206\n",
      "Chunk too long: 883\n",
      "Chunk too long: 1281\n",
      "Chunk too long: 889\n",
      "Chunk too long: 907\n",
      "Chunk too long: 920\n",
      "Chunk too long: 909\n",
      "Chunk too long: 914\n",
      "Chunk too long: 3701\n",
      "Chunk too long: 6330\n",
      "Chunk too long: 8974\n",
      "Chunk too long: 11616\n",
      "Chunk too long: 10656\n",
      "Chunk too long: 8027\n",
      "Chunk too long: 5383\n",
      "Chunk too long: 2741\n",
      "Chunk too long: 3701\n",
      "Chunk too long: 6330\n",
      "Chunk too long: 8974\n",
      "Chunk too long: 11616\n",
      "Chunk too long: 10656\n",
      "Chunk too long: 8027\n",
      "Chunk too long: 5383\n",
      "Chunk too long: 2741\n",
      "Chunk too long: 3701\n",
      "Chunk too long: 6330\n",
      "Chunk too long: 8974\n",
      "Chunk too long: 11616\n",
      "Chunk too long: 10661\n",
      "Chunk too long: 8046\n",
      "Chunk too long: 5405\n",
      "Chunk too long: 2784\n",
      "Chunk too long: 829\n",
      "Chunk too long: 786\n",
      "Chunk too long: 1345\n",
      "Chunk too long: 1286\n",
      "Chunk too long: 1064\n",
      "Chunk too long: 952\n",
      "Chunk too long: 669\n",
      "Chunk too long: 950\n",
      "Chunk too long: 1945\n",
      "Chunk too long: 1942\n",
      "Chunk too long: 1930\n",
      "Chunk too long: 1033\n",
      "Chunk too long: 663\n",
      "Chunk too long: 683\n",
      "Chunk too long: 676\n",
      "Chunk too long: 688\n",
      "Chunk too long: 708\n",
      "Chunk too long: 681\n",
      "Chunk too long: 671\n",
      "Chunk too long: 656\n",
      "Chunk too long: 659\n",
      "Chunk too long: 922\n",
      "Chunk too long: 911\n",
      "Chunk too long: 888\n",
      "Chunk too long: 888\n",
      "Chunk too long: 1012\n",
      "Chunk too long: 1406\n",
      "Chunk too long: 1933\n",
      "Chunk too long: 2584\n",
      "Chunk too long: 2119\n",
      "Chunk too long: 1484\n",
      "Chunk too long: 962\n",
      "Chunk too long: 1110\n",
      "Chunk too long: 1153\n",
      "Chunk too long: 1882\n",
      "Chunk too long: 1953\n",
      "Chunk too long: 2266\n",
      "Chunk too long: 2768\n",
      "Chunk too long: 2693\n",
      "Chunk too long: 2461\n",
      "Chunk too long: 1678\n",
      "Chunk too long: 1144\n",
      "Chunk too long: 719\n",
      "Chunk too long: 1384\n",
      "Chunk too long: 1646\n",
      "Chunk too long: 1592\n",
      "Chunk too long: 1304\n",
      "Chunk too long: 1061\n",
      "Chunk too long: 1139\n",
      "Chunk too long: 1028\n",
      "Chunk too long: 1277\n",
      "Chunk too long: 688\n",
      "Chunk too long: 1112\n",
      "Chunk too long: 1810\n",
      "Chunk too long: 2622\n",
      "Chunk too long: 823\n",
      "Chunk too long: 849\n",
      "Chunk too long: 849\n",
      "Chunk too long: 982\n",
      "Chunk too long: 982\n",
      "Chunk too long: 993\n",
      "Chunk too long: 956\n",
      "Chunk too long: 823\n",
      "Chunk too long: 849\n",
      "Chunk too long: 849\n",
      "Chunk too long: 967\n",
      "Chunk too long: 909\n",
      "Chunk too long: 823\n",
      "Chunk too long: 849\n",
      "Chunk too long: 849\n",
      "Chunk too long: 982\n",
      "Chunk too long: 991\n",
      "Chunk too long: 1304\n",
      "Chunk too long: 1487\n",
      "Chunk too long: 1477\n",
      "Chunk too long: 1165\n",
      "Chunk too long: 1051\n",
      "Chunk too long: 762\n",
      "Chunk too long: 837\n",
      "Chunk too long: 1068\n",
      "Chunk too long: 1372\n",
      "Chunk too long: 1670\n",
      "Chunk too long: 1657\n",
      "Chunk too long: 1793\n",
      "Chunk too long: 1707\n",
      "Chunk too long: 1699\n",
      "Chunk too long: 1708\n",
      "Chunk too long: 2198\n",
      "Chunk too long: 2236\n",
      "Chunk too long: 2172\n",
      "Chunk too long: 2669\n",
      "Chunk too long: 2793\n",
      "Chunk too long: 949\n",
      "Chunk too long: 1684\n",
      "Chunk too long: 2400\n",
      "Chunk too long: 2908\n",
      "Chunk too long: 2667\n",
      "Chunk too long: 2651\n",
      "Chunk too long: 2942\n",
      "Chunk too long: 2374\n",
      "Chunk too long: 1969\n",
      "Chunk too long: 1253\n",
      "Chunk too long: 1039\n",
      "Chunk too long: 1082\n",
      "Chunk too long: 924\n",
      "Chunk too long: 931\n",
      "Chunk too long: 853\n",
      "Chunk too long: 846\n",
      "Chunk too long: 1324\n",
      "Chunk too long: 1595\n",
      "Chunk too long: 817\n",
      "Chunk too long: 815\n",
      "Chunk too long: 999\n",
      "Chunk too long: 1448\n",
      "Chunk too long: 2175\n",
      "Chunk too long: 2774\n",
      "Chunk too long: 2116\n",
      "Chunk too long: 1396\n",
      "Chunk too long: 804\n",
      "Chunk too long: 1150\n",
      "Chunk too long: 1840\n",
      "Chunk too long: 2164\n",
      "Chunk too long: 2174\n",
      "Chunk too long: 4291\n",
      "Chunk too long: 4306\n",
      "Chunk too long: 2199\n",
      "Chunk too long: 2209\n",
      "Chunk too long: 869\n",
      "Chunk too long: 855\n",
      "Chunk too long: 868\n",
      "Chunk too long: 768\n",
      "Chunk too long: 742\n",
      "Chunk too long: 751\n",
      "Chunk too long: 960\n",
      "Chunk too long: 736\n",
      "Chunk too long: 761\n",
      "Chunk too long: 770\n",
      "Chunk too long: 793\n",
      "Chunk too long: 737\n",
      "Chunk too long: 689\n",
      "Chunk too long: 691\n",
      "Chunk too long: 841\n",
      "Chunk too long: 698\n",
      "Chunk too long: 891\n",
      "Chunk too long: 890\n",
      "Chunk too long: 1407\n",
      "Chunk too long: 1537\n",
      "Chunk too long: 817\n",
      "Chunk too long: 761\n",
      "Chunk too long: 674\n",
      "Chunk too long: 699\n",
      "Chunk too long: 696\n",
      "Chunk too long: 829\n",
      "Chunk too long: 823\n",
      "Chunk too long: 813\n",
      "Chunk too long: 813\n",
      "Chunk too long: 792\n",
      "Chunk too long: 792\n",
      "Chunk too long: 1183\n",
      "Chunk too long: 1581\n",
      "Chunk too long: 1204\n",
      "Chunk too long: 1608\n",
      "Chunk too long: 813\n",
      "Chunk too long: 792\n",
      "Chunk too long: 792\n",
      "Chunk too long: 1177\n",
      "Chunk too long: 1591\n",
      "Chunk too long: 1221\n",
      "Chunk too long: 1628\n",
      "Chunk too long: 841\n",
      "Chunk too long: 830\n",
      "Chunk too long: 825\n",
      "Chunk too long: 831\n",
      "Chunk too long: 1227\n",
      "Chunk too long: 1222\n",
      "Chunk too long: 1219\n",
      "Chunk too long: 1212\n",
      "Chunk too long: 813\n",
      "Chunk too long: 821\n",
      "Chunk too long: 822\n",
      "Chunk too long: 811\n",
      "Chunk too long: 812\n",
      "Chunk too long: 811\n",
      "Chunk too long: 810\n",
      "Chunk too long: 813\n",
      "Chunk too long: 813\n",
      "Chunk too long: 806\n",
      "Chunk too long: 807\n",
      "Chunk too long: 814\n",
      "Chunk too long: 814\n",
      "Chunk too long: 818\n",
      "Chunk too long: 819\n",
      "Chunk too long: 813\n",
      "Chunk too long: 814\n",
      "Chunk too long: 3183\n",
      "Chunk too long: 3198\n",
      "Chunk too long: 3197\n",
      "Chunk too long: 3195\n"
     ]
    }
   ],
   "source": [
    "long_chunks = []\n",
    "\n",
    "for record in dataset:\n",
    "    chunk = record['chunk']\n",
    "    chunk_len = len(tokenizer.encode(chunk, disallowed_special=()))\n",
    "    if chunk_len > max_tokens:\n",
    "        print(f'Chunk too long: {chunk_len}')\n",
    "        long_chunks.append(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we get a ton of excessively long chunk, but why? We'll figure that out soon. But first I want to see if the LangChain implementation of chunking works any better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out langchain has a function for splitting text using `tiktoken` already... See [here](https://langchain.readthedocs.io/en/latest/modules/utils/combine_docs_examples/textsplitter.html#tiktoken-openai-length-function).\n",
    "\n",
    "It is built into the `CharacterTextSplitter`, and with a single function we can separate using the `\".\\n\"` we used before, we include the `encoder_name` as before, the `max_tokens`, and we can even set a `chunk_overlap` with a single parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Opportunities and Challenges in Neural Dialog Tutoring\\nJakub Macina\\x031;2Nico Daheim\\x033Lingzhi Wang4\\nTanmay Sinha5Manu Kapur5Iryna Gurevych3Mrinmaya Sachan1\\n1Department of Computer Science, ETH Zürich2ETH AI Center\\n3UKP Lab, Department of Computer Science, Hessian Center for AI (hessian.AI), TU Darmstadt\\n4The Chinese University of Hong Kong\\n5Professorship for Learning Sciences and Higher Education, ETH Zürich\\njakub.macina@ai.ethz.ch\\nAbstract\\nDesigning dialog tutors has been challenging\\nas it involves modeling the diverse and com-\\nplex pedagogical strategies employed by hu-\\nman tutors. Although there have been sig-\\nniﬁcant recent advances in neural conversa-\\ntional systems using large language models\\nand growth in available dialog corpora, dialog\\ntutoring has largely remained unaffected by\\nthese advances. In this paper, we rigorously an-\\nalyze various generative language models on\\ntwo dialog tutoring datasets for language learn-\\ning using automatic and human evaluations to\\nunderstand the new opportunities brought by\\nthese advances as well as the challenges we\\nmust overcome to build models that would be\\nusable in real educational settings. We ﬁnd\\nthat although current approaches can model tu-\\ntoring in constrained learning scenarios when\\nthe number of concepts to be taught and pos-\\nsible teacher strategies are small, they perform\\npoorly in less constrained scenarios. Our hu-\\nman quality evaluation shows that both models\\nand ground-truth annotations exhibit low per-\\nformance in terms of equitable tutoring, which\\nmeasures learning opportunities for students\\nand how engaging the dialog is. To understand\\nthe behavior of our models in a real tutoring\\nsetting, we conduct a user study using expert\\nannotators and ﬁnd a signiﬁcantly large num-\\nber of model reasoning errors in 45% of con-\\nversations. Finally, we connect our ﬁndings to\\noutline future work.\\nhttps://github.com/eth-nlped/\\ndialog-tutoring\\n1 Introduction\\nThe goal of dialog tutoring research is to build sys-\\ntems that can tutor students using natural language\\nconversation (Wollny et al., 2021). For several\\ndecades, learning scientists have been studying the\\nfeatures of domain-speciﬁc dialog tutoring systems\\n*Equal contribution.that engender learning in students (Chi et al., 1994;\\nGraesser et al., 1995; Moore et al., 2004; Litman\\net al., 2006; Graesser, 2016; Ruan et al., 2019)\\nand have established strong learning gains that are\\neven comparable to human tutoring in speciﬁc do-\\nmains (Nye et al., 2014). However, these systems\\nrequire extensive authoring of materials by teach-\\ners (MacLellan and Koedinger, 2020) and therefore\\ncannot fully utilize the scalability of online learn-\\ning.\\nBuilding dialog tutors is technically challeng-\\ning as tutoring dialogs typically exhibit properties\\nthat are absent in other forms of dialog. Tutoring\\ndialogs are often long, enabling students to be ex-\\nposed to the concepts in a way that they can use\\nthem in future (Chi and Wylie, 2014), and grounded\\nin the learning scenarios (Graesser et al., 2009). Fi-\\nnally, good dialog tutors are engaging and create\\nopportunities to learn, providing students space to\\nseek and provide explanations, and self-reﬂect (Chi\\nand Wylie, 2014; Reiser, 2004).\\nThe growing success of deep neural network\\nbased language generators in other dialog settings\\n(Adiwardana et al., 2020; Roller et al., 2021) sug-\\ngests new possibilities in dialog tutoring that could\\nscale beyond domain-speciﬁc approaches. How-\\never, despite their promise, advances in neural gen-\\nerative models have seen little adoption in dialog\\ntutoring.\\nIn this paper, we contribute a comprehensive\\nstudy of applicability of neural generative models\\nin tutoring. We formally introduce the dialog tu-\\ntoring task and analyze existing tutoring datasets\\n(§2). Then, we describe several generative and\\nretrieval-based models for dialog tutoring (§3) and\\nbenchmark them on two open-access dialog tutor-\\ning datasets for language learning: CIMA (Stasaski\\net al., 2020, a crowdsourced role-played dataset\\nfor learning prepositional phrases in Italian) and\\nTeacher-Student Chatroom Corpus (TSCC) (Caines\\net al., 2020, a one-to-one English tutoring datasetarXiv:2301.09919v1  [cs.CL]  24 Jan 2023\\nfrom an online chatroom) (§5.1). We evaluate our\\nmodels on various automatic metrics (§4.2) as well\\nas two human evaluation studies: an evaluation of\\nthe quality of the generated response with respect\\nto various measures of goodness (§6.1), as well as\\na more realistic user study with a learning interface\\n(§6.2).\\nOverall, while we ﬁnd that pretrained models\\nimprove over simpler baselines in terms of auto-\\nmatic metrics, our consequent human evaluation\\nreveals several shortcomings that ought to be ad-\\ndressed before these models can be adopted in the\\nreal world. We ﬁnd that while neural generative\\nmodels can model more constrained learning set-\\ntings well, they struggle when the learning goal is\\nmore open-ended. Speciﬁcally, these models are\\nunable to understand and reason about student so-\\nlutions and misconceptions, and thus, are unable to\\nuse effective pedagogical strategies.\\nWe ﬁnd that the ﬁeld of dialog tutoring is signif-\\nicantly limited by the quantity and quality of avail-\\nable datasets. The available datasets are both too\\nsmall and not rich enough to capture the nuances\\nof the dialog tutoring problem. Our analysis also\\nreveals the inadequacy of automatic evaluation met-\\nrics for capturing tutoring quality. Not only are the\\nexisting metrics unable to capture faithfulness to\\nthe learning material and the student dialog history,\\nbut they also cannot capture moves of good human\\ntutors that allow learners the space for reﬂection,\\nexplanation, follow-ups, and real engagement in\\nthe process of learning. Based on our ﬁndings, we\\nend with an outline of potential avenues of future\\nresearch (§7). We hope that our paper will bring\\nattention to this underexplored natural language\\nprocessing application with the potential for signif-\\nicant social good.\\n2 The Dialog Tutoring Task\\nDialog tutoring can be described as a multi-turn\\ninteraction between two interlocutors, where one\\nperforms the role of a teacher seeking to teach the\\nother interlocutor who acts in the role of a student .\\nWe then can describe a dialog tutoring session for-\\nmally as a sequence of turns H= (u1;:::;ujHj)\\nthat are taken by either of the interlocutors. Each\\nturnut2V\\x03is a ﬁnite sequence of tokens from a\\nvocabularyV.\\nFurther, each turn utcan be associated with a\\nsequence of dialog acts at2A that indicate the\\naction taken by the interlocutor in the correspond-\\nFigure 1: Examples of tutoring conversations from both\\ndatasets. The (image) grounding is shown in the second row\\nand dialog acts in brackets which we assume to indicate the\\npedagogical strategy.\\ning turn. The dialog act is a key aspect of dialog\\ntutoring as it can refer to the teaching strategy em-\\nployed by the tutor. These may include strategies\\nsuch as providing a hint orseeking a clariﬁcation\\n(see Appendix A for more details). The set of\\ndialog actsAis usually ﬁxed according to a prede-\\nﬁned taxonomy and may be split into two subsets\\nA=Ateacher[A student , each corresponding to the\\nteacher and student role. Each dialog session H\\nmay also be accompanied with some grounding\\ninformation K, which grounds the response in rel-\\nevant information and may refer to the teaching\\nmaterial that needs to be taught to the student. This\\ninformation Kmay come in various formats, in-\\ncluding images and videos. However, we restrict\\nourselves to using only text-based grounding in this\\nwork such that K2K\\x12V\\x03is again a sequence\\nof tokens from the common vocabulary VandK\\nis used to describe the set of possible groundings\\n(e.g., a textbook with a set of chapters). In Section\\n3 we derive different methods to model the role of\\nthe teacher, to which we restrict this work.\\n2.1 Existing tutoring datasets\\nTo our knowledge, only three conversational tutor-\\ning datasets are openly available: CIMA (Stasaski\\net al., 2020) is a crowd-sourced dataset, where\\nannotators were asked to role-play students and\\nteachers by working through an exercise on trans-\\nlating a prepositional phrase from English to Italian,\\ngiven an image and a shared set of concepts. TSCC\\n(Caines et al., 2020) uses real teachers leading one-\\non-one language tutoring sessions in English lan-\\nguage learning, thus creating a more open-ended\\nscenario. Finally, TalkMoves (Suresh et al., 2022a).\\nis a collection of scraped classroom transcripts of\\nK-12 mathematics lesson videos that contain chal-\\nlenging, multi-party interactions.\\nThe scarcity of tutoring datasets stands in con-\\nCIMA TSCC\\nModel sBLEU / BLEU-1 ( \") BERT F1 (\")Q2(\") sBLEU / BLEU-1 ( \") BERT F1 (\")\\nRule-based (Stasaski et al., 2020)\\x030.34/- 0.45 - - -\\nLSTM (Stasaski et al., 2020)\\x030.31/- 0.53 - - -\\nSeq2seq 2.89 / 28.0 0.676 0.372 - -\\nDialoGPT 4.12 / 35.6 0.697 0.571 0.63 / 18.5 0.661\\nBi-Encoder (RoBERTa-base) 5.89 / 23.9 0.690 0.344 1.367 / 8.8 0.638\\nCTRL (BART-base) 5.99 / 42.5 0.702 0.673 - -\\nt5-small 7.36 / 34.0 0.672 0.676 2.72 / 12.1 0.646\\nBART-large 8.61 / 38.7 0.715 0.673 1.85 / 13.7 0.658\\nBART-base 9.58 / 42.5 0.726 0.680 2.67 /18.6 0.670\\nmt5-small 11.26 / 41.0 0.700 0.624 1.80 / 14.9 0.653\\nBART-basey5.61 / 41.03 0.707 0.642 1.90 / 15.4 0.659\\nBART-largey5.65 / 42.67 0.694 0.607 1.74 / 15.1 0.660\\nTable 1: Comparison of models on CIMA and TSCC. We note that the strong sacrebleu differences are caused by the brevity\\npenalty (all generative models generate too short sequences),y: use predicted dialog act label, others use ground-truth. * numbers\\ntaken from (Stasaski et al., 2020) - here, numbers may not be comparable as there is no standard split in CIMA.\\ntrast to other dialog scenarios, where plenty of\\ndatasets have been proposed and studied. For\\nexample, task-oriented dialog has been studied\\nin domains like reservations (Wen et al., 2017;\\nBudzianowski et al., 2018; Kim et al., 2020) or\\npublic service information (Feng et al., 2020).\\nOn the other hand, chit-chat or open-domain di-\\nalog has been studied on movies (Zhou et al.,\\n2018), Wikipedia knowledge (Dinan et al., 2019),\\nagent personae (Dinan et al., 2020), knowledge\\ngraphs (Moon et al., 2019), and open-ended set-\\ntings (Komeili et al., 2022).\\nFurthermore, we note the following limitations\\nand characteristics of tutoring datasets, also in com-\\nparison to other dialog domains: 1) Low pedagog-\\nical quality (CIMA), 2) Limited teaching strate-\\ngies (all), 3) Exclusive focus on classroom settings\\n(TalkMoves), 4) Small dataset size (all). 5) Sig-\\nniﬁcantly larger context sizes (TSCC) 6) Harder\\nreadability according to the Flesch score (TSCC).\\nWe provide more evidence in Table 2 which shows\\na comparison of the dialogue tutoring dataset statis-\\ntics with widely-used goal-oriented and chit-chat\\ndatasets.\\n2.2 Related work on generative dialog models\\nSimilarly, while the advent of large pretrained mod-\\nels has sparked ample research on generative mod-\\nels for dialog (Bao et al., 2021; Peng et al., 2021;\\nRoller et al., 2021; Shuster et al., 2022; Cohen et al.,\\n2022), this has not carried over to research on tutor-\\ning systems, where existing solutions are predomi-\\nnantly rule-based and do not generate open-ended\\nresponses. For example, the authors on CIMA de-\\nﬁne heuristics to select responses (Stasaski et al.,\\n2020). Pretrained transformers in general haveonly very recently been studied in this setting, how-\\never only for dialog act classiﬁcation (Suresh et al.,\\n2022b) and to study the pedagogical ability of exist-\\ning large pretrained models (Tack and Piech, 2022).\\n3 Dialog Tutoring Models\\nAfter introducing the tutoring task, this section\\nhighlights the models we evaluate on the task. We\\nnote that our aim is an analysis of existing models.\\nWe explore turn-level models that can generate a\\nteacher response y:=ut+1given a tutoring session\\nH= (u1;:::;ujHj). During training, we obtain\\nthe dialog history by teacher forcing, i.e., we take\\nthe ground-truth dialog history. Furthermore, we\\ndo not model the problem of retrieving grounding\\ninformation but rather assume it as given.\\nGenerative Model In order to study if genera-\\ntive models can capture a given teaching strategy,\\nwe ﬁrst derive a model that assumes the ground-\\ntruth dialog act sequence a=fa1;:::; ajHjgto\\nbe given as an input. Then, given dialog history\\nH<t=fu1;:::;utg, grounding information K\\nandat+1\\x12A teacher , the set of dialog acts relevant\\nat timestept+ 1, the teacher response yis gen-\\nerated according to a locally-normalized language\\ngeneration model. In the case that no grounding\\ninformationKis given, the dependency on Kmay\\nbe dropped.\\ny?= argmax\\ny2V\\x03fp(yjat+1;H<t;K)g (1)\\n= argmax\\ny2V\\x03jyjY\\ni=1fp(yijy<i;at+1;H<t;K)g\\nFor all experiments, we separate the turns in the\\ndialog by specialhteacheriandhstudentitags. In\\nDataset Train samples #DA Tgt. length Src. length #prev. turns corpus-div. Flesch score F1 (^y;K)\\nCIMA 2,715 5 14.71 9.70 4.55 0.149 84.64 0.196\\nTSCC 5,845 23 16.09 11.72 68.28 0.327 73.00 -\\nMultiWoZ 2.1 56,781 34\\x0319.86 14.49 7.86 0.069 90.90 -\\nSchema-Guided Dialog 164,982 10\\x0314.30 10.36 11.38 0.049 95.37 -\\nDSTC9 19,184 - 21.61 11.65 11.70 0.050 81.85 0.47\\nPersonachat 127,162 - 12.26 11.65 6.51 0.162 91.80 0.10\\nFaithDial 18,357 - 21.72 17.33 4.54 0.274 83.28 0.47\\nCMU_DoG 81,468 - 14.49 18.23 18.73 0.178 79.54 0.02\\nTable 2: Dialogue dataset statistics. Target length and source length in avg. number of tokens (Bart tokenizer). # prev. turns\\nis avg. for each teacher response. Corpus-div is ngram entropy averaged for uni to four-grams. hlines separate: tutoring,\\ntask-oriented, open-domain dialog. \\x03We only count system dialog acts.\\nCIMA we encode the triples deﬁning the grounding\\ninformation in a simple natural language format,\\nwhere we separate the English and Italian words\\nfor an object, color, and preposition as well as\\nthe whole phrase by the word \"is\", for example\\nas \"blue is blu\" in Figure 1. Further, we add the\\ngrammar rules separated by a special token. We\\nstudy different models to parametrize pthat are\\ndescribed in Section 4.\\nFinally, we use the version of CTRL (Keskar\\net al., 2019) presented by Rashkin et al. (2021).\\nThe aim of the model is to improve the faithful-\\nness of grounded response generation models, a\\nsigniﬁcant problem in neural language generation\\n(Roller et al., 2021) which holds high importance\\nin the ﬁeld of tutoring, where one trusts a teacher\\nto present correct information. The model is aug-\\nmented by a sequence of control tokens that are\\nintended to steer the generations to desirable prop-\\nerties. We use the lexical overlap andentailment\\ntokens , which we obtain as follows. In training,\\nthe lexical overlap is measured on a token-level be-\\ntween ground-truth response and grounding. Then,\\nthree equally sized buckets are created indicating\\nlow, medium, and high overlap which is indicated\\nby a control token. Entailment is determined by\\nan MNLI model and again a corresponding token\\nis added. At test time, we always use the token\\nthat encourages the desirable property, in this case\\nhigh lexical overlap and entailment. Finally, using\\na sequence of control tokens c, the model from\\nequation 1 becomes:\\np(yjat+1;c;H<t;K) (2)\\nJoint Model In order to study how well current\\nneural models can decide on a reasonable teach-\\ning strategy and perform in real case scenarios, we\\nalso implement a model that ﬁrst decides the di-\\nalog actat+12A teacher (instead of assuming the\\nground-truth dialog act) and then uses it to gener-ate a response y=ut+1. We use a simple model\\nthat again takes the grounding and dialog context\\nas input but now generates the concatenation of\\ndialog act and response in one utterance, akin to\\nSOLOIST (Peng et al., 2021). Thus, for a given\\n~y:=at+1\\x0eywith act sequence at+1of lengthN\\nand response yof length T, the model is\\np(~yjK;H<t) =m+NY\\ni=1p(~yij~y<i;K;H<t)(3)\\nIn training, we use teacher forcing and prepend\\nat+1toyto obtain the label sequence. At test time,\\nthe model performs a beam search over the dialog\\nact sequence and response jointly.\\nRetrieval-based model Since generative models\\nare known to produce erroneous outputs that are\\nfactually incorrect and potentially inappropriate\\n(Ji et al., 2022), we also experiment with using a\\nretrieval-based model that selects responses from\\nthe training corpus at test time. As opposed to\\nprevious work on the topic (e.g., Stasaski et al.\\n(2020)), we do not employ a rule-based model but\\nrather a learned retrieval model that does not re-\\nquire handcrafting elaborate and possibly brittle\\nrules. Therefore, we use the Bi-Encoder architec-\\nture (Mazaré et al., 2018; Dinan et al., 2019) where\\na dialog context encoder encH<t;\\x12and a response\\nencoder ency;\\x12encode contextH<tand possible\\nresponses yinto a ﬁxed size vector of same dimen-\\nsionn. In our experiments, the weights \\x12of both\\nencoders are shared.\\nThe model is trained using contrastive learning.\\nSuppose we are given a training pair H;^yfrom a\\ntraining datasetDthat we use for teacher forcing.\\nWe then train the model by sampling a negative\\nresponse \\x16yfrom the set of responses in Dand\\nusing the Triplet Loss criterion, which for a metric\\nfunctiond:Rn\\x02Rn!Ris deﬁned as:\\nL(\\x12;H;^y;\\x16y) = [m+d(encH;\\x12(H);ency;\\x12(^y))\\n\\x00d(encH;\\x12(H);ency;\\x12(\\x16y))]+,\\n(4)\\nwheremis a margin hyperparameter, and dis the\\neuclidean norm in our experiments. Further, we\\ndo stratiﬁed sampling on CIMA to not select neg-\\natives from the same preposition, color, or object\\nthat might be false negatives. At test time, given\\na dialog contextH<t, we choose a response y?\\nfrom the training set Dby maximum inner product\\nsearch using the decision rule\\ny?= argmax\\ny2DfencH;\\x12(H<t)Tency;\\x12(y)g. (5)\\n4 Experiments\\nWe use the following models for parameterizing\\npin Equation 2: A sequence-to-sequence model\\n(Sutskever et al., 2014) with a copy mechanism (Gu\\net al., 2016) trained from scratch. A wide range\\nof pretrained Transformers, namely BART (Lewis\\net al., 2020), DialoGPT (Zhang et al., 2020), T5\\n(Raffel et al., 2020) and its multilingual version\\nmT5 (Xue et al., 2021).\\nBART and T5 are pretrained encoder-decoder\\nmodels that were trained on denoising and text-\\nto-text tasks, respectively. mT5 bases on T5 but\\nis multilingual which might help with the code-\\nswitched utterances in CIMA. Lastly, DialoGPT is\\nan autoregressive language model based on GPT-2\\n(Radford et al., 2019) that was pretrained on a large\\ndialog dataset obtained from Reddit. With this, we\\nintend to study whether large-scale dialog-speciﬁc\\npretraining can aid in training educational tutors,\\nas well.\\nWe implement our experiments using the\\nHuggingface transformers library and ﬁnetune\\nthe checkpoints provided as part of it for all\\nTransformer-based models. For these models, we\\nuse an initial learning rate of 3:25\\x0210\\x005, 500\\nwarmup steps and linear learning rate decay. We\\ntrain the models using a batch size of 8 and evalu-\\nate on the validation sets after each epoch. In the\\nend, we select the best model to be the one that has\\na minimal loss on the validation set. The sequence-\\nto-sequence baseline is trained from scratch using\\nan initial learning rate of 0:001for 25,000 steps\\nusing the Adam optimizer and a dropout rate of\\n0:1We use beam search with a beam size of 10 to\\ngenerate model responses.4.1 Dataset splits\\nSince there are no ofﬁcial dataset splits for CIMA\\nand TSCC, we split both datasets randomly into\\ntraining, validation and test sets. We provide the\\nexact split of the dataset in an accompanying code\\nrepository. For CIMA, we use all such samples\\nwith less than three annotated tutor responses for\\ntraining. The other conversations are split ran-\\ndomly into equally-sized validation and test sets\\nwhich results in 2715/300/300 samples each.\\nFor TSCC, we split randomly along the conver-\\nsations to obtain 82/10/11 training, validation, and\\ntest conversations each.\\n4.2 Evaluation metrics\\nTo evaluate our models, we use the BLEU im-\\nplementation provided by the sacrebleu package\\n(sBLEU) (Post, 2018) to measure lexical overlap\\nbetween generated and ground-truth response. Fur-\\nthermore, we use BERT F1 (BERTScore) to mea-\\nsure their semantic similarity. Lastly, for CIMA\\nwe also calculate Q2(Honovich et al., 2021) which\\nmeasures the factual consistency of the response y\\nwith the grounding information Kby employing a\\nquestion-answering based matching.\\n5 Results\\nIn this section, we summarize our main ﬁndings\\nin terms of automatic evaluation. First, we give\\nan overview of the performance of different mod-\\nels that we train on CIMA and TSCC in Section\\n5.1. Then, we assess their ability to stay faithful\\nto teaching strategies (Section 5.2) and study how\\ngrounding annotations can inﬂuence the faithful-\\nness of neural dialog tutors (Section 5.3), before\\nstudying their scaling behavior with dataset size\\nand complexity (Section 5.4) and their generaliza-\\ntion capabilities (Section 5.5). We then ﬁnish with\\nan assessment of using education-speciﬁc data for\\npretraining (Section 5.6).\\n5.1 Comparison of different models\\nTable 1 shows the key results from our experiments.\\nFirst, all automatic metrics are signiﬁcantly higher\\non CIMA, which indicates that the models can ﬁt\\nCIMA much better than TSCC, with which cur-\\nrent approaches still struggle. We further analyse\\nthis ﬁnding in Section 5.2 and show that this is be-\\ncause TSCC has richer teaching strategies which\\nare harder to model . Our comparison also sug-\\ngests that ﬁnetuning large pretrained Transformer\\nMethod\\nGT BART base BART large CTRL Retrieval\\nDA F1 78.3 81.0 70.1 63.0 43.1\\nTable 3: F1 score of the dialog act classiﬁcation based on the\\ngenerated responses of our models.\\nHint\\nQuestionCorrectionConﬁrmationOther00:20:40:6\\nFigure 2: Distribution of predicted and ground-truth dialog\\nacts on CIMA.\\nmodels generally gives better results than the rule-\\nbased and LSTM model reported in (Stasaski et al.,\\n2020), and our implemented retrieval and sequence-\\nto-sequence baselines. This illustrates the potential\\nof large language models for dialog tutoring.\\nWe also see a signiﬁcant difference among dif-\\nferent large language models. Dialog-speciﬁc pre-\\ntraining of DialoGPT does not help and gives worse\\nresults than BART and T5, primarily because the\\nmodel tends to generate short and generic responses\\nmore often. Multilingual pretraining in mT5 im-\\nproves over T5 only in some metrics, notably in\\nBLEU and BERT F1 on CIMA but not in terms\\nofQ2. Similarly, adding control tokens to BART\\ndoes not improve Q2or other automatic metrics.\\nSurprisingly, using very large models actually de-\\ngrades performance in our experiments. Finally, the\\nlast two rows show results obtained with our joint\\nmodel that does not use the ground-truth dialog act\\nbut predicts it together with the response sequence\\nand still provides reasonable performance.\\n5.2 How well can generative models capture\\nteaching strategies?\\nWe study this question ﬁrst by evaluating the di-\\nalog act prediction accuracy of our joint model.\\nWe ﬁnd that it is signiﬁcantly low on TSCC with\\n21:8compared to 71:2on CIMA for BART-base\\nwhich indicates signiﬁcant room for improvement.\\nNotably, the joint model tends to predict more fre-Model sBLEU ( \") BERT F1 (\")Q2(\")\\nBART-base 6.69 / 38.6 0.718 0.571\\n+ triples 9.20 / 45.3 0.730 0.642\\n+ grammar rules 9.58 / 42.5 0.726 0.680\\nTable 4: Comparison of models with different inputs on\\nCIMA. Triples are made up of preposition, object, and color\\ntranslations. Grammar rules are a textual description of a\\nlearning concept.\\nquently occuring dialog acts, which results in fewer\\nfollow-up questions and \"Other\" never being pre-\\ndicted in CIMA, the least frequent act in the data.\\nThe distribution of dialog acts in the ground-truth\\nannotations and model predictions with a BART-\\nbase joint model is found in Figure 2.\\nThen, we evaluate how well different models can\\nstick to a given ground-truth dialog act by predict-\\ning the dialog act of the generated response with\\na BART-base model trained to predict the ground-\\ntruth dialog act sequence based on the ground-truth\\nresponse. The results are shown in Table 3. No-\\ntably, BART-base performs better than the ground-\\ntruth annotations . The CTRL model, on the other\\nhand, has worse performance since the control to-\\nkens do not respect tutoring principles (e.g., lexical\\noverlap to grounding discourages follow-up ques-\\ntions in favor of just giving hints).\\n5.3 Does grounding in learning concepts\\nhelp?\\nPrior work has shown that grounding responses in\\nrelevant data can improve their quality, especially\\nin terms of faithfulness (Shuster et al., 2021). We\\nintend to validate this for dialog tutoring by study-\\ning three models with different inputs on CIMA.\\nThe ﬁrst model is not provided grounding informa-\\ntion, whereas the second and third are grounded\\nin learning concepts (cf. Equation 1) with one us-\\ning only the (preposition, object, color) triples and\\nthe other making use of additional grammar rules.\\nThe results with these models are shown in Table\\n4 and suggest that grounding responses in relevant\\nknowledge helps the model to produce better and\\nmore faithful responses .\\n5.4 How do models scale with more data?\\nDue to the limited availability of high-quality ped-\\nagogical datasets and the time-consuming pro-\\ncess of authoring new materials (MacLellan and\\nKoedinger, 2020), it is important to understand how\\nquickly generative models can generalize to new\\nsettings. Thus, we assess how well the model can\\nmodel tutoring in low-resource scenarios. We con-\\n0 20 40 60 80 100020406080\\n% of training data usedQ2\\x01100\\n(a)0 1 2 3 4 5 6 7\\nNo. of concepts0510152025\\nsBLEU\\n(b)\\nFigure 3: Performance of BART-base on CIMA as a function\\nof: (a) training data size uniformly sampled from the training\\ndata, (b) the number of concepts, where only the speciﬁc\\nnumber of concepts is retained and all others are excluded.\\nstruct a study, where we randomly sample subsets\\nof the CIMA training set and test the performance\\nof the various models. We can see from Figure\\n3(a) that with more training data, the faithfulness\\nof responses appears to improve and is not satu-\\nrated before we reach the full training set. This\\nsupports the intuition that additional training data\\nmight improve the performance further .\\nSimilarly, we study how well our model can deal\\nwith an increase in complexity with respect to learn-\\ning concepts at similar training data sizes. There-\\nfore, we construct different training datasets with\\n735 samples and a varying number of concepts each\\ntime. We begin by taking samples concerned with\\nthe concept “in front of the” and evaluate exclu-\\nsively on it, gradually adding new concepts. Figure\\n3(b) suggests that Q2drops sharply at four con-\\ncepts. BLEU on the other hand increases, and this\\nmight be due to the metric encouraging generic ut-\\nterances that, for example, repeat a grammar rule.\\n5.5 Can models generalize to new concepts?\\nAs the students progress and gain new knowledge,\\nit might be a desirable property of dialog tutoring\\nmodels to be able to handle new concepts that suit\\nthis increase in prior knowledge. Hence, we study\\nhow well our CIMA model can generalize to new\\nconcepts that it has not seen in training, for exam-\\nple, a new preposition. For this analysis, we create\\na set-up where we ﬁrst train the model on all of the\\ntraining data and evaluate on the subset of samples\\nfor each preposition separately. We then compare\\nthis number to a model that is not trained on the\\ncorresponding concept it is evaluated on, creating a\\nzero-shot set-up which we carry out for a grounded\\nand ungrounded response generation model. As\\nmeasured by Q2(cf. Table 5), this model can in-\\ndeed generalize to new concepts well, albeit with\\nperformance degradation . Furthermore, grounding\\ninformation improves generalization as these de-Concept #Samples full data zero-shot zero-shot\\nwithout grounding\\ntrain/test Q2Q2Q2\\nis behind the 549/90 0.698 0.603 0.533\\nis in front of the 735/84 0.616 0.512 0.500\\nis next to the 547/51 0.497 0.539 0.483\\nis on top of the 224/30 0.683 0.578 0.567\\nis under the 270/24 0.854 0.646 0.625\\nis inside of the 390/21 0.579 0.643 0.190\\nall concepts 2715 / 300 0.644 0.570 0.502\\nTable 5: Performance of a grounded BART-base model by\\nlearning concept. Full data uses the entire training data and\\nzero-shot removes the concept of the row from the training\\ndata.\\nMethod sBLEU BERT F1 Q2\\nBART-base 6.69 / 38.6 0.718 0.571\\n+ Ed. data 7.31 /41.4 0.727 0.577\\n+ Non-Ed. data 6.60 / 39.4 0.721 0.583\\nTable 6: Inﬂuence of pretraining on educational and non-\\neducational data. Please note that no grounding information is\\nused in this setting.\\nﬁne the preposition and how it is used. Without this\\ninformation, we observe that the model generates\\ngeneric responses more often.\\n5.6 Does education-speciﬁc pre-training\\nhelp?\\nAs educational data are widely available on the\\ninternet, next we study how education-speciﬁc pre-\\ntraining effect results. In Table 6, we show re-\\nsults obtained with ﬁnetuning a BART-base model\\ndirectly on CIMA and pretraining it on tutoring\\ndialogs from TSCC or non-tutoring dialogs from\\nMultiWoZ 2.1 (Eric et al., 2020), Personachat\\n(Zhang et al., 2018), CMU DoG (Zhou et al.,\\n2018), DSTC9 (Kim et al., 2020) and Topicalchat\\n(Gopalakrishnan et al., 2019). In both cases, we\\nonly see minor improvements , which may be ex-\\nplained by the different dataset settings and the lack\\nof a uniﬁed dialog act taxonomy.\\n6 Human evaluation\\nWe further evaluate previously assessed models\\nwith human judgments by ﬁrst obtaining quality es-\\ntimates according to different criteria and conduct-\\ning a simulation study, where expert annotators are\\nasked to provide rewritings of conversations and to\\ncategorize errors made by the model.\\n6.1 Quality of the generated responses\\nWe perform a human quality evaluation of the gen-\\nerated response for four models - retrieval (Bi-\\nEncoder), BART-base, BART-base CTRL and the\\njoint model (BART-base). A randomly chosen\\nQuality Attribute sBLEU BERTScore\\nFluency 0.14 0.12\\nCoherence 0.17 0.26\\nCorrectness 0.06 0.15\\nEquitable Tutoring 0.08 0.16\\nTable 7: Pearson correlation coefﬁcients between the human\\njudgements on our quality criteria and automatic metrics.\\nsubset of the CIMA test set conversations were\\nannotated by 4 annotators (1 annotator speaking C1\\nlevel Italian). All annotators labeled 60 examples\\nin total, of which 20 overlapped. To further distin-\\nguish the quality of training data for the models,\\nwe annotated ground-truth responses on a small\\nsample of 20 examples. We evaluate the follow-\\ning criteria on a 3-point Likert scale (disagree to\\ncompletely agree) and outline our ﬁndings in the\\nfollowing, as shown in Figure 4.\\nFluency \"The response is grammatically correct\\nand ﬂuent.\" We ﬁnd that all models have very high\\nﬂuency scores.\\nCoherence \"The response naturally follows up\\non previous utterance and context and has no log-\\nical conﬂicts with the context or DA label.\" We\\nﬁnd that all generative models are able to produce\\ncoherent responses but not the retrieval model.\\nCorrectness \"The response is factually correct\\nand respects learning concepts being taught.\" All\\nmodels score comparable to ground-truth responses\\non the constrained CIMA dataset. It is noteworthy,\\nhowever, that a response may be correct in itself\\nbut not coherent with the context or the grounding\\n(often the case in the retrieval model), and this\\ncould explain the discrepancy between correctness\\nand our automatic Q2scores.\\nEquitable tutoring \"The response gives a learn-\\ning opportunity for the student by providing space\\nfor reﬂection, explanation, pointing to follow-up\\nchallenge, or engaging student in other ways.\"\\nHere we ﬁnd signiﬁcant deﬁciencies not only for\\nour evaluated models but notably also for the an-\\nnotated ground-truth responses (gt). This might\\nexplain the insufﬁciencies in the responses as they\\nreﬂect this distributional behavior of the training\\ndata. We think that future dataset collections should\\ntake better care of this property and resort to more\\nexpert annotators as opposed to crowdsourcing.\\nFurthermore, Table 7 shows that our automatic\\nmetrics correlate poorly with human judgements.\\nFigure 4: Comparison of models on four criteria (reporting\\nM) in the human quality evaluation. We observed high SD\\nfor coherence and equitable metrics.\\nFigure 5: User interface used for the qualitative evaluation.\\nThe interface is based on ParlAI (Miller et al., 2017) library.\\n6.2 User study with a learning interface\\nLastly, we seek to study how well dialog tutor-\\ning models can perform in a realistic setting with\\nquestions obtained from real users (containing out-\\nof-distribution samples) and not the ﬁxed dataset.\\nTherefore, we randomly sampled conversations\\nfrom the CIMA test set. We asked two C1-level\\nexpert Italian speakers to 1) rephrase these conver-\\nsations using a conversational dialogue interface\\n(Figure 5) and 2) assign erroneous model responses\\nto predeﬁned error categories. The interface used\\nin the qualitative evaluation is shown in Figure 5.\\nWe obtain all model responses from the BART-base\\nmodel that ﬁrst predicts the dialog act and then the\\nresponse and adopt existing error categories (Bom-\\nmasani et al., 2021). These categories describe\\nthe ideal behavior of tutoring models as simulating\\nthe behavior of good human teachers along two\\ndimensions:\\nUnderstanding \"Being able to understand and\\nreason about student solutions, misconceptions,\\nand learning concepts.\" We ﬁnd that of the 20\\nmodiﬁed conversations, 45% exhibit Understand-\\ning errors , such as an incorrect solution assessment\\nor incorrect translations.\\nPedagogy \"Being able to use effective pedagogy\\nto instruct students.\" We ﬁnd that 10% of the re-\\nsponses exhibit Pedagogical errors , for example\\ntelling the correct solution directly without offering\\nany engagement point to the student.\\n50% of the conversations were labeled good by\\nthe annotators. Examples of the conversations are\\navailable in Table 8.\\n7 Discussion: Towards more equitable\\nand faithful tutoring systems\\nIn this section, we outline directions of research\\nthat we think can be important steps towards more\\nequitable and faithful tutoring models. Namely,\\nwe ﬁrst address the small scale and quality of cur-\\nrent tutoring datasets and cast doubt on the crowd-\\nsourcing data quality checks. Then, we suggest\\nways of improving the underperformance of both\\nequitable tutoring and teaching strategy prediction\\nidentiﬁed in current generative models under these\\nconstraints by drawing from learning sciences lit-\\nerature. Finally, we outline desiderata for more\\nreliable dialog evaluation of neural tutoring mod-\\nels.\\nDatasets Based on the analysis in §2.1 and Table\\n2, we think that the community will beneﬁt from\\na dataset that lies between CIMA and TSCC in\\nterms of its difﬁculty. Moreover, the low equitable\\ntutoring scores of CIMA’s ground-truth responses\\nindicate that crowdsourcing with untrained annota-\\ntors can lead to low pedagogical quality. A similar\\nobservation has been found by human evaluation\\nfor the TSCC dataset (Tack and Piech, 2022). Fi-\\nnally, we encourage the establishment of better\\ndialog act taxonomies that are backed by learning\\nsciences research. As outlined in §5.6 and in He\\net al. (2022), a uniﬁed taxonomy may also strongly\\naid in transfer learning.\\nModels So far, dialog tutoring models have only\\ncovered limited domain-speciﬁc settings linked to\\na particular activity, such as learning Italian prepo-\\nsitions or solving math word problems. We argue\\nthat the community could beneﬁt from working on\\nproblems common to learning in general, for exam-\\nple tracking problem-solving states and modeling\\npedagogies used by teachers. Here, knowledge trac-\\ning (Srivastava and Goodman, 2021) could be usedfor tracking problem-solving states and increasing\\nthe coherence of dialog tutoring conversations and\\ndialog act selection performance which would con-\\ntribute to better modeling of global teaching strate-\\ngies. Furthermore, validated instruction quality\\ncoding schemes (Michaels et al., 2010; Hennessy\\net al., 2016) used by classroom teachers can be\\ncomputationally modeled (Demszky et al., 2021;\\nGanesh et al., 2021) and incorporated into models.\\nWe also think that recently proposed constrained\\ndecoding approaches that can balance between mul-\\ntiple criteria (Qin et al., 2022) hold great promise in\\nimproving faithfulness in complex tutoring dialogs.\\nFinally, as data collection is labor-intensive in ex-\\npert domains, we see great potential in few-shot\\nlearning methods, such as prompt-based methods\\n(Schick and Schütze, 2022).\\nEvaluation Our experiments highlight the insuf-\\nﬁciencies of current automatic dialog evaluation\\nmetrics, as both BLEU and BertScore show com-\\nparatively low correlation with our collected human\\njudgements from §6.1. This is in line with previous\\nresearch (Mehri and Eskenazi, 2020; Mehri et al.,\\n2022) and shows the necessity not only for better\\nautomatic evaluation metrics but also for veriﬁca-\\ntion based on human judgements or user studies\\nthat should incorporate criteria relevant to tutoring\\n(e.g., equitable tutoring outcomes).\\n8 Conclusion\\nIn this work, we reﬂected on the state of research\\nin dialog tutoring and explored the potential of\\ndeep generative models in this domain. We found\\nsome promising initial results with these models\\nin comparison to rule- or retrieval-based methods.\\nHowever, we also established limitations of cur-\\nrently available benchmarks and evaluation criteria.\\nFurthermore, we showed that there are a number of\\nchallenges that need to be addressed before deep\\ngenerative models of text can be deployed as in-\\ntelligent tutoring systems on a larger scale, such\\nas controllability and being able to model a sound\\npedagogical strategy. Based on these ﬁndings, we\\noutline potential avenues for future research.\\nLimitations\\nA key limitation of our work is the use of only\\ntwo available tutoring datasets. Despite a limited\\nnumber of datasets available in this domain, using\\nthe TalkMoves dataset (Suresh et al., 2022a) could\\nhelp further generalize our ﬁndings. This remains\\nan avenue for future work.\\nBased on the prior work, we focused on the spe-\\nciﬁc conversational goal of dialog tutors which is\\nproviding learning aid for students’ skill develop-\\nment and more opportunities to learn. While this is\\nthe most widespread type (Wollny et al., 2021), it is\\nnot covering all the goals of human tutors, and other\\naspects could be important, for example, rapport-\\nbuilding or mentoring on the meta-cognitive level.\\nWe acknowledge this both as a prerequisite for our\\nwork and at the same time as a limitation. For fur-\\nther discussion we refer the reader to Appendix B\\nand C.\\nFinally, our user study could be further extended\\nwith more participants. In the future, we plan\\na more comprehensive study with real language\\nlearners using an end-to-end dialog tutoring sys-\\ntem.\\nEthics Statement\\nWe do not foresee any signiﬁcant harm directly as a\\nresult of our work. Having said that, we must under-\\nstand that automatic tutoring is a high-stake setting\\nthat can pose signiﬁcant harm if appropriate care is\\nnot taken before the deployment of these systems.\\nIssues of biases and lack of trust, and other ethical\\nissues such as privacy concerns must be considered.\\nConsidering learners only as data points within a\\nneural dialog tutoring context may prevent us from\\nseeing the societal and socioeconomic barriers that\\nthey may be up against, thereby running the risk of\\nnot only failing to help relevant learner subgroups\\nbut also sometimes giving additional privileges to\\nthose who use these systems.\\n9 Acknowledgements\\nThis project was made possible by an ETH AI Cen-\\nter Doctoral Fellowship to Jakub Macina with par-\\ntial support by the Asuera Stiftung and the ETH\\nZurich Foundation and has received funding by\\nthe German Federal Ministry of Education and Re-\\nsearch and the Hessian Ministry of Higher Educa-\\ntion, Research, Science and the Arts within their\\njoint support of the National Research Center for\\nApplied Cybersecurity ATHENE. We thank the\\ngroup members and our reviewers for their valu-\\nable feedback.References\\nDaniel Adiwardana, Minh-Thang Luong, David R So,\\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\\net al. 2020. Towards a human-like open-domain\\nchatbot. ArXiv preprint , abs/2001.09977.\\nSiqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng\\nWang, Wenquan Wu, Zhen Guo, Zhibin Liu, and\\nXinchao Xu. 2021. PLATO-2: Towards building\\nan open-domain chatbot via curriculum learning. In\\nFindings of the Association for Computational Lin-\\nguistics: ACL-IJCNLP 2021 , pages 2513–2525, On-\\nline. Association for Computational Linguistics.\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\\nAltman, Simran Arora, Sydney von Arx, Michael S\\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\\nBrunskill, et al. 2021. On the opportunities\\nand risks of foundation models. arXiv preprint\\narXiv:2108.07258 .\\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\\nmadan, and Milica Gaši ´c. 2018. MultiWOZ - a\\nlarge-scale multi-domain Wizard-of-Oz dataset for\\ntask-oriented dialogue modelling. In Proceedings of\\nthe 2018 Conference on Empirical Methods in Nat-\\nural Language Processing , pages 5016–5026, Brus-\\nsels, Belgium. Association for Computational Lin-\\nguistics.\\nAndrew Caines, Helen Yannakoudakis, Helena Ed-\\nmondson, Helen Allen, Pascual Pérez-Paredes, Bill\\nByrne, and Paula Buttery. 2020. The teacher-\\nstudent chatroom corpus. In Proceedings of the 9th\\nWorkshop on NLP for Computer Assisted Language\\nLearning , pages 10–20, Gothenburg, Sweden. LiU\\nElectronic Press.\\nMichelene TH Chi, Nicholas De Leeuw, Mei-Hung\\nChiu, and Christian LaVancher. 1994. Eliciting self-\\nexplanations improves understanding. Cognitive sci-\\nence, 18(3):439–477.\\nMichelene TH Chi and Ruth Wylie. 2014. The icap\\nframework: Linking cognitive engagement to ac-\\ntive learning outcomes. Educational psychologist ,\\n49(4):219–243.\\nAaron Daniel Cohen, Adam Roberts, Alejandra\\nMolina, Alena Butryna, Alicia Jin, Apoorv Kul-\\nshreshtha, Ben Hutchinson, Ben Zevenbergen,\\nBlaise Hilary Aguera-Arcas, Chung ching Chang,\\nClaire Cui, Cosmo Du, Daniel De Freitas Adiwar-\\ndana, Dehao Chen, Dmitry (Dima) Lepikhin, Ed H.\\nChi, Erin Hoffman-John, Heng-Tze Cheng, Hongrae\\nLee, Igor Krivokon, James Qin, Jamie Hall, Joe Fen-\\nton, Johnny Soraker, Kathy Meier-Hellstern, Kris-\\nten Olson, Lora Mois Aroyo, Maarten Paul Bosma,\\nMarc Joseph Pickett, Marcelo Amorim Menegali,\\nMarian Croak, Mark Díaz, Matthew Lamm, Maxim\\nKrikun, Meredith Ringel Morris, Noam Shazeer,\\nQuoc V . Le, Rachel Bernstein, Ravi Rajakumar, Ray\\nKurzweil, Romal Thoppilan, Steven Zheng, Taylor\\nBos, Toju Duke, Tulsee Doshi, Vinodkumar Prab-\\nhakaran, Will Rusch, YaGuang Li, Yanping Huang,\\nYanqi Zhou, Yuanzhong Xu, and Zhifeng Chen.\\n2022. Lamda: Language models for dialog appli-\\ncations. In arXiv .\\nDorottya Demszky, Jing Liu, Zid Mancenido, Julie\\nCohen, Heather Hill, Dan Jurafsky, and Tatsunori\\nHashimoto. 2021. Measuring conversational uptake:\\nA case study on student-teacher interactions. In Pro-\\nceedings of the 59th Annual Meeting of the Associa-\\ntion for Computational Linguistics and the 11th In-\\nternational Joint Conference on Natural Language\\nProcessing (Volume 1: Long Papers) , pages 1638–\\n1653, Online. Association for Computational Lin-\\nguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2019. Wizard\\nof wikipedia: Knowledge-powered conversational\\nagents. In 7th International Conference on Learn-\\ning Representations, ICLR 2019, New Orleans, LA,\\nUSA, May 6-9, 2019 . OpenReview.net.\\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\\nSanchit Agarwal, Shuyang Gao, Adarsh Kumar,\\nAnuj Goyal, Peter Ku, and Dilek Hakkani-Tur. 2020.\\nMultiWOZ 2.1: A consolidated multi-domain dia-\\nlogue dataset with state corrections and state track-\\ning baselines. In Proceedings of the 12th Lan-\\nguage Resources and Evaluation Conference , pages\\n422–428, Marseille, France. European Language Re-\\nsources Association.\\nSong Feng, Hui Wan, Chulaka Gunasekara, Siva\\nPatel, Sachindra Joshi, and Luis Lastras. 2020.\\ndoc2dial: A goal-oriented document-grounded dia-\\nlogue dataset. In Proceedings of the 2020 Confer-\\nence on Empirical Methods in Natural Language\\nProcessing (EMNLP) , pages 8118–8128, Online. As-\\nsociation for Computational Linguistics.\\nScott Freeman, Sarah L Eddy, Miles McDonough,\\nMichelle K Smith, Nnadozie Okoroafor, Hannah\\nJordt, and Mary Pat Wenderoth. 2014. Active learn-\\ning increases student performance in science, engi-\\nneering, and mathematics. Proceedings of the na-\\ntional academy of sciences , 111(23):8410–8415.\\nAnanya Ganesh, Martha Palmer, and Katharina Kann.\\n2021. What would a teacher do? Predicting future\\ntalk moves. In Findings of the Association for Com-\\nputational Linguistics: ACL-IJCNLP 2021 , pages\\n4739–4751, Online. Association for Computational\\nLinguistics.\\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qin-\\nlang Chen, Anna Gottardi, Sanjeev Kwatra, AnuVenkatesh, Raefer Gabriel, and Dilek Hakkani-\\nTür. 2019. Topical-Chat: Towards Knowledge-\\nGrounded Open-Domain Conversations. In Proc. In-\\nterspeech 2019 , pages 1891–1895.\\nArthur C Graesser. 2016. Conversations with autotutor\\nhelp students learn. International Journal of Artiﬁ-\\ncial Intelligence in Education , 26(1):124–132.\\nArthur C Graesser, SIDNEY D’MELLO, and Natalie\\nPerson. 2009. Meta-knowledge in tutoring. In\\nHandbook of metacognition in education , pages\\n373–394. Routledge.\\nArthur C Graesser, Natalie K Person, and Joseph P\\nMagliano. 1995. Collaborative dialogue patterns in\\nnaturalistic one-to-one tutoring. Applied cognitive\\npsychology , 9(6):495–522.\\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.\\nLi. 2016. Incorporating copying mechanism in\\nsequence-to-sequence learning. In Proceedings of\\nthe 54th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 1631–1640, Berlin, Germany. Association for\\nComputational Linguistics.\\nWanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu,\\nZheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei\\nHuang, Luo Si, et al. 2022. Galaxy: A genera-\\ntive pre-trained model for task-oriented dialog with\\nsemi-supervised learning and explicit policy injec-\\ntion. Proceedings of the AAAI Conference on Arti-\\nﬁcial Intelligence .\\nSara Hennessy, Sylvia Rojas-Drummond, Rupert\\nHigham, Ana María Márquez, Fiona Maine,\\nRosa María Ríos, Rocío García-Carrión, Omar Tor-\\nreblanca, and María José Barrera. 2016. Developing\\na coding scheme for analysing classroom dialogue\\nacross educational contexts. Learning, culture and\\nsocial interaction , 9:16–44.\\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\\nNeeman, Idan Szpektor, and Omri Abend. 2021.\\nq2: Evaluating factual consistency in knowledge-\\ngrounded dialogues via question generation and\\nquestion answering. In Proceedings of the 2021\\nConference on Empirical Methods in Natural Lan-\\nguage Processing , pages 7856–7870, Online and\\nPunta Cana, Dominican Republic. Association for\\nComputational Linguistics.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\\nMadotto, and Pascale Fung. 2022. Survey of hallu-\\ncination in natural language generation. ACM Com-\\nputing Surveys .\\nNitish Shirish Keskar, Bryan McCann, Lav Varsh-\\nney, Caiming Xiong, and Richard Socher. 2019.\\nCTRL - A Conditional Transformer Language\\nModel for Controllable Generation. ArXiv preprint ,\\nabs/1909.05858.\\nSeokhwan Kim, Mihail Eric, Karthik Gopalakrishnan,\\nBehnam Hedayatnia, Yang Liu, and Dilek Hakkani-\\nTur. 2020. Beyond domain APIs: Task-oriented con-\\nversational modeling with unstructured knowledge\\naccess. In Proceedings of the 21th Annual Meeting\\nof the Special Interest Group on Discourse and Dia-\\nlogue , pages 278–289, 1st virtual meeting. Associa-\\ntion for Computational Linguistics.\\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\\n2022. Internet-augmented dialogue generation. In\\nProceedings of the 60th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers) , pages 8460–8478, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 7871–7880, Online. Association\\nfor Computational Linguistics.\\nDiane J Litman, Carolyn P Rosé, Kate Forbes-Riley,\\nKurt VanLehn, Dumisizwe Bhembe, and Scott Silli-\\nman. 2006. Spoken versus typed human and com-\\nputer dialogue tutoring. International Journal of Ar-\\ntiﬁcial Intelligence in Education , 16(2):145–170.\\nChristopher J MacLellan and Kenneth R Koedinger.\\n2020. Domain-general tutor authoring with appren-\\ntice learner models. International Journal of Artiﬁ-\\ncial Intelligence in Education , pages 1–42.\\nPierre-Emmanuel Mazaré, Samuel Humeau, Martin\\nRaison, and Antoine Bordes. 2018. Training mil-\\nlions of personalized dialogue agents. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing , pages 2775–2779,\\nBrussels, Belgium. Association for Computational\\nLinguistics.\\nShikib Mehri, Jinho Choi, Luis Fernando D’Haro, Jan\\nDeriu, Maxine Eskenazi, Milica Gasic, Kallirroi\\nGeorgila, Dilek Hakkani-Tur, Zekang Li, Verena\\nRieser, et al. 2022. Report from the nsf future di-\\nrections workshop on automatic evaluation of dialog:\\nResearch directions and challenges. ArXiv preprint ,\\nabs/2203.10012.\\nShikib Mehri and Maxine Eskenazi. 2020. USR: An\\nunsupervised and reference free evaluation metric\\nfor dialog generation. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 681–707, Online. Association for\\nComputational Linguistics.\\nSarah Michaels, Mary Catherine O’Connor,\\nMegan Williams Hall, and Lauren B Resnick.\\n2010. Accountable talk sourcebook: For classroom\\nconversation that works. Pittsburgh, PA: University\\nof Pittsburgh Institute for Learning .Alexander Miller, Will Feng, Dhruv Batra, Antoine\\nBordes, Adam Fisch, Jiasen Lu, Devi Parikh, and\\nJason Weston. 2017. ParlAI: A dialog research soft-\\nware platform. In Proceedings of the 2017 Con-\\nference on Empirical Methods in Natural Language\\nProcessing: System Demonstrations , pages 79–84,\\nCopenhagen, Denmark. Association for Computa-\\ntional Linguistics.\\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\\njen Subba. 2019. OpenDialKG: Explainable conver-\\nsational reasoning with attention-based walks over\\nknowledge graphs. In Proceedings of the 57th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 845–854, Florence, Italy. Associ-\\nation for Computational Linguistics.\\nJohanna D Moore, Kaska Porayska-Pomsta, Sebastian\\nVarges, and Claus Zinn. 2004. Generating tutorial\\nfeedback with affect. In FLAIRS Conference , pages\\n923–928.\\nBenjamin D Nye, Arthur C Graesser, and Xiangen Hu.\\n2014. Autotutor and family: A review of 17 years of\\nnatural language tutoring. International Journal of\\nArtiﬁcial Intelligence in Education , 24(4):427–469.\\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\\ndeh, Lars Liden, and Jianfeng Gao. 2021. Soloist:\\nBuilding task bots at scale with transfer learning and\\nmachine teaching. Transactions of the Association\\nfor Computational Linguistics , 9:807–824.\\nMatt Post. 2018. A call for clarity in reporting BLEU\\nscores. In Proceedings of the Third Conference on\\nMachine Translation: Research Papers , pages 186–\\n191, Belgium, Brussels. Association for Computa-\\ntional Linguistics.\\nLianhui Qin, Sean Welleck, Daniel Khashabi, and\\nYejin Choi. 2022. COLD decoding: Energy-based\\nconstrained text generation with langevin dynamics.\\nInAdvances in Neural Information Processing Sys-\\ntems.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch , 21(140):1–67.\\nHannah Rashkin, David Reitter, Gaurav Singh Tomar,\\nand Dipanjan Das. 2021. Increasing faithfulness\\nin knowledge-grounded dialogue with controllable\\nfeatures. In Proceedings of the 59th Annual Meet-\\ning of the Association for Computational Linguistics\\nand the 11th International Joint Conference on Nat-\\nural Language Processing (Volume 1: Long Papers) ,\\npages 704–718, Online. Association for Computa-\\ntional Linguistics.\\nBrian J. Reiser. 2004. Scaffolding Complex Learning:\\nThe Mechanisms of Structuring and Problematizing\\nStudent Work. Journal of the Learning Sciences ,\\n13(3):273–304. Publisher: Routledge _eprint:\\nhttps://doi.org/10.1207/s15327809jls1303_2.\\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\\nEric Michael Smith, Y-Lan Boureau, and Jason We-\\nston. 2021. Recipes for building an open-domain\\nchatbot. In Proceedings of the 16th Conference of\\nthe European Chapter of the Association for Compu-\\ntational Linguistics: Main Volume , pages 300–325,\\nOnline. Association for Computational Linguistics.\\nJeremy Roschelle and Stephanie D Teasley. 1995. The\\nconstruction of shared knowledge in collaborative\\nproblem solving. In Computer supported collabo-\\nrative learning , pages 69–97. Springer.\\nSherry Ruan, Liwei Jiang, Justin Xu, Bryce Joe-Kun\\nTham, Zhengneng Qiu, Yeshuang Zhu, Elizabeth L.\\nMurnane, Emma Brunskill, and James A. Landay.\\n2019. Quizbot: A dialogue-based adaptive learning\\nsystem for factual knowledge. In Proceedings of the\\n2019 CHI Conference on Human Factors in Com-\\nputing Systems, CHI 2019, Glasgow, Scotland, UK,\\nMay 04-09, 2019 , page 357. ACM.\\nTimo Schick and Hinrich Schütze. 2022. True few-\\nshot learning with prompts—a real-world perspec-\\ntive. Transactions of the Association for Computa-\\ntional Linguistics , 10:716–731.\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\\nand Jason Weston. 2021. Retrieval augmentation\\nreduces hallucination in conversation. In Findings\\nof the Association for Computational Linguistics:\\nEMNLP 2021 , pages 3784–3803, Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\\nEric Michael Smith, Stephen Roller, Megan Ung,\\nMoya Chen, Kushal Arora, Joshua Lane, et al. 2022.\\nBlenderbot 3: a deployed conversational agent that\\ncontinually learns to responsibly engage. ArXiv\\npreprint , abs/2208.03188.\\nTanmay Sinha and Manu Kapur. 2021. When problem\\nsolving followed by instruction works: Evidence for\\nproductive failure. Review of Educational Research ,\\n91(5):761–798.\\nMegha Srivastava and Noah Goodman. 2021. Ques-\\ntion generation for adaptive education. In Proceed-\\nings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (Volume 2: Short Papers) , pages 692–701,\\nOnline. Association for Computational Linguistics.\\nKatherine Stasaski, Kimberly Kao, and Marti A. Hearst.\\n2020. CIMA: A large open access dialogue datasetfor tutoring. In Proceedings of the Fifteenth Work-\\nshop on Innovative Use of NLP for Building Educa-\\ntional Applications , pages 52–64, Seattle, WA, USA\\nâ†’ Online. Association for Computational Linguis-\\ntics.\\nAbhijit Suresh, Jennifer Jacobs, Charis Harty, Mar-\\ngaret Perkoff, James H Martin, and Tamara Sumner.\\n2022a. The talkmoves dataset: K-12 mathematics\\nlesson transcripts annotated for teacher and student\\ndiscursive moves. arXiv preprint arXiv:2204.09652 .\\nAbhijit Suresh, Jennifer Jacobs, Margaret Perkoff,\\nJames H. Martin, and Tamara Sumner. 2022b. Fine-\\ntuning transformers with additional context to clas-\\nsify discursive moves in mathematics classrooms. In\\nProceedings of the 17th Workshop on Innovative Use\\nof NLP for Building Educational Applications (BEA\\n2022) , pages 71–81, Seattle, Washington. Associa-\\ntion for Computational Linguistics.\\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\\nSequence to sequence learning with neural networks.\\nInAdvances in Neural Information Processing Sys-\\ntems 27: Annual Conference on Neural Informa-\\ntion Processing Systems 2014, December 8-13 2014,\\nMontreal, Quebec, Canada , pages 3104–3112.\\nAnaïs Tack and Chris Piech. 2022. The AI Teacher\\nTest: Measuring the Pedagogical Ability of Blender\\nand GPT-3 in Educational Dialogues. In The 15th\\nInternational Conference on Educational Data Min-\\ning, page accepted.\\nTsung-Hsien Wen, David Vandyke, Nikola Mrkši ´c,\\nMilica Gaši ´c, Lina M. Rojas-Barahona, Pei-Hao Su,\\nStefan Ultes, and Steve Young. 2017. A network-\\nbased end-to-end trainable task-oriented dialogue\\nsystem. In Proceedings of the 15th Conference of\\nthe European Chapter of the Association for Compu-\\ntational Linguistics: Volume 1, Long Papers , pages\\n438–449, Valencia, Spain. Association for Computa-\\ntional Linguistics.\\nSebastian Wollny, Jan Schneider, Daniele Di Mitri,\\nJoshua Weidlich, Marc Rittberger, and Hendrik\\nDrachsler. 2021. Are we there yet?-a systematic lit-\\nerature review on chatbots in education. Frontiers in\\nartiﬁcial intelligence , 4.\\nLinting Xue, Noah Constant, Adam Roberts, Mi-\\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\\nBarua, and Colin Raffel. 2021. mT5: A massively\\nmultilingual pre-trained text-to-text transformer. In\\nProceedings of the 2021 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies ,\\npages 483–498, Online. Association for Computa-\\ntional Linguistics.\\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\\nsonalizing dialogue agents: I have a dog, do you\\nhave pets too? In Proceedings of the 56th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 2204–\\n2213, Melbourne, Australia. Association for Com-\\nputational Linguistics.\\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-\\nscale generative pre-training for conversational re-\\nsponse generation. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics: System Demonstrations , pages 270–\\n278, Online. Association for Computational Linguis-\\ntics.\\nRan Zhao, Tanmay Sinha, Alan W Black, and Justine\\nCassell. 2016. Socially-aware virtual agents: Au-\\ntomatically assessing dyadic rapport from temporal\\npatterns of behavior. In International conference on\\nintelligent virtual agents , pages 218–233. Springer.\\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\\nBlack. 2018. A dataset for document grounded con-\\nversations. In Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 708–713, Brussels, Belgium. Association\\nfor Computational Linguistics.\\nA Pedagogical strategy and dialog acts in\\ndialog tutoring\\nFigure 6: Dialogue between a tutor and a student solving\\nan algebra story problem. Key questions are: What teacher\\npedagogical strategies are the best in terms of learning gains\\nof students? How to adapt language models to generate peda-\\ngogically valid responses?\\nIn the context of this paper, we assume that the\\npedagogical strategy is represented using dialog\\nact annotations. An example of the teacher strat-\\negy is providing hints (cf. example in Figure 6),\\nwhere a teacher provides helpful support or clariﬁes\\ngoals to the student. Another example is Probing\\n(cf. example in Figure 4), which prompts students\\nto explain better or reﬂect on the current solution.\\nCIMA contains ﬁve teacher dialog acts - hint, open-\\nended question, correction, conﬁrmation, other .\\nTSCC contains more ﬁne-grained dialog acts such\\naseliciting, scaffolding, enquiry, or recap . From a\\nlearning science standpoint, pedagogical strategy\\ncould be viewed as a global strategy (knowing how\\nto effectively guide students e.g. using questioning\\nor providing contrasting cases) and dialog acts as\\na speciﬁc decision on how this strategy is imple-\\nmented on the local turn-based level.\\nB Equitable tutoring\\nAlthough tutoring is typically conceived as a sce-\\nnario where a subject matter expert works syn-\\nchronously with one or multiple students and takes\\ninterpretive authority, there is increasing empiri-\\ncal evidence supporting the case for incorporating\\nactive learning approaches in the classroom (Free-\\nman et al., 2014; Sinha and Kapur, 2021). With\\ncollaborative creation of knowledge where teach-\\ners position themselves as co-learners and students\\nalso take interpretive authority, such approaches\\nare better poised to build classroom equity than\\nmonologic educational practices where only one\\nvoice (primarily the teacher’s) tends to be heard,legitimized and sometimes imposed. Therefore, if\\nwe rethink of the goals of education as providing\\nopportunities for students to enter into the work-\\nforce with a positive identity about themselves and\\nthe subject matter, equitable tutoring via increased\\nstudent chances to pose ideas, construct knowledge\\nand as a result feel welcomed into the intellectual\\ndiscussion, holds tremendous promise.\\nC Conversational Goals\\nIn this work, we studied only 1:1 dialog tutor-\\ning settings with a speciﬁc focus on the role of\\na teacher/tutor. We focused on the most commonly\\nused goal of dialog tutoring which is a learning aid\\nto support students’ skill development and provide\\nopportunities to learn (Wollny et al., 2021).\\nHowever, teacher-student interactions may en-\\ntail multiple conversational goals that can serve\\ninteractional functions (e.g., turn-taking) and in-\\nterpersonal functions (e.g., rapport-building moves\\nsuch as self-disclosure, praise, social norm viola-\\ntion). Research in human tutoring and collabora-\\ntive learning, more generally, has shown that how\\nstudents manage the task space (consisting of the\\nproblem to be solved, e.g., do I agree with the in-\\nterlocutor’s reasoning?) and the relational space\\n(consisting of the interactional challenges and op-\\nportunities, e.g., can I disagree without threatening\\nthe interlocutor’s face?) is critical to learning out-\\ncomes (Roschelle and Teasley, 1995; Zhao et al.,\\n2016). Neural dialog tutoring, which can account\\nfor this crucial, but fundamental distinction among\\nconversational goals is challenging.\\nContext Generated teacher response\\nT: Please try to ﬁll in the blank in Italian.\\nS: how do you say next?\\nT: Remember that \"is next to the\" is \"e vicino\"\\nS: I don’t know how to say purple. Is it viola?Viola is the word for purple.\\nT: Please try to ﬁll in the blank in Italian.\\nS: how do you say next?\\nT: Remember that \"is next to the\" is \"e vicino\"\\nS: Purple is viola, right?That is correct.\\nT: Please try to ﬁll in the blank in Italian.\\nS: how do you say next?\\nT: Remember that \"is next to the\" is \"e vicino\"\\nS: I think purple could be violett.Viola is the word for purple.\\nT: Please try to ﬁll in the blank in Italian.\\nS: how do you say next?\\nT: Remember that \"is next to the\" is \"e vicino\"\\nS: I can’t remember the color. Can you give me a\\nhint?Purple is viola.\\nT: Please try to ﬁll in the blank in Italian.\\nS: how do you say next?\\nT: Remember that \"is next to the\" is \"e vicino\"\\nS: I guess it is e vicino all’albero. Is it correct to\\nstart the sentence with the preposition?The preposition for \"purple\" is \"viola\"\\nUnderstanding - Meaning mismatch\\nT: \"Blue\" is \"blu\". Please try to ﬁll in the blank in\\nItalian.\\nS: But what are the other words?\\nT: Can you give me your best guess?\\nS: e di fronte all’albero blu ( correct )Almost, tree is all’albero’. Please try again.\\nUnderstanding - Incorrect solution assessment\\nT: \"Blue\" is \"blu\". Please try to ﬁll in the blank in\\nItalian.\\nS: But what are the other words?\\nT: Can you give me your best guess?\\nS: e di fronte blu all’albero ( adjective position\\nswap )Almost, you need to add blu after \"all’albero\" to\\nmake the word \"tree\".\\nUnderstanding - Incorrect translation\\nT: \"Blue\" is \"blu\". Please try to ﬁll in the blank in\\nItalian.\\nS: But what are the other words?\\nT: Can you give me your best guess?\\nS: e di fronte al albero blu ( wrong deﬁnite arti-\\ncle)Almost, tree is all’albero not albero.\\nT: \"Blue\" is \"blu\". Please try to ﬁll in the blank in\\nItalian.\\nS: But what are the other words?\\nT: Can you give me your best guess?\\nS: e di fronte al cane blu ( noun swap )Almost. Al cane is not the correct word for tree.\\nT: \"Blue\" is \"blu\". Please try to ﬁll in the blank in\\nItalian.\\nS: But what are the other words?\\nT: Can you give me your best guess?\\nS: e dietro all’albero blu\" ( wrong preposition )The correct answer is e di fronte all’albero blu.\\nPedagogy - Information reveal\\nTable 8: Examples of rephrased conversations from the qualitative study. Trefers to a teacher utterance, Srefers to a student\\nutterance. Bold text is information for the reader indicating error categories.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    encoding_name=encoder_name,\n",
    "    chunk_size=max_tokens,\n",
    "    chunk_overlap=100,  # let's add a small 2-4 sentence overlap\n",
    "    disallowed_special=()\n",
    ")\n",
    "paper_chunks = text_splitter.split_text(paper)\n",
    "len(paper_chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's integrate this into the previous code to create a new dataset using the `langchain` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cafe7288384ea985fb299767e48f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 818, which is longer than the specified 650\n",
      "Created a chunk of size 805, which is longer than the specified 650\n",
      "Created a chunk of size 783, which is longer than the specified 650\n",
      "Created a chunk of size 884, which is longer than the specified 650\n",
      "Created a chunk of size 769, which is longer than the specified 650\n",
      "Created a chunk of size 766, which is longer than the specified 650\n",
      "Created a chunk of size 952, which is longer than the specified 650\n",
      "Created a chunk of size 1410, which is longer than the specified 650\n",
      "Created a chunk of size 1346, which is longer than the specified 650\n",
      "Created a chunk of size 976, which is longer than the specified 650\n",
      "Created a chunk of size 748, which is longer than the specified 650\n",
      "Created a chunk of size 742, which is longer than the specified 650\n",
      "Created a chunk of size 893, which is longer than the specified 650\n",
      "Created a chunk of size 835, which is longer than the specified 650\n",
      "Created a chunk of size 759, which is longer than the specified 650\n",
      "Created a chunk of size 952, which is longer than the specified 650\n",
      "Created a chunk of size 825, which is longer than the specified 650\n",
      "Created a chunk of size 994, which is longer than the specified 650\n",
      "Created a chunk of size 728, which is longer than the specified 650\n",
      "Created a chunk of size 691, which is longer than the specified 650\n",
      "Created a chunk of size 744, which is longer than the specified 650\n",
      "Created a chunk of size 748, which is longer than the specified 650\n",
      "Created a chunk of size 790, which is longer than the specified 650\n",
      "Created a chunk of size 910, which is longer than the specified 650\n",
      "Created a chunk of size 782, which is longer than the specified 650\n",
      "Created a chunk of size 782, which is longer than the specified 650\n",
      "Created a chunk of size 792, which is longer than the specified 650\n",
      "Created a chunk of size 658, which is longer than the specified 650\n",
      "Created a chunk of size 896, which is longer than the specified 650\n",
      "Created a chunk of size 1950, which is longer than the specified 650\n",
      "Created a chunk of size 745, which is longer than the specified 650\n",
      "Created a chunk of size 826, which is longer than the specified 650\n",
      "Created a chunk of size 800, which is longer than the specified 650\n",
      "Created a chunk of size 818, which is longer than the specified 650\n",
      "Created a chunk of size 1297, which is longer than the specified 650\n",
      "Created a chunk of size 1465, which is longer than the specified 650\n",
      "Created a chunk of size 1682, which is longer than the specified 650\n",
      "Created a chunk of size 1533, which is longer than the specified 650\n",
      "Created a chunk of size 1247, which is longer than the specified 650\n",
      "Created a chunk of size 893, which is longer than the specified 650\n",
      "Created a chunk of size 1827, which is longer than the specified 650\n",
      "Created a chunk of size 1006, which is longer than the specified 650\n",
      "Created a chunk of size 732, which is longer than the specified 650\n",
      "Created a chunk of size 1977, which is longer than the specified 650\n",
      "Created a chunk of size 2651, which is longer than the specified 650\n",
      "Created a chunk of size 1184, which is longer than the specified 650\n",
      "Created a chunk of size 788, which is longer than the specified 650\n",
      "Created a chunk of size 1157, which is longer than the specified 650\n",
      "Created a chunk of size 1933, which is longer than the specified 650\n",
      "Created a chunk of size 768, which is longer than the specified 650\n",
      "Created a chunk of size 797, which is longer than the specified 650\n",
      "Created a chunk of size 993, which is longer than the specified 650\n",
      "Created a chunk of size 910, which is longer than the specified 650\n",
      "Created a chunk of size 810, which is longer than the specified 650\n",
      "Created a chunk of size 657, which is longer than the specified 650\n",
      "Created a chunk of size 1321, which is longer than the specified 650\n",
      "Created a chunk of size 1290, which is longer than the specified 650\n",
      "Created a chunk of size 907, which is longer than the specified 650\n",
      "Created a chunk of size 897, which is longer than the specified 650\n",
      "Created a chunk of size 903, which is longer than the specified 650\n",
      "Created a chunk of size 935, which is longer than the specified 650\n",
      "Created a chunk of size 1327, which is longer than the specified 650\n",
      "Created a chunk of size 783, which is longer than the specified 650\n",
      "Created a chunk of size 827, which is longer than the specified 650\n",
      "Created a chunk of size 3622, which is longer than the specified 650\n",
      "Created a chunk of size 2689, which is longer than the specified 650\n",
      "Created a chunk of size 2652, which is longer than the specified 650\n",
      "Created a chunk of size 2650, which is longer than the specified 650\n",
      "Created a chunk of size 2662, which is longer than the specified 650\n",
      "Created a chunk of size 3622, which is longer than the specified 650\n",
      "Created a chunk of size 2689, which is longer than the specified 650\n",
      "Created a chunk of size 2652, which is longer than the specified 650\n",
      "Created a chunk of size 2650, which is longer than the specified 650\n",
      "Created a chunk of size 2662, which is longer than the specified 650\n",
      "Created a chunk of size 3622, which is longer than the specified 650\n",
      "Created a chunk of size 2689, which is longer than the specified 650\n",
      "Created a chunk of size 2652, which is longer than the specified 650\n",
      "Created a chunk of size 2650, which is longer than the specified 650\n",
      "Created a chunk of size 2667, which is longer than the specified 650\n",
      "Created a chunk of size 822, which is longer than the specified 650\n",
      "Created a chunk of size 909, which is longer than the specified 650\n",
      "Created a chunk of size 1008, which is longer than the specified 650\n",
      "Created a chunk of size 867, which is longer than the specified 650\n",
      "Created a chunk of size 796, which is longer than the specified 650\n",
      "Created a chunk of size 733, which is longer than the specified 650\n",
      "Created a chunk of size 673, which is longer than the specified 650\n",
      "Created a chunk of size 808, which is longer than the specified 650\n",
      "Created a chunk of size 658, which is longer than the specified 650\n",
      "Created a chunk of size 843, which is longer than the specified 650\n",
      "Created a chunk of size 738, which is longer than the specified 650\n",
      "Created a chunk of size 864, which is longer than the specified 650\n",
      "Created a chunk of size 966, which is longer than the specified 650\n",
      "Created a chunk of size 930, which is longer than the specified 650\n",
      "Created a chunk of size 936, which is longer than the specified 650\n",
      "Created a chunk of size 1054, which is longer than the specified 650\n",
      "Created a chunk of size 734, which is longer than the specified 650\n",
      "Created a chunk of size 715, which is longer than the specified 650\n",
      "Created a chunk of size 810, which is longer than the specified 650\n",
      "Created a chunk of size 718, which is longer than the specified 650\n",
      "Created a chunk of size 1006, which is longer than the specified 650\n",
      "Created a chunk of size 761, which is longer than the specified 650\n",
      "Created a chunk of size 834, which is longer than the specified 650\n",
      "Created a chunk of size 669, which is longer than the specified 650\n",
      "Created a chunk of size 720, which is longer than the specified 650\n",
      "Created a chunk of size 783, which is longer than the specified 650\n",
      "Created a chunk of size 849, which is longer than the specified 650\n",
      "Created a chunk of size 881, which is longer than the specified 650\n",
      "Created a chunk of size 2127, which is longer than the specified 650\n",
      "Created a chunk of size 2139, which is longer than the specified 650\n",
      "Created a chunk of size 716, which is longer than the specified 650\n",
      "Created a chunk of size 729, which is longer than the specified 650\n",
      "Created a chunk of size 797, which is longer than the specified 650\n",
      "Created a chunk of size 791, which is longer than the specified 650\n",
      "Created a chunk of size 3141, which is longer than the specified 650\n"
     ]
    }
   ],
   "source": [
    "langchain_dataset = []\n",
    "\n",
    "for paper_path in tqdm(paper_paths):\n",
    "    doi = paper_path.split('/')[-1][:-4]\n",
    "    with open(paper_path) as f:\n",
    "        paper = f.read()\n",
    "    # clean text\n",
    "    paper = clean_text(paper)\n",
    "    paper_chunks = text_splitter.split_text(paper)\n",
    "    for i, chunk in enumerate(paper_chunks):\n",
    "        langchain_dataset.append({\n",
    "            'doi': doi,\n",
    "            'chunk-id': str(i),\n",
    "            'chunk': chunk\n",
    "        })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we're seeing these errors is because the `text_splitter` only splits on `\\n` characters — and it seems that there are instances where there are *no* `\\n` characters for more than `650` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk too long: 676\n"
     ]
    }
   ],
   "source": [
    "for i, record in enumerate(langchain_dataset):\n",
    "    chunk = record['chunk']\n",
    "    if len(tokenizer.encode(chunk)) > max_tokens:\n",
    "        print(f'Chunk too long: {len(tokenizer.encode(chunk))}')\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerningly, this `678` length chunk didn't seem to be raised in the log of longer than specified length chunks above. Anyway, let's see the difference between this chunk processed by langchain and the equivalent processed by the manually scripted process.\n",
    "\n",
    "But for now let's just handle all of these by identifying excessively long chunks (`676` is probably fine) and breaking them up using another `text_splitter` that will split on anything (ie it splits by tokens only, not with a specific separator in mind):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=encoder_name,\n",
    "    chunk_size=max_tokens,\n",
    "    chunk_overlap=100  # let's add a small 2-4 sentence overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk too long: 884\n",
      "2212.09710 1 split into 2 chunks\n",
      "Chunk too long: 951\n",
      "2301.06173 3 split into 2 chunks\n",
      "Chunk too long: 906\n",
      "2212.12454 0 split into 2 chunks\n",
      "Chunk too long: 1047\n",
      "2212.12454 1 split into 2 chunks\n",
      "Chunk too long: 1037\n",
      "2212.12454 2 split into 2 chunks\n",
      "Chunk too long: 977\n",
      "2212.12454 3 split into 2 chunks\n",
      "Chunk too long: 958\n",
      "2212.12454 4 split into 2 chunks\n",
      "Chunk too long: 1043\n",
      "2212.12454 7 split into 2 chunks\n",
      "Chunk too long: 1035\n",
      "2212.12454 8 split into 2 chunks\n",
      "Chunk too long: 869\n",
      "2212.12454 9 split into 2 chunks\n",
      "Chunk too long: 919\n",
      "2212.12454 10 split into 2 chunks\n",
      "Chunk too long: 880\n",
      "2212.12454 14 split into 2 chunks\n",
      "Chunk too long: 868\n",
      "2212.12454 16 split into 2 chunks\n",
      "Chunk too long: 1410\n",
      "2302.00456 3 split into 3 chunks\n",
      "Chunk too long: 1346\n",
      "2302.00456 14 split into 3 chunks\n",
      "Chunk too long: 975\n",
      "2301.07491 0 split into 2 chunks\n",
      "Chunk too long: 1506\n",
      "2301.07491 3 split into 3 chunks\n",
      "Chunk too long: 892\n",
      "2301.00671 3 split into 2 chunks\n",
      "Chunk too long: 951\n",
      "2301.00671 6 split into 2 chunks\n",
      "Chunk too long: 993\n",
      "2301.00671 10 split into 2 chunks\n",
      "Chunk too long: 909\n",
      "2301.00671 16 split into 2 chunks\n",
      "Chunk too long: 975\n",
      "2301.01181 0 split into 2 chunks\n",
      "Chunk too long: 938\n",
      "2301.01181 1 split into 2 chunks\n",
      "Chunk too long: 929\n",
      "2301.01181 3 split into 2 chunks\n",
      "Chunk too long: 862\n",
      "2301.01181 4 split into 2 chunks\n",
      "Chunk too long: 919\n",
      "2301.01181 5 split into 2 chunks\n",
      "Chunk too long: 994\n",
      "2301.01181 9 split into 2 chunks\n",
      "Chunk too long: 896\n",
      "2212.14546 8 split into 2 chunks\n",
      "Chunk too long: 1950\n",
      "2301.13867 33 split into 4 chunks\n",
      "Chunk too long: 1001\n",
      "2301.13668 0 split into 2 chunks\n",
      "Chunk too long: 1054\n",
      "2301.13668 1 split into 2 chunks\n",
      "Chunk too long: 961\n",
      "2301.13668 2 split into 2 chunks\n",
      "Chunk too long: 1011\n",
      "2301.13668 3 split into 2 chunks\n",
      "Chunk too long: 965\n",
      "2301.13668 4 split into 2 chunks\n",
      "Chunk too long: 950\n",
      "2301.13668 5 split into 2 chunks\n",
      "Chunk too long: 1008\n",
      "2301.13668 6 split into 2 chunks\n",
      "Chunk too long: 1029\n",
      "2301.13668 7 split into 2 chunks\n",
      "Chunk too long: 898\n",
      "2301.13668 8 split into 2 chunks\n",
      "Chunk too long: 1295\n",
      "2301.02773 7 split into 3 chunks\n",
      "Chunk too long: 1787\n",
      "2301.02773 8 split into 4 chunks\n",
      "Chunk too long: 1465\n",
      "2302.01676 0 split into 3 chunks\n",
      "Chunk too long: 1682\n",
      "2302.01676 1 split into 4 chunks\n",
      "Chunk too long: 1533\n",
      "2302.01676 2 split into 3 chunks\n",
      "Chunk too long: 1247\n",
      "2302.01676 4 split into 3 chunks\n",
      "Chunk too long: 893\n",
      "2302.01676 5 split into 2 chunks\n",
      "Chunk too long: 1827\n",
      "2302.01676 7 split into 4 chunks\n",
      "Chunk too long: 1006\n",
      "2302.01676 8 split into 2 chunks\n",
      "Chunk too long: 1977\n",
      "2302.01676 10 split into 4 chunks\n",
      "Chunk too long: 2651\n",
      "2302.01676 11 split into 5 chunks\n",
      "Chunk too long: 1184\n",
      "2302.01676 13 split into 3 chunks\n",
      "Chunk too long: 1157\n",
      "2212.10392 8 split into 3 chunks\n",
      "Chunk too long: 1933\n",
      "2212.10392 10 split into 4 chunks\n",
      "Chunk too long: 993\n",
      "2301.06841 13 split into 2 chunks\n",
      "Chunk too long: 909\n",
      "2301.06841 15 split into 2 chunks\n",
      "Chunk too long: 1321\n",
      "2212.14486 8 split into 3 chunks\n",
      "Chunk too long: 1290\n",
      "2301.01067 13 split into 3 chunks\n",
      "Chunk too long: 907\n",
      "2302.00402 6 split into 2 chunks\n",
      "Chunk too long: 897\n",
      "2302.00402 8 split into 2 chunks\n",
      "Chunk too long: 903\n",
      "2302.00402 11 split into 2 chunks\n",
      "Chunk too long: 935\n",
      "2302.00402 14 split into 2 chunks\n",
      "Chunk too long: 1326\n",
      "2302.01536 6 split into 3 chunks\n",
      "Chunk too long: 933\n",
      "2301.12971 40 split into 2 chunks\n",
      "Chunk too long: 930\n",
      "2301.12971 41 split into 2 chunks\n",
      "Chunk too long: 929\n",
      "2301.12971 42 split into 2 chunks\n",
      "Chunk too long: 923\n",
      "2301.12971 43 split into 2 chunks\n",
      "Chunk too long: 928\n",
      "2301.12971 44 split into 2 chunks\n",
      "Chunk too long: 930\n",
      "2301.12971 45 split into 2 chunks\n",
      "Chunk too long: 921\n",
      "2301.12971 46 split into 2 chunks\n",
      "Chunk too long: 919\n",
      "2301.12971 47 split into 2 chunks\n",
      "Chunk too long: 927\n",
      "2301.12971 48 split into 2 chunks\n",
      "Chunk too long: 919\n",
      "2301.12971 49 split into 2 chunks\n",
      "Chunk too long: 926\n",
      "2301.12971 50 split into 2 chunks\n",
      "Chunk too long: 930\n",
      "2301.12971 51 split into 2 chunks\n",
      "Chunk too long: 937\n",
      "2301.12971 52 split into 2 chunks\n",
      "Chunk too long: 3622\n",
      "2301.04647 3 split into 7 chunks\n",
      "Chunk too long: 2689\n",
      "2301.04647 4 split into 5 chunks\n",
      "Chunk too long: 2652\n",
      "2301.04647 5 split into 5 chunks\n",
      "Chunk too long: 2650\n",
      "2301.04647 6 split into 5 chunks\n",
      "Chunk too long: 2662\n",
      "2301.04647 7 split into 5 chunks\n",
      "Chunk too long: 3622\n",
      "2301.04647 9 split into 7 chunks\n",
      "Chunk too long: 2689\n",
      "2301.04647 10 split into 5 chunks\n",
      "Chunk too long: 2652\n",
      "2301.04647 11 split into 5 chunks\n",
      "Chunk too long: 2650\n",
      "2301.04647 12 split into 5 chunks\n",
      "Chunk too long: 2662\n",
      "2301.04647 13 split into 5 chunks\n",
      "Chunk too long: 3622\n",
      "2301.04647 15 split into 7 chunks\n",
      "Chunk too long: 2689\n",
      "2301.04647 16 split into 5 chunks\n",
      "Chunk too long: 2652\n",
      "2301.04647 17 split into 5 chunks\n",
      "Chunk too long: 2650\n",
      "2301.04647 18 split into 5 chunks\n",
      "Chunk too long: 2667\n",
      "2301.04647 19 split into 5 chunks\n",
      "Chunk too long: 861\n",
      "2301.04521 5 split into 2 chunks\n",
      "Chunk too long: 873\n",
      "2301.04521 6 split into 2 chunks\n",
      "Chunk too long: 895\n",
      "2301.04521 7 split into 2 chunks\n",
      "Chunk too long: 862\n",
      "2301.04521 12 split into 2 chunks\n",
      "Chunk too long: 863\n",
      "2301.04521 13 split into 2 chunks\n",
      "Chunk too long: 909\n",
      "2301.00395 17 split into 2 chunks\n",
      "Chunk too long: 1008\n",
      "2301.00395 18 split into 2 chunks\n",
      "Chunk too long: 867\n",
      "2212.13492 13 split into 2 chunks\n",
      "Chunk too long: 863\n",
      "2212.13860 21 split into 2 chunks\n",
      "Chunk too long: 965\n",
      "2212.13860 22 split into 2 chunks\n",
      "Chunk too long: 929\n",
      "2301.11847 14 split into 2 chunks\n",
      "Chunk too long: 935\n",
      "2301.11847 17 split into 2 chunks\n",
      "Chunk too long: 1053\n",
      "2301.11847 18 split into 2 chunks\n",
      "Chunk too long: 1005\n",
      "2301.04752 7 split into 2 chunks\n",
      "Chunk too long: 880\n",
      "2301.04752 21 split into 2 chunks\n",
      "Chunk too long: 1193\n",
      "2301.04752 22 split into 3 chunks\n",
      "Chunk too long: 923\n",
      "2301.09896 0 split into 2 chunks\n",
      "Chunk too long: 1020\n",
      "2301.09896 1 split into 2 chunks\n",
      "Chunk too long: 983\n",
      "2301.09896 2 split into 2 chunks\n",
      "Chunk too long: 990\n",
      "2301.09896 3 split into 2 chunks\n",
      "Chunk too long: 1017\n",
      "2301.09896 4 split into 2 chunks\n",
      "Chunk too long: 940\n",
      "2301.09896 5 split into 2 chunks\n",
      "Chunk too long: 1015\n",
      "2301.09896 6 split into 2 chunks\n",
      "Chunk too long: 903\n",
      "2301.09896 7 split into 2 chunks\n",
      "Chunk too long: 2126\n",
      "2302.01025 13 split into 4 chunks\n",
      "Chunk too long: 2138\n",
      "2302.01025 15 split into 4 chunks\n",
      "Chunk too long: 943\n",
      "2301.10095 0 split into 2 chunks\n",
      "Chunk too long: 896\n",
      "2301.10095 1 split into 2 chunks\n",
      "Chunk too long: 858\n",
      "2301.10095 2 split into 2 chunks\n",
      "Chunk too long: 974\n",
      "2301.10095 3 split into 2 chunks\n",
      "Chunk too long: 933\n",
      "2301.10095 4 split into 2 chunks\n",
      "Chunk too long: 913\n",
      "2301.10095 5 split into 2 chunks\n",
      "Chunk too long: 903\n",
      "2301.10095 6 split into 2 chunks\n",
      "Chunk too long: 865\n",
      "2301.10095 7 split into 2 chunks\n",
      "Chunk too long: 977\n",
      "2301.10095 8 split into 2 chunks\n",
      "Chunk too long: 1001\n",
      "2301.10095 9 split into 2 chunks\n",
      "Chunk too long: 967\n",
      "2301.10095 10 split into 2 chunks\n",
      "Chunk too long: 919\n",
      "2301.10095 11 split into 2 chunks\n",
      "Chunk too long: 878\n",
      "2301.10095 12 split into 2 chunks\n",
      "Chunk too long: 946\n",
      "2301.10095 13 split into 2 chunks\n",
      "Chunk too long: 973\n",
      "2301.10095 14 split into 2 chunks\n",
      "Chunk too long: 990\n",
      "2301.10095 15 split into 2 chunks\n",
      "Chunk too long: 975\n",
      "2301.10095 16 split into 2 chunks\n",
      "Chunk too long: 989\n",
      "2301.10095 17 split into 2 chunks\n",
      "Chunk too long: 952\n",
      "2301.10095 18 split into 2 chunks\n",
      "Chunk too long: 956\n",
      "2301.10095 19 split into 2 chunks\n",
      "Chunk too long: 944\n",
      "2301.10095 20 split into 2 chunks\n",
      "Chunk too long: 878\n",
      "2301.10095 21 split into 2 chunks\n",
      "Chunk too long: 971\n",
      "2301.10095 22 split into 2 chunks\n",
      "Chunk too long: 965\n",
      "2301.10095 23 split into 2 chunks\n",
      "Chunk too long: 944\n",
      "2301.10095 24 split into 2 chunks\n",
      "Chunk too long: 979\n",
      "2301.10095 25 split into 2 chunks\n",
      "Chunk too long: 878\n",
      "2301.10095 26 split into 2 chunks\n",
      "Chunk too long: 882\n",
      "2301.10095 27 split into 2 chunks\n",
      "Chunk too long: 999\n",
      "2301.10095 28 split into 2 chunks\n",
      "Chunk too long: 926\n",
      "2301.10095 29 split into 2 chunks\n",
      "Chunk too long: 932\n",
      "2301.10095 30 split into 2 chunks\n",
      "Chunk too long: 948\n",
      "2301.10095 31 split into 2 chunks\n",
      "Chunk too long: 933\n",
      "2301.10095 32 split into 2 chunks\n",
      "Chunk too long: 954\n",
      "2301.10095 33 split into 2 chunks\n",
      "Chunk too long: 945\n",
      "2301.10095 34 split into 2 chunks\n",
      "Chunk too long: 906\n",
      "2301.10095 35 split into 2 chunks\n",
      "Chunk too long: 931\n",
      "2301.10095 36 split into 2 chunks\n",
      "Chunk too long: 867\n",
      "2301.10095 37 split into 2 chunks\n",
      "Chunk too long: 903\n",
      "2212.11182 13 split into 2 chunks\n",
      "Chunk too long: 853\n",
      "2212.11182 14 split into 2 chunks\n",
      "Chunk too long: 875\n",
      "2212.10879 9 split into 2 chunks\n",
      "Chunk too long: 868\n",
      "2212.10879 28 split into 2 chunks\n",
      "Chunk too long: 946\n",
      "2212.10879 29 split into 2 chunks\n",
      "Chunk too long: 936\n",
      "2212.10879 30 split into 2 chunks\n",
      "Chunk too long: 881\n",
      "2212.10879 34 split into 2 chunks\n",
      "Chunk too long: 900\n",
      "2301.03953 18 split into 2 chunks\n",
      "Chunk too long: 866\n",
      "2301.03953 19 split into 2 chunks\n",
      "Chunk too long: 961\n",
      "2301.06340 0 split into 2 chunks\n",
      "Chunk too long: 1053\n",
      "2301.06340 1 split into 2 chunks\n",
      "Chunk too long: 1043\n",
      "2301.06340 2 split into 2 chunks\n",
      "Chunk too long: 1050\n",
      "2301.06340 3 split into 2 chunks\n",
      "Chunk too long: 1066\n",
      "2301.06340 4 split into 2 chunks\n",
      "Chunk too long: 1067\n",
      "2301.06340 5 split into 2 chunks\n",
      "Chunk too long: 1053\n",
      "2301.06340 6 split into 2 chunks\n",
      "Chunk too long: 1056\n",
      "2301.06340 7 split into 2 chunks\n",
      "Chunk too long: 1048\n",
      "2301.06340 8 split into 2 chunks\n",
      "Chunk too long: 1032\n",
      "2301.06340 9 split into 2 chunks\n",
      "Chunk too long: 1042\n",
      "2301.06340 10 split into 2 chunks\n",
      "Chunk too long: 1066\n",
      "2301.06340 11 split into 2 chunks\n",
      "Chunk too long: 1081\n",
      "2301.06340 12 split into 2 chunks\n",
      "Chunk too long: 1069\n",
      "2301.06340 13 split into 2 chunks\n",
      "Chunk too long: 1090\n",
      "2301.06340 14 split into 2 chunks\n",
      "Chunk too long: 1058\n",
      "2301.06340 15 split into 2 chunks\n",
      "Chunk too long: 1039\n",
      "2301.06340 16 split into 2 chunks\n",
      "Chunk too long: 1001\n",
      "2301.06340 17 split into 2 chunks\n",
      "Chunk too long: 1016\n",
      "2301.06340 18 split into 2 chunks\n",
      "Chunk too long: 1022\n",
      "2301.06340 19 split into 2 chunks\n",
      "Chunk too long: 1054\n",
      "2301.06340 20 split into 2 chunks\n",
      "Chunk too long: 998\n",
      "2301.06340 21 split into 2 chunks\n",
      "Chunk too long: 1034\n",
      "2301.06340 22 split into 2 chunks\n",
      "Chunk too long: 1060\n",
      "2301.06340 23 split into 2 chunks\n",
      "Chunk too long: 1025\n",
      "2301.06340 24 split into 2 chunks\n",
      "Chunk too long: 1042\n",
      "2301.06340 25 split into 2 chunks\n",
      "Chunk too long: 1046\n",
      "2301.06340 26 split into 2 chunks\n",
      "Chunk too long: 1079\n",
      "2301.06340 27 split into 2 chunks\n",
      "Chunk too long: 1086\n",
      "2301.06340 28 split into 2 chunks\n",
      "Chunk too long: 1054\n",
      "2301.06340 29 split into 2 chunks\n",
      "Chunk too long: 1071\n",
      "2301.06340 30 split into 2 chunks\n",
      "Chunk too long: 991\n",
      "2301.06340 31 split into 2 chunks\n",
      "Chunk too long: 926\n",
      "2301.06340 32 split into 2 chunks\n",
      "Chunk too long: 901\n",
      "2301.06340 33 split into 2 chunks\n",
      "Chunk too long: 923\n",
      "2301.06340 34 split into 2 chunks\n",
      "Chunk too long: 919\n",
      "2301.06340 35 split into 2 chunks\n",
      "Chunk too long: 915\n",
      "2301.06340 36 split into 2 chunks\n",
      "Chunk too long: 890\n",
      "2301.06340 37 split into 2 chunks\n",
      "Chunk too long: 907\n",
      "2301.06340 38 split into 2 chunks\n",
      "Chunk too long: 924\n",
      "2301.06340 39 split into 2 chunks\n",
      "Chunk too long: 924\n",
      "2301.06340 40 split into 2 chunks\n",
      "Chunk too long: 934\n",
      "2301.06340 41 split into 2 chunks\n",
      "Chunk too long: 923\n",
      "2301.06340 42 split into 2 chunks\n",
      "Chunk too long: 924\n",
      "2301.06340 43 split into 2 chunks\n",
      "Chunk too long: 3141\n",
      "2301.06009 85 split into 6 chunks\n"
     ]
    }
   ],
   "source": [
    "for i, record in enumerate(langchain_dataset):\n",
    "    chunk = record['chunk']\n",
    "    chunk_len = len(tokenizer.encode(chunk, disallowed_special=()))\n",
    "    if chunk_len > max_tokens+200:\n",
    "        print(f'Chunk too long: {chunk_len}')\n",
    "        # break into smaller chunks\n",
    "        mini_chunks = text_splitter.split_text(chunk, disallowed_special=())\n",
    "        # the first chunk must replace the original chunk\n",
    "        langchain_dataset[i]['chunk'] = mini_chunks[0]\n",
    "        # the rest of the chunks must be added to the dataset\n",
    "        for j, mini_chunk in enumerate(mini_chunks[1:]):\n",
    "            langchain_dataset.append({\n",
    "                'doi': record['doi'],\n",
    "                'chunk-id': f\"{record['chunk-id']}-{j}\",\n",
    "                'chunk': mini_chunk\n",
    "            })\n",
    "        print(f\"{record['doi']} {record['chunk-id']} split into {len(mini_chunks)} chunks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test length again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, record in enumerate(langchain_dataset):\n",
    "    chunk = record['chunk']\n",
    "    chunk_len = len(tokenizer.encode(chunk, disallowed_special=()))\n",
    "    if chunk_len > max_tokens+200:\n",
    "        print(f'Chunk too long: {chunk_len}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great, no excessively long chunks. Now we save to file as a JSONL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dataset.jsonl', 'w') as fp:\n",
    "    for record in langchain_dataset:\n",
    "        fp.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
