{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "In this notebook will use LangChain to setup a Pinecone vector DB and embed our arXiv papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain openai pinecone-client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "We start by loading the data that we'll be indexing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27051"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('dataset.jsonl', 'r') as fp:\n",
    "    dataset = [json.loads(line) for line in fp]\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the text itself, but also the metadata associated with each item, that being the `doi` and `chunk-id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [d['chunk'] for d in dataset]\n",
    "ids = [f\"{d['doi']}-{d['chunk-id']}\" for d in dataset]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build embeddings using our text and OpenAI's `text-embedding-ada-002` model. For this we use the embeddings util in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    document_model_name=model_name,\n",
    "    query_model_name=model_name,\n",
    "    openai_api_key='OPENAI_KEY'  # get at platform.openai.com\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.02255261316895485,\n",
       "  0.011016451753675938,\n",
       "  -0.003969615325331688,\n",
       "  -0.023044968023896217,\n",
       "  0.005108187440782785,\n",
       "  0.03769254311919212,\n",
       "  -0.022210698574781418,\n",
       "  -0.01309528574347496,\n",
       "  -0.036653123795986176,\n",
       "  -0.021075546741485596,\n",
       "  -0.005826205480843782,\n",
       "  0.03739165887236595,\n",
       "  -0.011180570349097252,\n",
       "  0.0021814079955220222,\n",
       "  0.012931167148053646,\n",
       "  0.013457714579999447,\n",
       "  0.009081222116947174,\n",
       "  0.00023976682859938592,\n",
       "  0.014319336041808128,\n",
       "  -0.00649635586887598,\n",
       "  -0.016890525817871094,\n",
       "  0.002629314549267292,\n",
       "  -0.013984261080622673,\n",
       "  -0.005986904725432396,\n",
       "  -0.006147604435682297,\n",
       "  -0.008575189858675003,\n",
       "  0.014729632064700127,\n",
       "  -0.006513451691716909,\n",
       "  0.0013710730709135532,\n",
       "  -0.03140133246779442,\n",
       "  0.013587641529738903,\n",
       "  -0.021526873111724854,\n",
       "  -0.008828205987811089,\n",
       "  -0.01509889867156744,\n",
       "  -0.022538935765624046,\n",
       "  0.005795433185994625,\n",
       "  -0.0024446812458336353,\n",
       "  -0.01842229813337326,\n",
       "  0.04067402705550194,\n",
       "  -0.02580762840807438,\n",
       "  0.006865622475743294,\n",
       "  0.00725540379062295,\n",
       "  0.01883259415626526,\n",
       "  -0.007084446959197521,\n",
       "  -0.005179989151656628,\n",
       "  -0.009176957421004772,\n",
       "  0.0097103426232934,\n",
       "  -0.011255791410803795,\n",
       "  -0.005665506236255169,\n",
       "  0.008759823627769947,\n",
       "  0.03323398903012276,\n",
       "  0.005621057469397783,\n",
       "  -0.02309967391192913,\n",
       "  -0.01123527716845274,\n",
       "  -0.004691052716225386,\n",
       "  0.009272693656384945,\n",
       "  -0.010195859707891941,\n",
       "  0.022238051518797874,\n",
       "  0.01653493568301201,\n",
       "  -0.0032977554947137833,\n",
       "  0.004191859159618616,\n",
       "  -0.0036721506621688604,\n",
       "  -0.013628670945763588,\n",
       "  0.0011240405729040504,\n",
       "  -0.01343036163598299,\n",
       "  -0.008034966886043549,\n",
       "  -0.012411459349095821,\n",
       "  -0.022962909191846848,\n",
       "  0.012623445130884647,\n",
       "  0.04619934782385826,\n",
       "  0.023537322878837585,\n",
       "  0.012896976433694363,\n",
       "  -0.010968584567308426,\n",
       "  0.0061065745539963245,\n",
       "  0.02129437029361725,\n",
       "  0.011385718360543251,\n",
       "  -0.01240462064743042,\n",
       "  0.0009317141957581043,\n",
       "  0.012575577944517136,\n",
       "  0.0010325786424800754,\n",
       "  0.02643674984574318,\n",
       "  -0.033890463411808014,\n",
       "  0.006725437939167023,\n",
       "  0.012014839798212051,\n",
       "  0.01578272506594658,\n",
       "  -0.015550225041806698,\n",
       "  -0.012726019136607647,\n",
       "  0.031346626579761505,\n",
       "  -0.02627263218164444,\n",
       "  -0.011016451753675938,\n",
       "  0.008944456465542316,\n",
       "  0.017724795266985893,\n",
       "  0.00395593885332346,\n",
       "  0.014880074188113213,\n",
       "  0.005795433185994625,\n",
       "  0.02560248039662838,\n",
       "  -0.0014454391784965992,\n",
       "  0.04185020923614502,\n",
       "  0.0037268567830324173,\n",
       "  -0.020159218460321426,\n",
       "  0.0036208636593073606,\n",
       "  0.018326561897993088,\n",
       "  -0.009744534268975258,\n",
       "  -0.00530307786539197,\n",
       "  -0.036762535572052,\n",
       "  -0.010866010561585426,\n",
       "  0.0044859047047793865,\n",
       "  -0.0097103426232934,\n",
       "  0.009081222116947174,\n",
       "  0.0014497131342068315,\n",
       "  -0.009717181324958801,\n",
       "  0.04152197390794754,\n",
       "  -0.00038550744648091495,\n",
       "  -0.01992671750485897,\n",
       "  0.006817754823714495,\n",
       "  0.0077545978128910065,\n",
       "  0.007303271908313036,\n",
       "  -0.014934780076146126,\n",
       "  -0.020624220371246338,\n",
       "  -0.011385718360543251,\n",
       "  -0.0047423397190868855,\n",
       "  0.02822837606072426,\n",
       "  0.020528484135866165,\n",
       "  -0.0035422234795987606,\n",
       "  0.008233276195824146,\n",
       "  0.0021814079955220222,\n",
       "  -0.000584244669880718,\n",
       "  -0.02651880867779255,\n",
       "  -0.01338933128863573,\n",
       "  -0.002208760939538479,\n",
       "  0.030061032623052597,\n",
       "  0.014565514400601387,\n",
       "  0.01675376109778881,\n",
       "  0.011522484011948109,\n",
       "  -0.01411418803036213,\n",
       "  0.04130314663052559,\n",
       "  -0.0052791438065469265,\n",
       "  -3.953374471166171e-05,\n",
       "  -0.026792339980602264,\n",
       "  -0.011153217405080795,\n",
       "  0.018490681424736977,\n",
       "  0.02932249940931797,\n",
       "  -0.001575366361066699,\n",
       "  -0.016343463212251663,\n",
       "  -0.004154248628765345,\n",
       "  0.006492936983704567,\n",
       "  -0.0015317723155021667,\n",
       "  -0.004243146162480116,\n",
       "  0.015810079872608185,\n",
       "  -0.01319785974919796,\n",
       "  0.015194634906947613,\n",
       "  0.004981679376214743,\n",
       "  0.009047030471265316,\n",
       "  -0.014524484053254128,\n",
       "  0.000592365104239434,\n",
       "  0.026081159710884094,\n",
       "  0.0026942782569676638,\n",
       "  -0.0023711698595434427,\n",
       "  -0.016589641571044922,\n",
       "  -0.01604258082807064,\n",
       "  -0.0024532291572541,\n",
       "  -0.004314947873353958,\n",
       "  0.01530404668301344,\n",
       "  0.005282563157379627,\n",
       "  0.011481454595923424,\n",
       "  0.042670801281929016,\n",
       "  0.018107738345861435,\n",
       "  -0.008171731606125832,\n",
       "  0.006342494860291481,\n",
       "  -0.01522198785096407,\n",
       "  -0.03391781821846962,\n",
       "  0.02527424320578575,\n",
       "  -0.03192104026675224,\n",
       "  0.01533140055835247,\n",
       "  -0.01850435696542263,\n",
       "  0.013710730709135532,\n",
       "  0.015933167189359665,\n",
       "  0.026040129363536835,\n",
       "  -0.01996774598956108,\n",
       "  -0.015864785760641098,\n",
       "  -0.01775214821100235,\n",
       "  0.015673313289880753,\n",
       "  0.0100249033421278,\n",
       "  0.032604869455099106,\n",
       "  -0.014045805670320988,\n",
       "  0.0016676830127835274,\n",
       "  0.009245340712368488,\n",
       "  -0.011816529557108879,\n",
       "  -0.0025780275464057922,\n",
       "  -0.008930779993534088,\n",
       "  0.008787176571786404,\n",
       "  0.02289452590048313,\n",
       "  0.010517258197069168,\n",
       "  -0.01641184650361538,\n",
       "  -0.6836081147193909,\n",
       "  -0.02314070425927639,\n",
       "  0.022730408236384392,\n",
       "  -0.03722753748297691,\n",
       "  0.015044192783534527,\n",
       "  0.026423072442412376,\n",
       "  0.005665506236255169,\n",
       "  -0.016548611223697662,\n",
       "  -0.01578272506594658,\n",
       "  0.031346626579761505,\n",
       "  -0.010742921382188797,\n",
       "  -0.0011727631790563464,\n",
       "  0.004947488196194172,\n",
       "  -0.02790013886988163,\n",
       "  0.011077996343374252,\n",
       "  -0.015755372121930122,\n",
       "  0.012965358793735504,\n",
       "  -0.020145541056990623,\n",
       "  -0.0015027096960693598,\n",
       "  0.008725631982088089,\n",
       "  -0.02009083516895771,\n",
       "  0.019776275381445885,\n",
       "  -0.027626609429717064,\n",
       "  0.010777113027870655,\n",
       "  0.00514921685680747,\n",
       "  0.00962828379124403,\n",
       "  0.0021899559069424868,\n",
       "  -0.021868785843253136,\n",
       "  -0.017738470807671547,\n",
       "  0.02051480859518051,\n",
       "  -0.02614954300224781,\n",
       "  -0.002347236033529043,\n",
       "  0.010168506763875484,\n",
       "  -0.01620669849216938,\n",
       "  0.04319050908088684,\n",
       "  -0.013444038107991219,\n",
       "  -0.002431004773825407,\n",
       "  -0.01079762727022171,\n",
       "  0.009163280948996544,\n",
       "  0.02836514264345169,\n",
       "  -0.016302434727549553,\n",
       "  -0.006280950736254454,\n",
       "  0.012117413803935051,\n",
       "  -0.0035353852435946465,\n",
       "  -0.0018822336569428444,\n",
       "  0.02389291301369667,\n",
       "  0.02973279543220997,\n",
       "  0.01261660736054182,\n",
       "  -0.0018121413886547089,\n",
       "  -0.015851108357310295,\n",
       "  -0.004150829743593931,\n",
       "  -0.009272693656384945,\n",
       "  0.01470227912068367,\n",
       "  -0.019571127369999886,\n",
       "  -0.005624476820230484,\n",
       "  0.0039320047944784164,\n",
       "  0.04152197390794754,\n",
       "  -0.03771989420056343,\n",
       "  0.02275776118040085,\n",
       "  0.012001163326203823,\n",
       "  -0.00032161237322725356,\n",
       "  0.011289983056485653,\n",
       "  -0.013498743996024132,\n",
       "  -0.013293595984578133,\n",
       "  -0.008130702190101147,\n",
       "  -0.0032362111378461123,\n",
       "  -0.008876074105501175,\n",
       "  0.000947100343182683,\n",
       "  -0.0019112962763756514,\n",
       "  -0.008240114897489548,\n",
       "  0.015946844592690468,\n",
       "  0.016343463212251663,\n",
       "  -0.026587191969156265,\n",
       "  -0.02690175175666809,\n",
       "  0.012185796163976192,\n",
       "  0.023373205214738846,\n",
       "  0.006246759090572596,\n",
       "  -0.04089285060763359,\n",
       "  -0.027339400723576546,\n",
       "  0.03137398138642311,\n",
       "  0.022579966112971306,\n",
       "  -0.015249340794980526,\n",
       "  -0.05125966668128967,\n",
       "  -0.015345077030360699,\n",
       "  0.01179601438343525,\n",
       "  -0.008766661398112774,\n",
       "  -0.029377205297350883,\n",
       "  0.016193022951483727,\n",
       "  0.02322276309132576,\n",
       "  0.0037610481958836317,\n",
       "  0.011248953640460968,\n",
       "  -0.004533772822469473,\n",
       "  -0.01803935505449772,\n",
       "  -0.01812141388654709,\n",
       "  0.01163189671933651,\n",
       "  -0.011358365416526794,\n",
       "  -0.0036824080161750317,\n",
       "  0.007029741071164608,\n",
       "  0.016890525817871094,\n",
       "  0.012055869214236736,\n",
       "  -0.0032806596718728542,\n",
       "  0.0066536362282931805,\n",
       "  -0.0069750347174704075,\n",
       "  0.0006509177619591355,\n",
       "  0.02214231714606285,\n",
       "  -0.004663699772208929,\n",
       "  -0.010271080769598484,\n",
       "  -0.002711373846977949,\n",
       "  0.02677866257727146,\n",
       "  -0.021608931943774223,\n",
       "  -0.017642734572291374,\n",
       "  -0.016548611223697662,\n",
       "  0.002711373846977949,\n",
       "  -0.017601706087589264,\n",
       "  -0.02271673083305359,\n",
       "  -0.030225150287151337,\n",
       "  0.0097103426232934,\n",
       "  0.005285982508212328,\n",
       "  0.013642347417771816,\n",
       "  -0.005501387640833855,\n",
       "  0.012876461260020733,\n",
       "  0.015126252546906471,\n",
       "  0.00036221457412466407,\n",
       "  0.002460067393258214,\n",
       "  0.014374042861163616,\n",
       "  0.014469778165221214,\n",
       "  0.010824980214238167,\n",
       "  -0.04234256595373154,\n",
       "  -0.007795627228915691,\n",
       "  -0.021978197619318962,\n",
       "  -0.001031723921187222,\n",
       "  0.0014291983097791672,\n",
       "  0.03670782968401909,\n",
       "  -0.012575577944517136,\n",
       "  0.022415846586227417,\n",
       "  -0.0030208055395632982,\n",
       "  0.030881624668836594,\n",
       "  0.008356365375220776,\n",
       "  0.008411071263253689,\n",
       "  -0.01653493568301201,\n",
       "  -0.027011163532733917,\n",
       "  0.015345077030360699,\n",
       "  -0.017765823751688004,\n",
       "  -0.01850435696542263,\n",
       "  0.00012116559082642198,\n",
       "  -0.023564675822854042,\n",
       "  0.010667700320482254,\n",
       "  -0.01750596985220909,\n",
       "  0.005850139539688826,\n",
       "  0.006407458335161209,\n",
       "  -0.01591949164867401,\n",
       "  -0.020706279203295708,\n",
       "  -0.026286307722330093,\n",
       "  0.011816529557108879,\n",
       "  0.02893955633044243,\n",
       "  -0.007761436048895121,\n",
       "  0.014442425221204758,\n",
       "  -0.016398170962929726,\n",
       "  -0.00904019270092249,\n",
       "  -0.009306884370744228,\n",
       "  0.008848721161484718,\n",
       "  0.017177732661366463,\n",
       "  -0.015509195625782013,\n",
       "  0.0033695572055876255,\n",
       "  -0.007392169442027807,\n",
       "  -0.0023831368889659643,\n",
       "  -0.0027609514072537422,\n",
       "  0.03252280876040459,\n",
       "  0.0002835744817275554,\n",
       "  -0.017478616908192635,\n",
       "  -0.013703892007470131,\n",
       "  -0.004157667979598045,\n",
       "  -0.0007710148929618299,\n",
       "  0.002795142587274313,\n",
       "  0.011269467882812023,\n",
       "  0.006554481107741594,\n",
       "  -0.02309967391192913,\n",
       "  -0.007433198858052492,\n",
       "  0.014456101693212986,\n",
       "  -0.006783563178032637,\n",
       "  0.021157605573534966,\n",
       "  0.02409806102514267,\n",
       "  -0.0143056595697999,\n",
       "  0.015714343637228012,\n",
       "  0.00385336484760046,\n",
       "  0.01213109027594328,\n",
       "  -0.0028293340001255274,\n",
       "  0.026532486081123352,\n",
       "  -0.010332625359296799,\n",
       "  -0.019571127369999886,\n",
       "  0.017806854099035263,\n",
       "  -0.006038191728293896,\n",
       "  -0.0296507366001606,\n",
       "  0.044175222516059875,\n",
       "  0.0015873332740738988,\n",
       "  0.011522484011948109,\n",
       "  -0.014237277209758759,\n",
       "  0.01649390533566475,\n",
       "  0.0028566871769726276,\n",
       "  0.013580802828073502,\n",
       "  0.03241339698433876,\n",
       "  -0.02681969292461872,\n",
       "  0.027571901679039,\n",
       "  -0.009115412831306458,\n",
       "  -0.0031062839552760124,\n",
       "  -0.03235869109630585,\n",
       "  -0.011310497298836708,\n",
       "  -0.008137540891766548,\n",
       "  0.016439199447631836,\n",
       "  -0.0006000581197440624,\n",
       "  0.008752984926104546,\n",
       "  -0.03525811806321144,\n",
       "  -0.008199085481464863,\n",
       "  0.0028395913541316986,\n",
       "  -0.010394169948995113,\n",
       "  0.03728224337100983,\n",
       "  -0.020651573315262794,\n",
       "  0.020706279203295708,\n",
       "  -0.012931167148053646,\n",
       "  -0.016808466985821724,\n",
       "  -0.0031644091941416264,\n",
       "  -0.0036106063053011894,\n",
       "  0.012021677568554878,\n",
       "  -0.009737695567309856,\n",
       "  -0.031346626579761505,\n",
       "  0.016603318974375725,\n",
       "  0.006677570287138224,\n",
       "  0.031182508915662766,\n",
       "  0.007693053223192692,\n",
       "  -0.01813509128987789,\n",
       "  0.0011761822970584035,\n",
       "  0.0018446232425048947,\n",
       "  9.514811245026067e-05,\n",
       "  0.00839739479124546,\n",
       "  0.01742391102015972,\n",
       "  0.012137928046286106,\n",
       "  0.019407009705901146,\n",
       "  -0.016056256368756294,\n",
       "  0.04573434591293335,\n",
       "  -0.01812141388654709,\n",
       "  -0.016316110268235207,\n",
       "  0.020706279203295708,\n",
       "  0.04056461527943611,\n",
       "  -0.0290216151624918,\n",
       "  0.016603318974375725,\n",
       "  0.015933167189359665,\n",
       "  0.020788339897990227,\n",
       "  0.010154830291867256,\n",
       "  -0.01509889867156744,\n",
       "  0.02346894145011902,\n",
       "  -0.011125864461064339,\n",
       "  0.028036905452609062,\n",
       "  -0.015153605490922928,\n",
       "  0.00973085779696703,\n",
       "  0.027749696746468544,\n",
       "  -0.009518871083855629,\n",
       "  -0.0019454876892268658,\n",
       "  0.024467328563332558,\n",
       "  0.013868010602891445,\n",
       "  0.022566288709640503,\n",
       "  0.0151399290189147,\n",
       "  -0.0070502557791769505,\n",
       "  0.011700279079377651,\n",
       "  -0.007289595436304808,\n",
       "  0.01675376109778881,\n",
       "  0.0030413202475756407,\n",
       "  -0.001454841811209917,\n",
       "  -0.01996774598956108,\n",
       "  -0.0003453326062299311,\n",
       "  0.0077272444032132626,\n",
       "  -0.014688602648675442,\n",
       "  -0.020118188112974167,\n",
       "  0.009477841667830944,\n",
       "  0.012254178524017334,\n",
       "  0.02171834371984005,\n",
       "  0.008240114897489548,\n",
       "  -0.010277919471263885,\n",
       "  0.0002857114595826715,\n",
       "  0.017259791493415833,\n",
       "  0.02727101929485798,\n",
       "  -0.011030128225684166,\n",
       "  -0.020692603662610054,\n",
       "  0.02995162084698677,\n",
       "  0.017478616908192635,\n",
       "  -0.007788788992911577,\n",
       "  -0.008889750577509403,\n",
       "  -0.03763783350586891,\n",
       "  0.017984649166464806,\n",
       "  -0.007672538515180349,\n",
       "  -0.01173447072505951,\n",
       "  0.008958132937550545,\n",
       "  0.008445262908935547,\n",
       "  0.013341464102268219,\n",
       "  0.0024036518298089504,\n",
       "  0.016151992604136467,\n",
       "  0.015522872097790241,\n",
       "  0.016808466985821724,\n",
       "  -0.002654958050698042,\n",
       "  0.0037576290778815746,\n",
       "  -0.0009376977104693651,\n",
       "  -0.0036174445413053036,\n",
       "  0.011878074146807194,\n",
       "  -0.010872848331928253,\n",
       "  -0.00881452951580286,\n",
       "  0.03282369300723076,\n",
       "  -0.017027290537953377,\n",
       "  0.018381267786026,\n",
       "  0.008992324583232403,\n",
       "  0.022251728922128677,\n",
       "  -0.015522872097790241,\n",
       "  0.0012770468601956964,\n",
       "  0.00011945601727347821,\n",
       "  -0.009395781904459,\n",
       "  -0.014839044772088528,\n",
       "  0.019489068537950516,\n",
       "  -0.0035559001844376326,\n",
       "  -0.017191410064697266,\n",
       "  0.008910264819860458,\n",
       "  0.02234746515750885,\n",
       "  -3.0531806260114536e-05,\n",
       "  0.01632978767156601,\n",
       "  -0.032303985208272934,\n",
       "  -0.003514870535582304,\n",
       "  0.02121231146156788,\n",
       "  0.022785114124417305,\n",
       "  0.027886463329195976,\n",
       "  -0.0057509844191372395,\n",
       "  -0.0035319661255925894,\n",
       "  0.012890137732028961,\n",
       "  0.0030584160704165697,\n",
       "  -0.006407458335161209,\n",
       "  -0.025205861777067184,\n",
       "  0.004109799861907959,\n",
       "  -0.010291595943272114,\n",
       "  -0.007364816032350063,\n",
       "  -0.008431586436927319,\n",
       "  0.026573514565825462,\n",
       "  0.006886137183755636,\n",
       "  0.015044192783534527,\n",
       "  -0.02130804769694805,\n",
       "  0.003183214459568262,\n",
       "  0.01100961398333311,\n",
       "  -0.00340374861843884,\n",
       "  -0.024043355137109756,\n",
       "  -0.0047525973059237,\n",
       "  0.012459326535463333,\n",
       "  0.008144378662109375,\n",
       "  -0.0021882462315261364,\n",
       "  0.03060809336602688,\n",
       "  0.0006192908040247858,\n",
       "  0.011789176613092422,\n",
       "  0.023113351315259933,\n",
       "  -0.006523709278553724,\n",
       "  -0.03525811806321144,\n",
       "  -0.0034875173587352037,\n",
       "  0.005443262401968241,\n",
       "  0.0048825242556631565,\n",
       "  0.006626283284276724,\n",
       "  -0.016945231705904007,\n",
       "  0.007857171818614006,\n",
       "  0.006174957379698753,\n",
       "  0.004903039429336786,\n",
       "  0.017478616908192635,\n",
       "  0.012144766747951508,\n",
       "  0.0047525973059237,\n",
       "  0.016110962256789207,\n",
       "  0.007200697902590036,\n",
       "  -0.003846526611596346,\n",
       "  0.0030532872769981623,\n",
       "  -0.013088447973132133,\n",
       "  -0.009060706943273544,\n",
       "  0.009088059887290001,\n",
       "  0.011727632023394108,\n",
       "  -0.018162444233894348,\n",
       "  0.020911427214741707,\n",
       "  -0.023400558158755302,\n",
       "  0.0033336563501507044,\n",
       "  -0.016931554302573204,\n",
       "  0.0048825242556631565,\n",
       "  0.01661699451506138,\n",
       "  -0.010483067482709885,\n",
       "  0.005508225876837969,\n",
       "  -0.019407009705901146,\n",
       "  -0.006906652357429266,\n",
       "  -0.009682989679276943,\n",
       "  -0.018559062853455544,\n",
       "  0.01323889009654522,\n",
       "  -0.00990181416273117,\n",
       "  -0.01202851627022028,\n",
       "  9.044679609360173e-05,\n",
       "  -4.3707495933631435e-06,\n",
       "  0.010989098809659481,\n",
       "  -0.023537322878837585,\n",
       "  0.002906264504417777,\n",
       "  -0.013067932799458504,\n",
       "  -0.006910071242600679,\n",
       "  -0.017259791493415833,\n",
       "  0.00870511680841446,\n",
       "  0.0063048843294382095,\n",
       "  0.012849108316004276,\n",
       "  -0.0009616316528990865,\n",
       "  -0.03052603453397751,\n",
       "  -0.006804078351706266,\n",
       "  -0.003371266881003976,\n",
       "  -0.012746534310281277,\n",
       "  -0.02572556957602501,\n",
       "  0.00395935820415616,\n",
       "  -0.04100226238369942,\n",
       "  -0.006174957379698753,\n",
       "  0.003133636899292469,\n",
       "  0.004612412769347429,\n",
       "  0.005056900437921286,\n",
       "  0.005525321699678898,\n",
       "  0.005764661356806755,\n",
       "  0.015003163367509842,\n",
       "  -0.014934780076146126,\n",
       "  0.002680601552128792,\n",
       "  -0.004444875288754702,\n",
       "  0.009224825538694859,\n",
       "  0.0053885565139353275,\n",
       "  -0.006233082618564367,\n",
       "  -0.004738920833915472,\n",
       "  0.011584028601646423,\n",
       "  -0.030936330556869507,\n",
       "  -0.019475391134619713,\n",
       "  -0.004079027567058802,\n",
       "  0.010052256286144257,\n",
       "  -0.012001163326203823,\n",
       "  0.02100716345012188,\n",
       "  -0.002164312405511737,\n",
       "  -0.013772274367511272,\n",
       "  0.03730959817767143,\n",
       "  0.0013838948216289282,\n",
       "  0.0075289346277713776,\n",
       "  0.005658668000251055,\n",
       "  0.01230204664170742,\n",
       "  0.014100511558353901,\n",
       "  0.002820786088705063,\n",
       "  -0.00942313577979803,\n",
       "  -0.005815948359668255,\n",
       "  -0.003600348951295018,\n",
       "  0.00904019270092249,\n",
       "  0.0010889944387599826,\n",
       "  -0.02501438930630684,\n",
       "  0.021362753584980965,\n",
       "  -0.030772212892770767,\n",
       "  0.026587191969156265,\n",
       "  0.028419848531484604,\n",
       "  0.00725540379062295,\n",
       "  0.030143091455101967,\n",
       "  0.001103525748476386,\n",
       "  -0.017615381628274918,\n",
       "  0.0019933555740863085,\n",
       "  -0.0016112672165036201,\n",
       "  -0.008452100679278374,\n",
       "  0.03698136284947395,\n",
       "  -0.01754700019955635,\n",
       "  -0.01152932271361351,\n",
       "  -0.04075608775019646,\n",
       "  -0.010455713607370853,\n",
       "  0.004574802238494158,\n",
       "  0.016275081783533096,\n",
       "  -0.004349139519035816,\n",
       "  -0.01299271173775196,\n",
       "  -0.015386106446385384,\n",
       "  0.0008522193529643118,\n",
       "  0.004441455937922001,\n",
       "  -0.003658474190160632,\n",
       "  -0.015509195625782013,\n",
       "  -0.022169670090079308,\n",
       "  -0.007925554178655148,\n",
       "  0.0017523065907880664,\n",
       "  -0.0026139284018427134,\n",
       "  0.024111738428473473,\n",
       "  -0.016972584649920464,\n",
       "  -0.012151604518294334,\n",
       "  -0.010749760083854198,\n",
       "  -0.025752922520041466,\n",
       "  -0.002405361272394657,\n",
       "  -0.030854271724820137,\n",
       "  -0.0038020778447389603,\n",
       "  -0.007132315076887608,\n",
       "  0.016808466985821724,\n",
       "  0.010065932758152485,\n",
       "  0.026135865598917007,\n",
       "  -0.009949682280421257,\n",
       "  0.0029951620381325483,\n",
       "  0.02051480859518051,\n",
       "  -0.0024019421543926,\n",
       "  0.01140623353421688,\n",
       "  0.008930779993534088,\n",
       "  -0.009929167106747627,\n",
       "  -0.02744881436228752,\n",
       "  0.014073158614337444,\n",
       "  0.009717181324958801,\n",
       "  0.015755372121930122,\n",
       "  0.006267273798584938,\n",
       "  -0.01966686360538006,\n",
       "  0.00731011014431715,\n",
       "  0.01411418803036213,\n",
       "  0.023865560069680214,\n",
       "  0.006287788972258568,\n",
       "  -0.015030516311526299,\n",
       "  -0.0011360074859112501,\n",
       "  -0.006404039449989796,\n",
       "  0.013840657658874989,\n",
       "  -0.024494681507349014,\n",
       "  0.00250622583553195,\n",
       "  -0.002964389743283391,\n",
       "  0.004807303659617901,\n",
       "  0.051943495869636536,\n",
       "  0.010619832202792168,\n",
       "  0.011201085522770882,\n",
       "  0.013040579855442047,\n",
       "  0.025616157799959183,\n",
       "  -0.005792014300823212,\n",
       "  0.00401064520701766,\n",
       "  -0.004923554137349129,\n",
       "  -0.00791187770664692,\n",
       "  -0.003494355594739318,\n",
       "  -0.009354752488434315,\n",
       "  -0.03553164750337601,\n",
       "  0.004786788485944271,\n",
       "  0.023633059114217758,\n",
       "  0.03153809905052185,\n",
       "  0.014291983097791672,\n",
       "  -0.0027199217583984137,\n",
       "  -0.019680539146065712,\n",
       "  0.004971421789377928,\n",
       "  0.0029490035958588123,\n",
       "  -0.01173447072505951,\n",
       "  -0.010619832202792168,\n",
       "  -0.010469391010701656,\n",
       "  -0.009806078858673573,\n",
       "  -0.037583127617836,\n",
       "  -0.008835043758153915,\n",
       "  -0.01061299443244934,\n",
       "  -0.024152766913175583,\n",
       "  -0.006800659000873566,\n",
       "  0.01109167281538248,\n",
       "  -0.010592479258775711,\n",
       "  0.008958132937550545,\n",
       "  -0.008062319830060005,\n",
       "  -0.030799565836787224,\n",
       "  -0.016575966030359268,\n",
       "  0.010872848331928253,\n",
       "  0.049974072724580765,\n",
       "  0.013170506805181503,\n",
       "  0.006889556534588337,\n",
       "  0.014524484053254128,\n",
       "  -0.013177345506846905,\n",
       "  -0.022648349404335022,\n",
       "  0.009272693656384945,\n",
       "  -0.008117025718092918,\n",
       "  -0.004417521879076958,\n",
       "  0.028666025027632713,\n",
       "  0.03274163603782654,\n",
       "  0.008486292324960232,\n",
       "  0.01604258082807064,\n",
       "  0.005542417522519827,\n",
       "  0.008547836914658546,\n",
       "  -0.015550225041806698,\n",
       "  -0.031592804938554764,\n",
       "  0.016110962256789207,\n",
       "  -0.0023352690041065216,\n",
       "  -0.0060587069019675255,\n",
       "  -0.020665250718593597,\n",
       "  -0.012938005849719048,\n",
       "  -0.036489006131887436,\n",
       "  0.02242952398955822,\n",
       "  0.0006081786123104393,\n",
       "  0.0006466438644565642,\n",
       "  -0.01892833039164543,\n",
       "  -0.00990181416273117,\n",
       "  -0.03796607255935669,\n",
       "  0.016931554302573204,\n",
       "  -0.020993487909436226,\n",
       "  0.017820529639720917,\n",
       "  0.02043274976313114,\n",
       "  -0.014100511558353901,\n",
       "  -0.004075608681887388,\n",
       "  -0.0006940843304619193,\n",
       "  -0.014592867344617844,\n",
       "  0.0072827572003006935,\n",
       "  0.0021061869338154793,\n",
       "  0.018449651077389717,\n",
       "  -0.013396169990301132,\n",
       "  -0.0006145894876681268,\n",
       "  0.016521258279681206,\n",
       "  0.025780275464057922,\n",
       "  -0.01553654856979847,\n",
       "  0.014962133951485157,\n",
       "  0.006311722565442324,\n",
       "  0.02180040255188942,\n",
       "  -0.034902527928352356,\n",
       "  -0.009936005808413029,\n",
       "  -0.0012308885343372822,\n",
       "  0.0038807180244475603,\n",
       "  0.00923166424036026,\n",
       "  0.01079762727022171,\n",
       "  -0.012794402427971363,\n",
       "  -0.027681315317749977,\n",
       "  -0.02151319570839405,\n",
       "  -0.007357977796345949,\n",
       "  -0.0006782708805985749,\n",
       "  -0.005939037073403597,\n",
       "  -0.020077159628272057,\n",
       "  -0.005169731564819813,\n",
       "  -0.010866010561585426,\n",
       "  -0.030143091455101967,\n",
       "  -0.002314754296094179,\n",
       "  -0.007905039936304092,\n",
       "  0.003593510715290904,\n",
       "  -0.025246890261769295,\n",
       "  -0.013991099782288074,\n",
       "  -0.0052791438065469265,\n",
       "  0.002268595853820443,\n",
       "  0.024207474663853645,\n",
       "  0.013833818957209587,\n",
       "  -0.004537191707640886,\n",
       "  0.012684989720582962,\n",
       "  0.06094265729188919,\n",
       "  -0.011638734489679337,\n",
       "  0.016015227884054184,\n",
       "  -0.007084446959197521,\n",
       "  7.495384488720447e-05,\n",
       "  -0.030553387477993965,\n",
       "  -0.006342494860291481,\n",
       "  -0.004335463047027588,\n",
       "  -0.023168057203292847,\n",
       "  0.013847495429217815,\n",
       "  -0.01808038540184498,\n",
       "  -0.0048825242556631565,\n",
       "  0.013108962215483189,\n",
       "  -0.007542611099779606,\n",
       "  0.015249340794980526,\n",
       "  -0.0014770661946386099,\n",
       "  -0.015509195625782013,\n",
       "  -0.002661796286702156,\n",
       "  0.0055526746436953545,\n",
       "  0.009395781904459,\n",
       "  0.01219947263598442,\n",
       "  -0.03312457725405693,\n",
       "  0.019899364560842514,\n",
       "  0.005538998171687126,\n",
       "  0.00802129041403532,\n",
       "  0.011522484011948109,\n",
       "  -0.027011163532733917,\n",
       "  0.010387331247329712,\n",
       "  -0.005538998171687126,\n",
       "  0.005915103014558554,\n",
       "  0.004557706415653229,\n",
       "  -0.0242211502045393,\n",
       "  -0.011242114938795567,\n",
       "  0.00027353077894076705,\n",
       "  0.0001494803000241518,\n",
       "  0.002870363648980856,\n",
       "  -0.006451907102018595,\n",
       "  -0.010585641488432884,\n",
       "  0.007474228739738464,\n",
       "  -0.013519259169697762,\n",
       "  0.017847882583737373,\n",
       "  0.009006001055240631,\n",
       "  -0.004612412769347429,\n",
       "  -0.004879105370491743,\n",
       "  -0.0003823020088020712,\n",
       "  -0.02222437597811222,\n",
       "  -0.014606543816626072,\n",
       "  -0.006205729674547911,\n",
       "  -0.033097222447395325,\n",
       "  -0.04018167033791542,\n",
       "  5.275617877487093e-05,\n",
       "  -0.019475391134619713,\n",
       "  -0.01663067191839218,\n",
       "  -0.012336238287389278,\n",
       "  0.02242952398955822,\n",
       "  0.004226050339639187,\n",
       "  -0.005573189817368984,\n",
       "  -0.03115515597164631,\n",
       "  -0.011501968838274479,\n",
       "  -0.06400620192289352,\n",
       "  -0.002642991254106164,\n",
       "  -0.022744083777070045,\n",
       "  0.008356365375220776,\n",
       "  0.03867725282907486,\n",
       "  0.018996711820364,\n",
       "  0.004414102993905544,\n",
       "  0.010859171859920025,\n",
       "  -0.0016505873063579202,\n",
       "  0.027093224227428436,\n",
       "  -0.0200498066842556,\n",
       "  -0.0036174445413053036,\n",
       "  0.0039661964401602745,\n",
       "  -0.02167731337249279,\n",
       "  -0.005884330719709396,\n",
       "  0.01616566814482212,\n",
       "  -0.02108922228217125,\n",
       "  -0.020446425303816795,\n",
       "  0.02798219956457615,\n",
       "  0.00852048397064209,\n",
       "  -0.010831818915903568,\n",
       "  0.05284614488482475,\n",
       "  0.006605768110603094,\n",
       "  -0.01724611595273018,\n",
       "  -0.0009975325083360076,\n",
       "  0.012473003938794136,\n",
       "  -0.007665700279176235,\n",
       "  0.013293595984578133,\n",
       "  0.02590336464345455,\n",
       "  -0.001858299714513123,\n",
       "  -0.01608360931277275,\n",
       "  0.012295208871364594,\n",
       "  0.00979240145534277,\n",
       "  -0.001399280852638185,\n",
       "  0.0005739872576668859,\n",
       "  0.0036721506621688604,\n",
       "  0.021526873111724854,\n",
       "  0.007843495346605778,\n",
       "  -0.019872011616826057,\n",
       "  -0.0037234376650303602,\n",
       "  0.005689440295100212,\n",
       "  -0.02513747848570347,\n",
       "  0.02836514264345169,\n",
       "  0.008294820785522461,\n",
       "  -0.029349852353334427,\n",
       "  0.01608360931277275,\n",
       "  0.008062319830060005,\n",
       "  -0.001584768993780017,\n",
       "  0.0018993293633684516,\n",
       "  0.016138315200805664,\n",
       "  -0.00800077524036169,\n",
       "  0.001818979624658823,\n",
       "  0.010544611141085625,\n",
       "  -0.008055481128394604,\n",
       "  -0.02515115588903427,\n",
       "  -0.0034345209132879972,\n",
       "  0.0014112478820607066,\n",
       "  -0.01909244805574417,\n",
       "  0.00780930370092392,\n",
       "  -0.015208311378955841,\n",
       "  -0.011495131067931652,\n",
       "  -0.011180570349097252,\n",
       "  0.018600093200802803,\n",
       "  0.011173732578754425,\n",
       "  -0.021814079955220222,\n",
       "  -0.015290370211005211,\n",
       "  -0.0067938207648694515,\n",
       "  -0.0026600868441164494,\n",
       "  0.009977035224437714,\n",
       "  -0.00904019270092249,\n",
       "  -0.006698084995150566,\n",
       "  0.006270693149417639,\n",
       "  0.008158055134117603,\n",
       "  0.01821715012192726,\n",
       "  -0.0050637386739254,\n",
       "  0.003566157538443804,\n",
       "  0.008684602566063404,\n",
       "  -0.004475647583603859,\n",
       "  -0.006376686505973339,\n",
       "  0.016890525817871094,\n",
       "  0.20208454132080078,\n",
       "  -0.006923747714608908,\n",
       "  0.01470227912068367,\n",
       "  0.03156545013189316,\n",
       "  -0.0025917040184140205,\n",
       "  0.00646216468885541,\n",
       "  0.003969615325331688,\n",
       "  -0.009347914718091488,\n",
       "  0.003843107493594289,\n",
       "  0.006957939360290766,\n",
       "  -0.01937965489923954,\n",
       "  0.018641121685504913,\n",
       "  -0.009238502010703087,\n",
       "  -0.009840269573032856,\n",
       "  0.02893955633044243,\n",
       "  -0.010948069393634796,\n",
       "  -0.01734185218811035,\n",
       "  -0.030772212892770767,\n",
       "  -0.027257341891527176,\n",
       "  -0.026764987036585808,\n",
       "  -0.012726019136607647,\n",
       "  0.009149604476988316,\n",
       "  -0.005867235362529755,\n",
       "  -0.021267017349600792,\n",
       "  0.014715955592691898,\n",
       "  -0.0023814274463802576,\n",
       "  0.0009197472245432436,\n",
       "  0.0063322377391159534,\n",
       "  0.022415846586227417,\n",
       "  0.023496294394135475,\n",
       "  -0.02460409328341484,\n",
       "  -0.020118188112974167,\n",
       "  0.015454488806426525,\n",
       "  0.012206311337649822,\n",
       "  -0.030252505093812943,\n",
       "  0.007836656644940376,\n",
       "  0.037610482424497604,\n",
       "  -0.002157473936676979,\n",
       "  0.029377205297350883,\n",
       "  0.006342494860291481,\n",
       "  0.04113902896642685,\n",
       "  -0.0031644091941416264,\n",
       "  -0.021622607484459877,\n",
       "  -0.01522198785096407,\n",
       "  -0.004588478710502386,\n",
       "  0.03162015974521637,\n",
       "  ...],\n",
       " [-0.003931369166821241,\n",
       "  -0.006192067172378302,\n",
       "  0.009164644405245781,\n",
       "  -0.03178444877266884,\n",
       "  -0.009857282973825932,\n",
       "  0.03283623233437538,\n",
       "  -0.026371600106358528,\n",
       "  -0.018431901931762695,\n",
       "  -0.01072949543595314,\n",
       "  -0.021189631894230843,\n",
       "  0.0027689537964761257,\n",
       "  0.016533557325601578,\n",
       "  -0.005454534664750099,\n",
       "  -0.004149422515183687,\n",
       "  -0.01703379675745964,\n",
       "  -0.0030463302973657846,\n",
       "  0.027038585394620895,\n",
       "  0.009972723200917244,\n",
       "  0.018367767333984375,\n",
       "  -0.017085103318095207,\n",
       "  -0.016058970242738724,\n",
       "  0.000617282697930932,\n",
       "  -0.02760295942425728,\n",
       "  -0.023511257022619247,\n",
       "  0.004377095494419336,\n",
       "  0.002170911058783531,\n",
       "  0.012480334378778934,\n",
       "  -0.017264675348997116,\n",
       "  -0.0006160801858641207,\n",
       "  0.004017949104309082,\n",
       "  0.01068460289388895,\n",
       "  -0.0017508382443338633,\n",
       "  -0.002841103821992874,\n",
       "  -0.005412847734987736,\n",
       "  -0.015969183295965195,\n",
       "  -0.008080791682004929,\n",
       "  -0.01835494115948677,\n",
       "  0.004139802418649197,\n",
       "  0.002514024032279849,\n",
       "  -0.01426323875784874,\n",
       "  0.014314545318484306,\n",
       "  0.00808720476925373,\n",
       "  0.0032868299167603254,\n",
       "  -0.000613675219938159,\n",
       "  -0.02361387014389038,\n",
       "  0.002073107985779643,\n",
       "  0.011762041598558426,\n",
       "  -0.020214807242155075,\n",
       "  -0.01811123453080654,\n",
       "  0.022805791348218918,\n",
       "  0.007510005962103605,\n",
       "  -0.00456628855317831,\n",
       "  -0.029193464666604996,\n",
       "  -0.021253764629364014,\n",
       "  0.007195752579718828,\n",
       "  -0.00048059868277050555,\n",
       "  -0.004643248859792948,\n",
       "  0.009684124030172825,\n",
       "  0.011120708659291267,\n",
       "  -0.0014534202637150884,\n",
       "  -0.006429360248148441,\n",
       "  0.010075336322188377,\n",
       "  -0.002026611240580678,\n",
       "  0.012685560621321201,\n",
       "  -0.026576826348900795,\n",
       "  -0.00808720476925373,\n",
       "  0.0007307184278033674,\n",
       "  -0.009972723200917244,\n",
       "  -0.006631379947066307,\n",
       "  0.004877334926277399,\n",
       "  0.040532227605581284,\n",
       "  0.04001915827393532,\n",
       "  -0.011133535765111446,\n",
       "  0.00046376368845812976,\n",
       "  0.0184447281062603,\n",
       "  -0.014199106022715569,\n",
       "  -0.008645164780318737,\n",
       "  0.0033156899735331535,\n",
       "  -0.005887433886528015,\n",
       "  -0.005858574062585831,\n",
       "  0.02502480149269104,\n",
       "  -0.03563244268298149,\n",
       "  -0.010511443018913269,\n",
       "  0.019932620227336884,\n",
       "  0.00867081806063652,\n",
       "  0.005092181731015444,\n",
       "  -0.011665841564536095,\n",
       "  0.01866278052330017,\n",
       "  0.004463675431907177,\n",
       "  0.003957022912800312,\n",
       "  0.01380147971212864,\n",
       "  0.015135451219975948,\n",
       "  -0.0033477565739303827,\n",
       "  0.015109797939658165,\n",
       "  -0.003373409854248166,\n",
       "  0.015879398211836815,\n",
       "  -0.0033894432708621025,\n",
       "  0.038043856620788574,\n",
       "  -0.011819761246442795,\n",
       "  -0.01952216774225235,\n",
       "  0.00024991933605633676,\n",
       "  0.022882750257849693,\n",
       "  -0.020715046674013138,\n",
       "  -0.020304592326283455,\n",
       "  -0.022523604333400726,\n",
       "  -0.013031880371272564,\n",
       "  0.0019737014081329107,\n",
       "  0.002008974552154541,\n",
       "  0.0064934934489429,\n",
       "  -0.0045438422821462154,\n",
       "  -0.005579594522714615,\n",
       "  -0.0007555700722150505,\n",
       "  0.02056112512946129,\n",
       "  -0.037248603999614716,\n",
       "  0.00705465953797102,\n",
       "  -0.020060885697603226,\n",
       "  0.012615013867616653,\n",
       "  0.0058906408958137035,\n",
       "  0.006567246746271849,\n",
       "  -0.03783863037824631,\n",
       "  0.009183883666992188,\n",
       "  0.009318564087152481,\n",
       "  0.009228777140378952,\n",
       "  0.014878918416798115,\n",
       "  0.006868673022836447,\n",
       "  0.005672587547451258,\n",
       "  1.1148165413032984e-06,\n",
       "  0.00994706992059946,\n",
       "  -0.0029934202320873737,\n",
       "  -0.010011203587055206,\n",
       "  0.030168289318680763,\n",
       "  0.00240499759092927,\n",
       "  0.01386561244726181,\n",
       "  0.018765393644571304,\n",
       "  -0.027064239606261253,\n",
       "  0.022882750257849693,\n",
       "  -0.0020923479460179806,\n",
       "  0.0010758355492725968,\n",
       "  -0.006310713477432728,\n",
       "  -0.004813201725482941,\n",
       "  -0.0015969184460118413,\n",
       "  0.037864282727241516,\n",
       "  -0.012627840973436832,\n",
       "  -0.004540635272860527,\n",
       "  0.004053222481161356,\n",
       "  0.03114311583340168,\n",
       "  0.026910319924354553,\n",
       "  0.005393608007580042,\n",
       "  0.007958938367664814,\n",
       "  -0.006551213562488556,\n",
       "  0.016956835985183716,\n",
       "  -0.017136409878730774,\n",
       "  0.00848483107984066,\n",
       "  -0.027295120060443878,\n",
       "  -0.028269944712519646,\n",
       "  0.0069584595039486885,\n",
       "  -0.006204893812537193,\n",
       "  -0.013493639416992664,\n",
       "  -0.018855180591344833,\n",
       "  0.004640041850507259,\n",
       "  -0.015840917825698853,\n",
       "  -0.0028892038390040398,\n",
       "  0.011229735799133778,\n",
       "  0.003822342725470662,\n",
       "  0.0030944303143769503,\n",
       "  0.029347384348511696,\n",
       "  0.03186140954494476,\n",
       "  -0.016033317893743515,\n",
       "  0.0014454035554081202,\n",
       "  -0.02038155309855938,\n",
       "  0.012794586829841137,\n",
       "  0.012454681098461151,\n",
       "  -0.022703176364302635,\n",
       "  0.01595635712146759,\n",
       "  0.0038383761420845985,\n",
       "  0.00674040662124753,\n",
       "  -0.005214034579694271,\n",
       "  0.009401937015354633,\n",
       "  -0.006836606655269861,\n",
       "  -0.023973016068339348,\n",
       "  0.019021928310394287,\n",
       "  0.0027336806524544954,\n",
       "  0.01696966215968132,\n",
       "  0.018803874030709267,\n",
       "  -0.023126456886529922,\n",
       "  -0.0056116608902812,\n",
       "  0.019496513530611992,\n",
       "  -0.007914045825600624,\n",
       "  0.0005383186507970095,\n",
       "  -0.006631379947066307,\n",
       "  -0.006564040202647448,\n",
       "  0.02059960551559925,\n",
       "  -0.00011303489009151235,\n",
       "  0.005194794852286577,\n",
       "  -0.7043372392654419,\n",
       "  -0.02573026716709137,\n",
       "  0.024178242310881615,\n",
       "  -0.04509851336479187,\n",
       "  0.02806471846997738,\n",
       "  0.007356085814535618,\n",
       "  -0.000589224393479526,\n",
       "  -0.005268548149615526,\n",
       "  -0.018329288810491562,\n",
       "  0.01589222438633442,\n",
       "  -0.021972058340907097,\n",
       "  0.007843499071896076,\n",
       "  0.01632833108305931,\n",
       "  -0.021689871326088905,\n",
       "  0.0037261429242789745,\n",
       "  -0.029244771227240562,\n",
       "  -0.004967121873050928,\n",
       "  -0.021972058340907097,\n",
       "  -0.01316014677286148,\n",
       "  0.0018967414507642388,\n",
       "  -0.027012933045625687,\n",
       "  0.016764435917139053,\n",
       "  -0.009273670613765717,\n",
       "  -0.015443291515111923,\n",
       "  0.003854409558698535,\n",
       "  -0.012871546670794487,\n",
       "  0.015430464409291744,\n",
       "  -0.0027160439640283585,\n",
       "  -0.01881670020520687,\n",
       "  0.003124893642961979,\n",
       "  -0.013019053265452385,\n",
       "  0.017020968720316887,\n",
       "  -0.027474692091345787,\n",
       "  -0.01549459807574749,\n",
       "  0.036966416984796524,\n",
       "  -0.014083665795624256,\n",
       "  -0.0007351275999099016,\n",
       "  0.014814784750342369,\n",
       "  0.031194422394037247,\n",
       "  0.04607333987951279,\n",
       "  -0.012198147363960743,\n",
       "  -0.010870588943362236,\n",
       "  0.017264675348997116,\n",
       "  -0.0015119417803362012,\n",
       "  -0.01811123453080654,\n",
       "  0.005339094437658787,\n",
       "  0.03452935069799423,\n",
       "  0.023806270211935043,\n",
       "  0.005191588308662176,\n",
       "  -0.01835494115948677,\n",
       "  0.01010098960250616,\n",
       "  0.008035898208618164,\n",
       "  0.015584384091198444,\n",
       "  -0.002377741038799286,\n",
       "  -0.011928788386285305,\n",
       "  -0.010601229034364223,\n",
       "  -0.0017107549356296659,\n",
       "  -0.004675315227359533,\n",
       "  -0.002433857647702098,\n",
       "  0.018919315189123154,\n",
       "  0.011678668670356274,\n",
       "  0.005820094142109156,\n",
       "  -0.017110755667090416,\n",
       "  -0.001150390482507646,\n",
       "  -0.0006673868047073483,\n",
       "  0.025165895000100136,\n",
       "  -0.016533557325601578,\n",
       "  0.010588402859866619,\n",
       "  0.02176683209836483,\n",
       "  -0.024049974977970123,\n",
       "  -0.004713795147836208,\n",
       "  0.008664404973387718,\n",
       "  -0.02579439990222454,\n",
       "  8.637949940748513e-05,\n",
       "  0.019599126651883125,\n",
       "  0.01881670020520687,\n",
       "  0.022754482924938202,\n",
       "  0.0071059660986065865,\n",
       "  -0.018008621409535408,\n",
       "  0.023011017590761185,\n",
       "  0.0067339930683374405,\n",
       "  -0.004120562691241503,\n",
       "  -0.0094532435759902,\n",
       "  -0.011133535765111446,\n",
       "  0.02588418684899807,\n",
       "  -0.022792963311076164,\n",
       "  -0.016867049038410187,\n",
       "  -0.0025060074403882027,\n",
       "  -0.00019420355965849012,\n",
       "  0.0008882457623258233,\n",
       "  0.024575868621468544,\n",
       "  0.030553089454770088,\n",
       "  -0.005207621492445469,\n",
       "  -0.014673692174255848,\n",
       "  0.021382031962275505,\n",
       "  -0.009344217367470264,\n",
       "  -0.006849433295428753,\n",
       "  0.0004633628705050796,\n",
       "  0.014724998734891415,\n",
       "  -0.002882790518924594,\n",
       "  -0.005621280986815691,\n",
       "  -0.00046897452557459474,\n",
       "  0.006939219776540995,\n",
       "  0.006185653619468212,\n",
       "  0.013647560030221939,\n",
       "  -0.02392170950770378,\n",
       "  -0.01494305208325386,\n",
       "  -0.006317127030342817,\n",
       "  0.03704337775707245,\n",
       "  -0.018778221681714058,\n",
       "  -0.007638272363692522,\n",
       "  0.01308318693190813,\n",
       "  -0.00633316021412611,\n",
       "  -0.005047288257628679,\n",
       "  -0.001595315057784319,\n",
       "  -0.030270902439951897,\n",
       "  0.007849912159144878,\n",
       "  -0.00979315023869276,\n",
       "  0.014353025704622269,\n",
       "  -0.026230506598949432,\n",
       "  0.022485123947262764,\n",
       "  0.007830671966075897,\n",
       "  0.009543030522763729,\n",
       "  -0.010370349511504173,\n",
       "  0.0024979908484965563,\n",
       "  0.02834690548479557,\n",
       "  0.0027304738759994507,\n",
       "  -0.02665378712117672,\n",
       "  -0.007881978526711464,\n",
       "  -0.027962105348706245,\n",
       "  0.00601890729740262,\n",
       "  -0.013198627158999443,\n",
       "  0.024986321106553078,\n",
       "  -0.006451806984841824,\n",
       "  0.007484352681785822,\n",
       "  -0.0007182926055975258,\n",
       "  0.029655223712325096,\n",
       "  0.002977387048304081,\n",
       "  0.01983000710606575,\n",
       "  -0.020150672644376755,\n",
       "  -0.006592900026589632,\n",
       "  0.011851828545331955,\n",
       "  0.0023969809990376234,\n",
       "  -0.010306216776371002,\n",
       "  0.016713129356503487,\n",
       "  0.0016931182472035289,\n",
       "  -0.009895763359963894,\n",
       "  0.0037710361648350954,\n",
       "  -0.0019063614308834076,\n",
       "  0.019778700545430183,\n",
       "  -0.016764435917139053,\n",
       "  0.003004643600434065,\n",
       "  -0.011556815356016159,\n",
       "  0.004951088223606348,\n",
       "  0.002791400533169508,\n",
       "  0.013275586999952793,\n",
       "  0.014083665795624256,\n",
       "  -0.001578480121679604,\n",
       "  -0.022177284583449364,\n",
       "  -0.003937782719731331,\n",
       "  0.005560354329645634,\n",
       "  0.018149714916944504,\n",
       "  -0.0075164190493524075,\n",
       "  -0.009991963393986225,\n",
       "  -0.005454534664750099,\n",
       "  -0.022985363379120827,\n",
       "  -0.009280083701014519,\n",
       "  0.02154877781867981,\n",
       "  0.004033982753753662,\n",
       "  -0.02020197920501232,\n",
       "  -0.0042103491723537445,\n",
       "  -0.022125978022813797,\n",
       "  0.014622385613620281,\n",
       "  0.019868487492203712,\n",
       "  0.00658648693934083,\n",
       "  0.0017444249242544174,\n",
       "  -0.015905050560832024,\n",
       "  0.0031473401468247175,\n",
       "  -0.012287934310734272,\n",
       "  0.011165602132678032,\n",
       "  0.012409787625074387,\n",
       "  0.010517856106162071,\n",
       "  -0.007086726371198893,\n",
       "  3.1283757380151656e-06,\n",
       "  0.007574139162898064,\n",
       "  -0.001630588318221271,\n",
       "  0.010383176617324352,\n",
       "  0.0011239355662837625,\n",
       "  -0.026935972273349762,\n",
       "  -0.0076318588107824326,\n",
       "  0.02850082516670227,\n",
       "  0.007901218719780445,\n",
       "  0.0026984072756022215,\n",
       "  0.0213692057877779,\n",
       "  -0.004951088223606348,\n",
       "  -0.0047266217879951,\n",
       "  0.0023552943021059036,\n",
       "  0.007612619083374739,\n",
       "  0.026782052591443062,\n",
       "  -0.010152296163141727,\n",
       "  0.02438346855342388,\n",
       "  0.008279604837298393,\n",
       "  0.007125206291675568,\n",
       "  -0.028475170955061913,\n",
       "  0.016033317893743515,\n",
       "  -0.06244014948606491,\n",
       "  -0.006176033988595009,\n",
       "  -0.0378129743039608,\n",
       "  0.009337804280221462,\n",
       "  0.005945154000073671,\n",
       "  -0.0023937742225825787,\n",
       "  -0.023985842242836952,\n",
       "  -0.018624301999807358,\n",
       "  0.008234711363911629,\n",
       "  0.01657203584909439,\n",
       "  0.02293405681848526,\n",
       "  -0.023344509303569794,\n",
       "  0.01418627891689539,\n",
       "  -0.03794124349951744,\n",
       "  -0.0019207914592698216,\n",
       "  -0.007734472397714853,\n",
       "  -0.01595635712146759,\n",
       "  0.0058168875984847546,\n",
       "  0.00040103335049934685,\n",
       "  -0.019637607038021088,\n",
       "  0.02449890784919262,\n",
       "  -0.0054641542956233025,\n",
       "  0.017174890264868736,\n",
       "  0.00023909684387035668,\n",
       "  -0.01802144944667816,\n",
       "  -0.016212889924645424,\n",
       "  0.022177284583449364,\n",
       "  0.027808185666799545,\n",
       "  0.0012369704199954867,\n",
       "  -0.011325934901833534,\n",
       "  -0.005242894869297743,\n",
       "  0.023229070007801056,\n",
       "  -0.021689871326088905,\n",
       "  0.026268986985087395,\n",
       "  -0.016033317893743515,\n",
       "  -0.013698866590857506,\n",
       "  0.012146840803325176,\n",
       "  0.038967374712228775,\n",
       "  -0.017046622931957245,\n",
       "  0.02385757677257061,\n",
       "  0.005236481316387653,\n",
       "  0.049382615834474564,\n",
       "  0.0011511922348290682,\n",
       "  0.003559396369382739,\n",
       "  0.007984591647982597,\n",
       "  0.013378200121223927,\n",
       "  0.0029725770000368357,\n",
       "  0.006467840168625116,\n",
       "  0.025473734363913536,\n",
       "  -0.0010686205932870507,\n",
       "  -0.031553566455841064,\n",
       "  0.027012933045625687,\n",
       "  0.009036378003656864,\n",
       "  -0.004066049121320248,\n",
       "  0.006855846382677555,\n",
       "  0.016123102977871895,\n",
       "  -0.009709777310490608,\n",
       "  0.02133072540163994,\n",
       "  0.003325310070067644,\n",
       "  0.012313587591052055,\n",
       "  0.01083852257579565,\n",
       "  0.005428881384432316,\n",
       "  -0.016764435917139053,\n",
       "  0.02526850812137127,\n",
       "  -0.01243544090539217,\n",
       "  -0.011511921882629395,\n",
       "  -0.0054577412083745,\n",
       "  -0.007811432238668203,\n",
       "  0.010049683041870594,\n",
       "  0.017418595030903816,\n",
       "  -0.002531660720705986,\n",
       "  -0.004296929109841585,\n",
       "  -0.0046945554204285145,\n",
       "  -0.0011864654952660203,\n",
       "  -0.004316168837249279,\n",
       "  -0.0091454042121768,\n",
       "  -0.035119377076625824,\n",
       "  0.0002130427019437775,\n",
       "  0.028859971091151237,\n",
       "  -0.008920937776565552,\n",
       "  -6.949441012693569e-05,\n",
       "  -0.015917876735329628,\n",
       "  0.008548964746296406,\n",
       "  3.617517359089106e-05,\n",
       "  0.0051787616685032845,\n",
       "  -0.0018117649015039206,\n",
       "  0.016700303182005882,\n",
       "  -0.007093139458447695,\n",
       "  0.00044091622112318873,\n",
       "  -0.008472004905343056,\n",
       "  -0.0066955131478607655,\n",
       "  -0.0014758668839931488,\n",
       "  -0.00872853770852089,\n",
       "  0.01099244225770235,\n",
       "  -0.009331390261650085,\n",
       "  0.028013411909341812,\n",
       "  0.0019801147282123566,\n",
       "  -0.00480999518185854,\n",
       "  -0.007849912159144878,\n",
       "  0.0007716033724136651,\n",
       "  -0.01780339516699314,\n",
       "  0.01672595739364624,\n",
       "  -0.008132098242640495,\n",
       "  0.006067007314413786,\n",
       "  0.004838855005800724,\n",
       "  -0.0067724729888141155,\n",
       "  -0.018098408356308937,\n",
       "  -0.0073304325342178345,\n",
       "  0.008677231147885323,\n",
       "  0.0052942014299333096,\n",
       "  -0.002028214745223522,\n",
       "  -0.019445206969976425,\n",
       "  0.03286188840866089,\n",
       "  0.02865474484860897,\n",
       "  0.013147319667041302,\n",
       "  -0.0018887247424572706,\n",
       "  -0.005284581333398819,\n",
       "  -0.01386561244726181,\n",
       "  -0.000796054198872298,\n",
       "  0.08286018669605255,\n",
       "  0.012255867943167686,\n",
       "  -0.01165301539003849,\n",
       "  0.02868039719760418,\n",
       "  0.03316972777247429,\n",
       "  -0.022946882992982864,\n",
       "  -0.03470892459154129,\n",
       "  0.0072214058600366116,\n",
       "  0.010639709420502186,\n",
       "  -0.024845227599143982,\n",
       "  -0.0035914629697799683,\n",
       "  -0.008811911568045616,\n",
       "  0.04171227663755417,\n",
       "  -0.0034407498314976692,\n",
       "  0.013993878848850727,\n",
       "  -0.017598168924450874,\n",
       "  0.015468944795429707,\n",
       "  0.028629090636968613,\n",
       "  0.020522646605968475,\n",
       "  -0.0224466435611248,\n",
       "  0.00513386819511652,\n",
       "  0.008151338435709476,\n",
       "  0.014276065863668919,\n",
       "  0.014994358643889427,\n",
       "  0.01734163612127304,\n",
       "  -0.003998709376901388,\n",
       "  0.008734950795769691,\n",
       "  0.030450476333498955,\n",
       "  0.01387843955308199,\n",
       "  0.0014790735440328717,\n",
       "  -0.016674650833010674,\n",
       "  -0.0003166580281686038,\n",
       "  0.006689100060611963,\n",
       "  0.005092181731015444,\n",
       "  -0.01727750338613987,\n",
       "  -0.0032836233731359243,\n",
       "  0.0011455805506557226,\n",
       "  0.01532785128802061,\n",
       "  0.012050640769302845,\n",
       "  0.013814305886626244,\n",
       "  0.013057533651590347,\n",
       "  -0.012582947500050068,\n",
       "  0.005794440861791372,\n",
       "  -0.029834797605872154,\n",
       "  0.0020987612660974264,\n",
       "  -0.009741843678057194,\n",
       "  -0.024114109575748444,\n",
       "  0.017482729628682137,\n",
       "  -0.008189818821847439,\n",
       "  -0.016700303182005882,\n",
       "  0.01224945392459631,\n",
       "  0.019355420023202896,\n",
       "  0.010261323302984238,\n",
       "  -0.031656183302402496,\n",
       "  -0.014981531538069248,\n",
       "  0.002156481146812439,\n",
       "  -0.004899781662970781,\n",
       "  -0.006160000339150429,\n",
       "  -0.02555069513618946,\n",
       "  -0.02278013713657856,\n",
       "  -0.012813827022910118,\n",
       "  0.002454700879752636,\n",
       "  0.021099844947457314,\n",
       "  -0.009684124030172825,\n",
       "  1.5641878690075828e-06,\n",
       "  -0.013224280439317226,\n",
       "  -0.004588735289871693,\n",
       "  -0.002507610712200403,\n",
       "  -0.02579439990222454,\n",
       "  0.010877002030611038,\n",
       "  -0.014840438030660152,\n",
       "  -0.005624487530440092,\n",
       "  -0.024575868621468544,\n",
       "  0.007118792738765478,\n",
       "  0.015699824318289757,\n",
       "  0.035888977348804474,\n",
       "  0.01171073503792286,\n",
       "  0.009581509977579117,\n",
       "  -0.003338136710226536,\n",
       "  0.009260844439268112,\n",
       "  0.01154398825019598,\n",
       "  -0.01089624222368002,\n",
       "  -0.027218159288167953,\n",
       "  -0.03222055360674858,\n",
       "  0.0017492348561063409,\n",
       "  0.012736867181956768,\n",
       "  0.0033445500303059816,\n",
       "  0.00276254047639668,\n",
       "  -0.0184447281062603,\n",
       "  0.010434483177959919,\n",
       "  0.012890786863863468,\n",
       "  -0.017008142545819283,\n",
       "  0.012089121155440807,\n",
       "  -0.02508893422782421,\n",
       "  0.003565809689462185,\n",
       "  -0.008234711363911629,\n",
       "  0.01773926243185997,\n",
       "  -0.0007070692954584956,\n",
       "  0.007734472397714853,\n",
       "  -0.0023665176704525948,\n",
       "  0.004563082009553909,\n",
       "  -0.01066536270081997,\n",
       "  -0.019791526719927788,\n",
       "  -0.0027962105814367533,\n",
       "  0.019073234871029854,\n",
       "  -0.004816408269107342,\n",
       "  -0.02290840446949005,\n",
       "  0.0010397606529295444,\n",
       "  -0.008266778662800789,\n",
       "  0.017508381977677345,\n",
       "  0.020099366083741188,\n",
       "  -0.009138991124927998,\n",
       "  0.01226869411766529,\n",
       "  0.0064934934489429,\n",
       "  -0.00713161937892437,\n",
       "  0.011101468466222286,\n",
       "  -0.007426632568240166,\n",
       "  0.02392170950770378,\n",
       "  -0.0008537741377949715,\n",
       "  0.0016153566539287567,\n",
       "  0.005258928053081036,\n",
       "  -0.019291287288069725,\n",
       "  -0.012486747466027737,\n",
       "  -0.006592900026589632,\n",
       "  0.002623050706461072,\n",
       "  -0.000715486763510853,\n",
       "  0.0010622072732076049,\n",
       "  -0.004960708320140839,\n",
       "  -0.011903135105967522,\n",
       "  0.006438980344682932,\n",
       "  -0.008260364644229412,\n",
       "  0.034041937440633774,\n",
       "  -0.002510817488655448,\n",
       "  0.0022077877074480057,\n",
       "  -0.01632833108305931,\n",
       "  -0.010030442848801613,\n",
       "  0.010780802927911282,\n",
       "  0.02698727883398533,\n",
       "  -0.013301240280270576,\n",
       "  -0.006163207348436117,\n",
       "  -0.025845706462860107,\n",
       "  -0.0037582095246762037,\n",
       "  0.013737346045672894,\n",
       "  -0.0031617700587958097,\n",
       "  0.0012858720729127526,\n",
       "  -0.045457661151885986,\n",
       "  0.005438501015305519,\n",
       "  -0.019496513530611992,\n",
       "  0.00880549754947424,\n",
       "  0.01072949543595314,\n",
       "  -0.00138127023819834,\n",
       "  -0.004255242180079222,\n",
       "  -0.018560167402029037,\n",
       "  0.011075815185904503,\n",
       "  -0.0008225091733038425,\n",
       "  -0.0045438422821462154,\n",
       "  -0.0091454042121768,\n",
       "  -0.005393608007580042,\n",
       "  0.027628611773252487,\n",
       "  0.043944116681814194,\n",
       "  0.025499388575553894,\n",
       "  -0.005342301446944475,\n",
       "  0.0335032194852829,\n",
       "  -0.01000478956848383,\n",
       "  -0.01534067839384079,\n",
       "  0.006836606655269861,\n",
       "  -0.002416220959275961,\n",
       "  0.011274628341197968,\n",
       "  -0.03470892459154129,\n",
       "  0.0182908084243536,\n",
       "  0.0028138472698628902,\n",
       "  0.005733514204621315,\n",
       "  -0.0064934934489429,\n",
       "  -0.01721336878836155,\n",
       "  -0.0013387820217758417,\n",
       "  0.03114311583340168,\n",
       "  -0.007433045655488968,\n",
       "  0.008414285257458687,\n",
       "  -0.013378200121223927,\n",
       "  -0.0035754297859966755,\n",
       "  0.006682686507701874,\n",
       "  0.005544321145862341,\n",
       "  -0.03037351556122303,\n",
       "  0.022728830575942993,\n",
       "  0.015148278325796127,\n",
       "  -0.007394565735012293,\n",
       "  0.029193464666604996,\n",
       "  0.004960708320140839,\n",
       "  0.0181368887424469,\n",
       "  -0.0184447281062603,\n",
       "  0.025717440992593765,\n",
       "  -0.013070359826087952,\n",
       "  0.0030190737452358007,\n",
       "  0.006137554068118334,\n",
       "  0.0058746072463691235,\n",
       "  0.0030848102178424597,\n",
       "  0.001932014711201191,\n",
       "  -0.012403374537825584,\n",
       "  -0.002413014182820916,\n",
       "  0.01998392678797245,\n",
       "  0.002797813853248954,\n",
       "  0.013596253469586372,\n",
       "  0.0025060074403882027,\n",
       "  -0.024139761924743652,\n",
       "  -0.0018085582414641976,\n",
       "  0.020522646605968475,\n",
       "  -0.037736017256975174,\n",
       "  -0.015853744000196457,\n",
       "  -0.0010533889289945364,\n",
       "  -0.022459471598267555,\n",
       "  0.002073107985779643,\n",
       "  0.004075669217854738,\n",
       "  -0.03234881907701492,\n",
       "  -0.029398690909147263,\n",
       "  0.01745707541704178,\n",
       "  0.003338136710226536,\n",
       "  -0.021600084379315376,\n",
       "  0.02474261447787285,\n",
       "  -0.0010606038849800825,\n",
       "  -0.024832401424646378,\n",
       "  0.0011616138508543372,\n",
       "  -0.003995502833276987,\n",
       "  0.046868592500686646,\n",
       "  0.018431901931762695,\n",
       "  0.00625620037317276,\n",
       "  -0.005768787581473589,\n",
       "  0.009780323132872581,\n",
       "  -0.031245728954672813,\n",
       "  0.011858241632580757,\n",
       "  -1.712308221613057e-05,\n",
       "  0.0022735244128853083,\n",
       "  0.014673692174255848,\n",
       "  0.02508893422782421,\n",
       "  0.004614388570189476,\n",
       "  0.01332689356058836,\n",
       "  -0.015186757780611515,\n",
       "  0.012422613799571991,\n",
       "  0.008734950795769691,\n",
       "  -0.03224620595574379,\n",
       "  -0.0011736388551071286,\n",
       "  -0.009568683803081512,\n",
       "  0.005707860924303532,\n",
       "  -0.02299818955361843,\n",
       "  -0.0073304325342178345,\n",
       "  -0.01291002705693245,\n",
       "  0.006506320089101791,\n",
       "  0.0018678815104067326,\n",
       "  -0.0039121294394135475,\n",
       "  -0.001757251564413309,\n",
       "  -0.010562749579548836,\n",
       "  -0.009645643644034863,\n",
       "  -0.007843499071896076,\n",
       "  -0.023100802674889565,\n",
       "  -0.00019280063861515373,\n",
       "  -0.007657512091100216,\n",
       "  -0.012448267079889774,\n",
       "  -0.0061503807082772255,\n",
       "  0.003488849848508835,\n",
       "  0.0009499740554019809,\n",
       "  -0.01258936058729887,\n",
       "  -0.010453722439706326,\n",
       "  0.01690552942454815,\n",
       "  -0.020727872848510742,\n",
       "  -0.0051434882916510105,\n",
       "  0.024614349007606506,\n",
       "  -0.006592900026589632,\n",
       "  -0.007849912159144878,\n",
       "  -0.008933763951063156,\n",
       "  0.004232795909047127,\n",
       "  0.02286992408335209,\n",
       "  -0.03191271424293518,\n",
       "  -0.019009100273251534,\n",
       "  -0.0005527486209757626,\n",
       "  0.020715046674013138,\n",
       "  0.027038585394620895,\n",
       "  -0.004486122168600559,\n",
       "  -0.0035914629697799683,\n",
       "  -0.014134972356259823,\n",
       "  -0.013128080405294895,\n",
       "  -0.008940177969634533,\n",
       "  -0.004527808632701635,\n",
       "  0.0002623451582621783,\n",
       "  -0.026217680424451828,\n",
       "  0.008856804110109806,\n",
       "  -0.03098919615149498,\n",
       "  -0.022523604333400726,\n",
       "  0.002010578056797385,\n",
       "  -0.010953961871564388,\n",
       "  0.012198147363960743,\n",
       "  -0.019778700545430183,\n",
       "  -0.025319814682006836,\n",
       "  0.0026278607547283173,\n",
       "  -0.001385278650559485,\n",
       "  0.016828570514917374,\n",
       "  0.012294347397983074,\n",
       "  -0.02489653415977955,\n",
       "  -0.009889350272715092,\n",
       "  0.012582947500050068,\n",
       "  -0.010735909454524517,\n",
       "  0.029065197333693504,\n",
       "  0.00785632524639368,\n",
       "  0.006349193397909403,\n",
       "  -0.02698727883398533,\n",
       "  -0.003437543287873268,\n",
       "  -0.0007194951176643372,\n",
       "  -0.03550417721271515,\n",
       "  0.005201207939535379,\n",
       "  0.004389922134578228,\n",
       "  -0.03068135678768158,\n",
       "  0.021779658272862434,\n",
       "  0.00027817804948426783,\n",
       "  0.008067965507507324,\n",
       "  0.002315211109817028,\n",
       "  0.0013564185937866569,\n",
       "  0.022151630371809006,\n",
       "  -0.008779844269156456,\n",
       "  -0.00400512246415019,\n",
       "  -0.0042488290928304195,\n",
       "  -0.01308318693190813,\n",
       "  0.0091454042121768,\n",
       "  -0.007849912159144878,\n",
       "  0.009286497719585896,\n",
       "  0.022767310962080956,\n",
       "  -0.005409641191363335,\n",
       "  -0.018034275621175766,\n",
       "  -0.0008361375075764954,\n",
       "  0.021997710689902306,\n",
       "  0.01657203584909439,\n",
       "  -0.00705465953797102,\n",
       "  0.005967600736767054,\n",
       "  0.009427590295672417,\n",
       "  0.015917876735329628,\n",
       "  0.0023280377499759197,\n",
       "  -0.01332689356058836,\n",
       "  -0.0025765541940927505,\n",
       "  -0.004425195511430502,\n",
       "  -0.028295598924160004,\n",
       "  0.0387364961206913,\n",
       "  -0.003931369166821241,\n",
       "  -0.0028731704223901033,\n",
       "  -0.005810474045574665,\n",
       "  0.030963541939854622,\n",
       "  0.0189064871519804,\n",
       "  0.011781281791627407,\n",
       "  0.022036191076040268,\n",
       "  -0.017174890264868736,\n",
       "  -0.02068939246237278,\n",
       "  0.0016834982670843601,\n",
       "  -0.025922667235136032,\n",
       "  -0.021574432030320168,\n",
       "  0.0077793654054403305,\n",
       "  0.01340385340154171,\n",
       "  0.015468944795429707,\n",
       "  -0.005165935028344393,\n",
       "  -0.012390547432005405,\n",
       "  0.009812390431761742,\n",
       "  -0.03871084004640579,\n",
       "  -0.018701260909438133,\n",
       "  -0.015058491379022598,\n",
       "  0.009267257526516914,\n",
       "  0.02040720544755459,\n",
       "  0.014571079052984715,\n",
       "  0.00013948985724709928,\n",
       "  0.030168289318680763,\n",
       "  0.00674040662124753,\n",
       "  0.020368726924061775,\n",
       "  -0.008600271306931973,\n",
       "  -0.012390547432005405,\n",
       "  0.019804352894425392,\n",
       "  0.012518813833594322,\n",
       "  0.0009227173868566751,\n",
       "  0.012294347397983074,\n",
       "  -0.03683815151453018,\n",
       "  -0.032759275287389755,\n",
       "  0.006714753340929747,\n",
       "  -0.005226861219853163,\n",
       "  -0.00046055702841840684,\n",
       "  0.0024498908314853907,\n",
       "  -0.006092660594731569,\n",
       "  -0.02588418684899807,\n",
       "  -0.0071444460190832615,\n",
       "  0.0017620616126805544,\n",
       "  0.00905561726540327,\n",
       "  0.01066536270081997,\n",
       "  0.002127621090039611,\n",
       "  -0.015853744000196457,\n",
       "  -0.011768454685807228,\n",
       "  0.01841907575726509,\n",
       "  0.004271275829523802,\n",
       "  -0.01853451505303383,\n",
       "  -0.005861780606210232,\n",
       "  -0.004970328416675329,\n",
       "  0.0068814996629953384,\n",
       "  0.023113630712032318,\n",
       "  -0.018983447924256325,\n",
       "  -0.026756400242447853,\n",
       "  -0.007888391613960266,\n",
       "  -0.010428069159388542,\n",
       "  0.0006669859867542982,\n",
       "  -0.005884227342903614,\n",
       "  0.011441375128924847,\n",
       "  0.00553470104932785,\n",
       "  0.010068923234939575,\n",
       "  -0.008311671204864979,\n",
       "  -0.00674040662124753,\n",
       "  0.0014638418797403574,\n",
       "  -0.01525089144706726,\n",
       "  0.0017380116041749716,\n",
       "  -0.020343072712421417,\n",
       "  -0.020830485969781876,\n",
       "  -0.005076148081570864,\n",
       "  0.003060760209336877,\n",
       "  -0.010594815947115421,\n",
       "  -0.005727101117372513,\n",
       "  0.004521395545452833,\n",
       "  -0.021779658272862434,\n",
       "  -0.0353502593934536,\n",
       "  0.011011682450771332,\n",
       "  0.013429506681859493,\n",
       "  0.011024508625268936,\n",
       "  -0.010851348750293255,\n",
       "  -0.004354648757725954,\n",
       "  -0.010716669261455536,\n",
       "  -0.006843019742518663,\n",
       "  -0.01211477443575859,\n",
       "  -0.00043851122609339654,\n",
       "  -0.01721336878836155,\n",
       "  -0.015032838098704815,\n",
       "  0.003233920084312558,\n",
       "  -0.005214034579694271,\n",
       "  -0.0038063095416873693,\n",
       "  -0.03391367197036743,\n",
       "  -0.0181368887424469,\n",
       "  0.00891452468931675,\n",
       "  -0.02478109486401081,\n",
       "  -0.007805018685758114,\n",
       "  0.1841907501220703,\n",
       "  -8.327304385602474e-05,\n",
       "  -0.01791883446276188,\n",
       "  0.02533264085650444,\n",
       "  -0.006236960180103779,\n",
       "  0.042379263788461685,\n",
       "  0.028552131727337837,\n",
       "  -0.0015760750975459814,\n",
       "  0.0026952007319778204,\n",
       "  -0.005884227342903614,\n",
       "  0.010274149477481842,\n",
       "  0.022279897704720497,\n",
       "  -0.035863324999809265,\n",
       "  -0.012627840973436832,\n",
       "  0.021138325333595276,\n",
       "  0.0052172415889799595,\n",
       "  -0.015109797939658165,\n",
       "  -0.02755165286362171,\n",
       "  -0.03745383024215698,\n",
       "  -0.010658949613571167,\n",
       "  -0.03222055360674858,\n",
       "  -0.019291287288069725,\n",
       "  -0.006041354034096003,\n",
       "  -0.02035589888691902,\n",
       "  0.04363627731800079,\n",
       "  -0.005262134596705437,\n",
       "  -0.022831443697214127,\n",
       "  0.013519292697310448,\n",
       "  0.00874777790158987,\n",
       "  0.004588735289871693,\n",
       "  -0.0215231254696846,\n",
       "  -0.014353025704622269,\n",
       "  -0.006605726666748524,\n",
       "  0.0028491204138845205,\n",
       "  0.0021693077869713306,\n",
       "  0.006618553306907415,\n",
       "  0.02112549915909767,\n",
       "  -0.00023508851882070303,\n",
       "  0.045791152864694595,\n",
       "  0.013852786272764206,\n",
       "  0.021497471258044243,\n",
       "  -0.012448267079889774,\n",
       "  -0.019753046333789825,\n",
       "  0.0015335867647081614,\n",
       "  -0.006127933971583843,\n",
       "  0.030399169772863388,\n",
       "  ...]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"here is some text to encode\", \"and some more\"]\n",
    "\n",
    "embeds = embedding_model.embed_documents(docs)\n",
    "embeds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 2 embeddings each with dimensionality of `1536` (`text-embedding-ada-002`'s embedding size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeds), len(embeds[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vector DB\n",
    "\n",
    "Now we can move on to building the vector DB, which is where we'll store our embeddings and metadata.\n",
    "\n",
    "We start by initializing a Pinecone index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=\"YOUR_API_KEY\",  # app.pinecone.io\n",
    "    environment=\"us-east1-gcp\"  # check aligns to env in console (next to api key)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply this across all of our documents, we'll do everything in batches. So we iterate through `texts`, embed a number of the texts, then add them to Pinecone — then we move onto the next batch. We start by creating a langchain vector store object using the `Pinecone.from_texts` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "index_name = \"arxiv-bot\"\n",
    "docsearch = Pinecone.from_texts(\n",
    "    texts,\n",
    "    embedding_model,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do this directly with OpenAI + Pinecone it's faster and seems more reliable (for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def embed(docs: list):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Embedding.create(\n",
    "        input=docs, engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    embeds = [r['embedding'] for r in res['data']]\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "index_name = \"arxiv-bot\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, 1536)\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "for i in tqdm(range(9400, len(texts), batch_size)):\n",
    "    i_end = min(i+batch_size, len(texts))\n",
    "    embeds_batch = embed(texts[i:i_end])\n",
    "    ids_batch = ids[i:i_end]\n",
    "    assert len(embeds_batch) == len(ids_batch)\n",
    "    to_upsert = zip(ids_batch, embeds_batch)\n",
    "    index.upsert(to_upsert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "index_name = \"arxiv-bot\"\n",
    "\n",
    "docsearch = Pinecone.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch.similarity_search(\"what is react?\", k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will submit a PR to fix this at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '2301.07094-2',\n",
       "              'score': 0.803957462,\n",
       "              'sparseValues': {},\n",
       "              'values': []},\n",
       "             {'id': '2301.07094-22',\n",
       "              'score': 0.786604226,\n",
       "              'sparseValues': {},\n",
       "              'values': []},\n",
       "             {'id': '2301.07094-23',\n",
       "              'score': 0.786588728,\n",
       "              'sparseValues': {},\n",
       "              'values': []},\n",
       "             {'id': '2301.07094-17',\n",
       "              'score': 0.78344804,\n",
       "              'sparseValues': {},\n",
       "              'values': []},\n",
       "             {'id': '2301.07094-0',\n",
       "              'score': 0.780215323,\n",
       "              'sparseValues': {},\n",
       "              'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "\n",
    "xq = embed([\"what is react?\"])[0]\n",
    "xc = index.query(xq, top_k=5)\n",
    "xc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a local key-value store to extract whatever it is we're seeing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = {}\n",
    "\n",
    "for record in dataset:\n",
    "    key = f\"{record['doi']}-{record['chunk-id']}\"\n",
    "    if key not in kv:\n",
    "        kv[key] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2301.07094\n",
      "REACT\n",
      "CLIP\n",
      "Semi-ViT\n",
      "SimCLR-v2\n",
      "              60657075808590Classification / Retrieval Performance+2.8+1.1+0.4\n",
      "+3.8+3.5 +5.1+2.0+1.4+3.0\n",
      "+10\n",
      "Classification\n",
      "ImageNetZero-Shot 1% 10%\n",
      "Classification\n",
      "ELEVATER BenchmarkZero-Shot Few-ShotLP FT\n",
      "Full-ShotLP FT\n",
      "Retrieval\n",
      "Flickr30KI2T T2I\n",
      "Detection\n",
      "MSCOCOZero-Shot Open-Voc\n",
      "Segmentation\n",
      "MSCOCOZero-Shot Anno-FreeREACT\n",
      "CLIP\n",
      "1520253035404550\n",
      "Dense Prediction Performance+1.5+3.4\n",
      "+3.6+2.6Figure 1. REACT achieves the best zero-shot ImageNet performance among public checkpoints with nearly 5\u0002smaller data size (Left),\n",
      "achieves new SoTA on semi-supervised ImageNet classiﬁcation in the 1% labelled data setting (Middle), and consistently transfer better\n",
      "than CLIP on across a variety of tasks, including ImageNet classiﬁcation, zero/few/full-shot classiﬁcation on 20 datasets in ELEV ATER\n",
      "benchmark, image-text retrieval, object detection and segmentation (Right). Please see the detailed numbers and settings in the experimental\n",
      "section. For the left ﬁgure, circle size indicates model size.\n",
      "and evaluation set, and richer text descriptions about concept\n",
      "semantics. Such multi-modal knowledge sources are generally available on the web, and further open-sourced like\n",
      "LAION [59, 60]. They cover a variety of domains, making it\n",
      "possible to develop customized visual models for task-level\n",
      "transfer. Similar retrieval-augmented intuitions have been\n",
      "exploited in computer vision for class-level transfer [44], but\n",
      "not yet for task-level transfer (similar to that of CLIP). Our\n",
      "main ﬁndings/contributions can be summarized as follows.\n",
      "We propose to explore the potential of the web-scale\n",
      "image-text corpus as external knowledge to signiﬁcantly\n",
      "improve task-level transfer performance on the target domain at an affordable cost. A simple and effective strategy\n",
      "is proposed. To begin with, we build a large-scale multimodal indexing system to retrieve the relevant image-text\n",
      "pairs using CLIP features and approximate nearest neighbor search. For a CV problem, the task instruction is often\n",
      "sufﬁciently speciﬁed with text such as class names, which\n",
      "allows us to utilize them as queries to retrieve the relevant\n",
      "image-text pair knowledge from the indexing system. No\n",
      "images from the CV problem are needed. To efﬁciently build\n",
      "the customized visual model, we propose a novel modularized learning strategy: only updating the additional trainable\n",
      "weights on the retrieved knowledge, and freezing the original model weights. Hence, the model masters the new skill\n",
      "without forgetting basic skills.\n",
      "2301.07094\n",
      "REACT 96.6 99.9 86.8 98.0 78.7 94.0 61.1 84.1\n",
      "Table 4. Image-text retrieval results on Flickr30K [55] and\n",
      "MSCOCO [42] datasets. CLIPy, Bletchleyy: our evaluation.\n",
      "performance to the baseline on 15/1/4 datasets for CLIP\n",
      "and 14/0/6 datasets for OpenCLIP, respectively. Most of\n",
      "the improved and failure datasets are consistent for both\n",
      "checkpoints. For the top two datasets that gains the most,\n",
      "i.e. StanfordCars and FGVC Aircraft, relevant image-text\n",
      "knowledge is retrieved from the web-crawled data LAION400M to describe the concepts; see Fig. 5a. Interestingly,\n",
      "this observation is complementary to K-LITE [63], which\n",
      "failed on these two datasets, because no knowledge was\n",
      "extracted from Wiktionary for them, as it often requires\n",
      "domain-speciﬁc knowledge and even visual knowledge to\n",
      "best deﬁne a car brand ( e.g. BMW X6 SUV or Audi R8) or\n",
      "an aircraft model type ( e.g. DC-10 or A321).\n",
      "Limitations. As shown in Fig. 4, REACT struggles on the\n",
      "PatchCamelyon dataset, a cancer cell recognition benchmark.\n",
      "We visualize the retrieved samples and the samples from the\n",
      "original training set in Fig. 5b. The retrieved images are\n",
      "either instruction photos and from another sensing method,\n",
      "which exhibits a different visual distribution from PatchCamelyon. This suggests the importance of ensuring the\n",
      "retrieval quality for the domain-of-interest.\n",
      "4.2. Image-Text Retrieval\n",
      "To demonstrate the generality of REACT , we consider\n",
      "Flickr30K [81] and MSCOCO [42] image-text retrieval tasks,\n",
      "in both zero-shot and full-shot settings. We use the standard\n",
      "image-text contrastive objective [57]. For image-text retrieval task, following [31, 57], we use the CLIP-L/14 with\n",
      "336x336 input resolution in both zero-shot, customization,\n",
      "and ﬁne-tuning stage. We use the captions from MSCOCO\n",
      "as queries to retrieve 6M image-text pairs and perform customization. Note that none of the caption queries are used in\n",
      "the model training stage.\n",
      "As shown in Table 4, REACT improves the generic CLIP\n",
      "counterparts on both zero-shot and full-shot retrieval for\n",
      "Flickr30K and MSCOCO datasets. The gain on zero-shot\n",
      "task transfer is large. On Flickr30K, it achieves 3.4%/10.0%recall improvement for I2T and T2I retrieval, respectively.\n",
      "Afer ﬁne-tuning on full training data, REACT still improves\n",
      "over the baseline slightly. It provides another piece of evidence for REACT in data-rich settings. Furthermore, we\n",
      "conduct the same customization procedure of REACT on a\n",
      "large checkpoint Bletchley [1] with 864M parameters, and\n",
      "observe consistent gains over both datasets. It demonstrates\n",
      "2301.07094\n",
      "Afer ﬁne-tuning on full training data, REACT still improves\n",
      "over the baseline slightly. It provides another piece of evidence for REACT in data-rich settings. Furthermore, we\n",
      "conduct the same customization procedure of REACT on a\n",
      "large checkpoint Bletchley [1] with 864M parameters, and\n",
      "observe consistent gains over both datasets. It demonstrates\n",
      "that R EACT scales well with model size on retrieval tasks.\n",
      "4.3. Dense Prediction Tasks\n",
      "Although REACT is optimized with the image-level contrastive loss during the customization stage, we ﬁnd it beneﬁcial for dense prediction tasks as well. We showcase its\n",
      "application to dense prediction tasks on object detection and\n",
      "semantic segmentation.\n",
      "4.3.1 Object Detection\n",
      "For object detection, we choose the state-of-the-art RegionCLIP [86] as our framework. We conduct experiments in two\n",
      "settings: zero-shot inference with ground-truth (GT)/ Region\n",
      "Proposal Networks (RPN) boxes and open-vocabulary object detection on MSCOCO dataset. We perform the model\n",
      "customization following the same setting as Sec. 4.2. Following RegionCLIP, we conduct experiments on ResNet50\n",
      "backbone. Additionally, we present results on ViT-B/16\n",
      "backbone for zero-shot inference.\n",
      "Zero-shot inference. RegionCLIP alters the architecture of\n",
      "an object detector. It is able to perform zero-shot object detection, by (1) initializing its backbone and prediction head\n",
      "from the CLIP-pretrained checkpoints, and (2) employing a\n",
      "pretrained RPN network. The results are shown in Table 5.\n",
      "REACT consistently improves over CLIP checkpoint under\n",
      "all settings. When ground-truth region proposal is used, REACT improves over CLIP by +1.0 on overall AP50; when the\n",
      "pretrained RPN is used, REACT demonstrates +1.5/+1.4/+1.9\n",
      "AP50 improvements on novel, base, and all classes, respectively. These results are encouraging, as it shows that the\n",
      "customized knowledge from REACT transfers well to dense\n",
      "prediction tasks like object detection, under the RegionCLIP\n",
      "framework.\n",
      "Open-vocabulary detection (OVD). We further conduct experiments on the open-vocabulary settings, where the model\n",
      "ﬁnetunes on a set of selected categories ( base classes), and\n",
      "evaluate on both seen ( base) and unseen ( novel ) classes. We\n",
      "report the results in Table 5 (OVD).\n",
      "We can see that with the REACT customization, the detector yields improved performance on base with +2.3 AP50,\n",
      "and importantly, it signiﬁcantly improves novel categories\n",
      "with +6.4 AP50. This suggests that the injected knowledge\n",
      "during the model customization stage improves the learned\n",
      "ﬁne-grained visual feature that is beneﬁcial to both seen and\n",
      "unseen categories for object detection, when the downstream\n",
      "8\n",
      "PretrainBackboneRegion MSCOCO AP 50\n",
      "Method Proposals Novel Base AllZero-ShotCLIP ResNet-50 GT 58.6 58.2 58.3\n",
      "2301.07094\n",
      "zero-shot image classiﬁcation on ImageNet-1K, with different backbones and original pretraining datasets. There are\n",
      "three interesting ﬁndings.\n",
      "F1:REACT can beneﬁt from model’s own pre-training\n",
      "data. Compared to OpenCLIP [29] (ViT-B/32) trained on\n",
      "LAION-400M, by training on 10M relevant pairs from the\n",
      "same LAION-400M dataset, REACT improves over OpenCLIP by 3.5%. Note that the model purely uses the imagetext pairs that it has seen during its pre-training, and does\n",
      "notsee any extra data. This shows that REACT can more\n",
      "adequately adapt to the target domain during the model customization stage, suggesting a favorable property that no\n",
      "new data is required for customization.\n",
      "F2:REACT efﬁciently explores new image-text sources,\n",
      "even for large models. We costomize CLIP [57] ViT-L/14\n",
      "on 10M retrieved relevant image-text pairs, and the model\n",
      "achieves a 2.8% improvement to 78.1%. This surpasses all\n",
      "publicly available checkpoints from CLIP and OpenCLIP,\n",
      "including the checkpoint with a much larger ViT-H/14 backbone and trained on a much larger LAION-2B dataset. This\n",
      "suggests that REACT is a more sample-efﬁcient approach to\n",
      "improve the model performance on the domain-of-interest.\n",
      "F3: Scaling up the retrieval pool increases performance.\n",
      "We perform REACT in a privately collected dataset with over\n",
      "800M pairs, and train a customized model on 6M retrieved\n",
      "pairs. The performance is increased to 78.5%, yielding 0.9%\n",
      "gain compared with 6M pairs retrieved from LAION-400M.\n",
      "This suggests that REACT scales well with the larger retrievalf\u0012 Pretrain DataRetrieved DataMethodImageNet-1K\n",
      "Dataset Size Zero-Shot\n",
      "B/32WIT-400M– – CLIP 63.2\n",
      "L-400M 10M REACT 68.6 (+5.4)\n",
      "LAION-400M– – OpenCLIP 62.9\n",
      "L-400M 10M REACT 66.4 (+3.5)\n",
      "L/14WIT-400M– – CLIP 75.3\n",
      "L-400M 6M REACT 77.6 (+2.3)\n",
      "L-400M 10M REACT 78.1 (+2.8)\n",
      "W-800My6M REACT 78.5 (+3.2)\n",
      "LAION-400M – – OpenCLIP 72.8\n",
      "LAION-2B – – OpenCLIP 75.3\n",
      "H/14 LAION-2B – – OpenCLIP 78.0\n",
      "Table 1. Comparison of zero-shot task transfer with public checkpoints from CLIP [57] and OpenCLIP [29]. By continue pretraining on only 10M retrieved data, REACT outperforms allpublic\n",
      "CLIP/OpenCLIP checkpoints, including those with much larger\n",
      "model size and trained on the much larger LAION-2B dataset.\n",
      "2301.07094\n",
      "Learning Customized Visual Models with Retrieval-Augmented Knowledge\n",
      "Haotian LiuyxKilho SonzJianwei YangzCe LiuzJianfeng GaozYong Jae Leey{Chunyuan Liz{\n",
      "yUniversity of Wisconsin–MadisonzMicrosoft\n",
      "flht,yongjaelee g@cs.wisc.edu fkilhoson,jianwyan,ce.liu,jfgao,chunyl g@microsoft.com\n",
      "https://react-vl.github.io\n",
      "Abstract\n",
      "Image-text contrastive learning models such as CLIP\n",
      "have demonstrated strong task transfer ability. The high\n",
      "generality and usability of these visual models is achieved\n",
      "via a web-scale data collection process to ensure broad concept coverage, followed by expensive pre-training to feed\n",
      "all the knowledge into model weights. Alternatively, we\n",
      "propose REACT ,REtrieval- Augmented CusTomization, a\n",
      "framework to acquire the relevant web knowledge to build\n",
      "customized visual models for target domains. We retrieve the\n",
      "most relevant image-text pairs ( \u00183% of CLIP pre-training\n",
      "data) from the web-scale database as external knowledge,\n",
      "and propose to customize the model by only training new\n",
      "modualized blocks while freezing all the original weights.\n",
      "The effectiveness of REACT is demonstrated via extensive\n",
      "experiments on classiﬁcation, retrieval, detection and segmentation tasks, including zero, few, and full-shot settings.\n",
      "Particularly, on the zero-shot classiﬁcation task, compared\n",
      "with CLIP , it achieves up to 5.4% improvement on ImageNet\n",
      "and 3.7% on the ELEVATER benchmark (20 datasets).\n",
      "1. Introduction\n",
      "It has been a fundamental research problem in computer\n",
      "vision (CV) to build a transferable visual system that can\n",
      "easily adapt to a wide range of downstream tasks. With\n",
      "remarkable advances in deep learning, a de facto solution\n",
      "to achieve this is to train deep neural networks on a large\n",
      "amount of data to pursue the so-called generic visual representations. This dates back to the standard supervised training on ImageNet [16], whose superb representation power\n",
      "is further demonstrated in BiT [35]/ViT [18] by scaling up\n",
      "the training to JFT300M [65]. Along the way, recent efforts\n",
      "have been applied to the popular image self-supervised learning [11, 24, 25] to reduce the demand for labeled data. The\n",
      "core contribution; {equal advising; xwork initiated during an\n",
      "internship at Microsoft.third approach is image-text contrastive learning trained on\n",
      "billion-scale web-crawled image-text pairs. Such models,\n",
      "like CLIP [57] and ALIGN [31], are able to achieve great\n",
      "performance on different downstream domains, without the\n",
      "need of any human labels.\n",
      "Excellent empirical performance has been achieved with\n",
      "the above three pre-training methods, by following the well\n",
      "established two-stage pre-training then adaptation pipeline:\n",
      "model pre-training from scratch on large data, then model\n",
      "adaptation directly on downstream tasks. Speciﬁcally, the\n"
     ]
    }
   ],
   "source": [
    "for record in xc['matches']:\n",
    "    key = record['id']\n",
    "    print(kv[key]['doi']+'\\n'+kv[key]['chunk'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually gives me a paper (that I wasn't aware of) about **RE**trieval-**A**ugmented **C**us**T**omization (REACT). Let me try to be more specific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212.09146\n",
      "investigates how reasoning abilities emerge in large\n",
      "language models when they are prompted with a\n",
      "few intermediate reasoning steps known as chain\n",
      "of thoughts.\n",
      "Moreoever, Flan-T5 is an instruction-ﬁnetuned\n",
      "T5 model which is shown to have strong reasoning\n",
      "abilities, outperforming the T5 model (Chung et al.,\n",
      "2022; Raffel et al., 2020). Although this ﬁnetuned\n",
      "model was not initially constructed for retrieverbased language modeling, it can be coupled with\n",
      "DPR to complete the language modeling and question answering task using the retrieved statements.\n",
      "In this paper, we study the reasoning ability of\n",
      "REALM,kNN-LM, FiD with DPR, and ATLAS\n",
      "with Contriever as retriever-based language models and Flan-T5 as a reasoning language model\n",
      "coupled with DPR as a retriever. While retrievers\n",
      "generally select statements from a huge common\n",
      "corpus in the literature, as illustrated in Figure 2,\n",
      "we accompany each query with a data-speciﬁc collection of statements since we want to have more\n",
      "control over the statements and the reasoning abilities of the models. Although many recent papers\n",
      "focus on commonsense or arithmetic reasoning, we\n",
      "evaluate the models on reasoning via entailment\n",
      "and logical reasoning (Wei et al., 2022; Lewkowycz\n",
      "et al., 2022; Mishra et al., 2022).\n",
      "3 Problem Deﬁnition\n",
      "In our retriever-augmented language model reasoning setting, we provide the model with a complete set of statements S=fs1;s2;:::;s mg\n",
      "for each sample. In some cases, only some of\n",
      "these statements are necessary to predict the answer with the others contain distracting information. For a ﬁxed number of retrieved statements\n",
      "k, the model should retrieve the set of statements\n",
      "Sr=fr1;r2;:::;r kg\u0012Swhich ﬁnds more related and necessary and solve the target task by\n",
      "reasoning over them. A visualization of the general\n",
      "task is represented in Figure 2.\n",
      "We study REALM, kNN-LM, FiD, ATLAS, and\n",
      "Flan-T5 in this paper. Based on the implementation details stated in Appendix A, most of these\n",
      "models have almost the same size as presented in\n",
      "Table 1, and hence, the model size does not play animportant role in the results.\n",
      "Language Models Question Answering Models\n",
      "model # params model # params\n",
      "REALM 110M REALM-QA 270M\n",
      "kNN-LM 250M FiD 220M\n",
      "ATLAS 250M\n",
      "Flan-T5 250M\n",
      "Table 1: The number of parameters in the studied language models. Most of the language models have almost the same size, implying that model size does not\n",
      "play an important role in our analysis on reasoning abilities.\n",
      "Language Modeling (LM). In the language\n",
      "modeling task setup, we measure the performance\n",
      "of the retriever-augmented language models with\n",
      "two metrics: 1) Predicting the desired token correctly; 2) Assigning a higher likelihood to the gold\n",
      "sentence over a similar but incorrect sentence. We\n",
      "2212.10409\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. Advances in neural information processing\n",
      "systems , 33:1877–1901.\n",
      "Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil\n",
      "Houlsby, and Wei Wang. 2018. Ask the right\n",
      "questions: Active question reformulation with reinforcement learning. In International Conference on\n",
      "Learning Representations .\n",
      "Denis Emelin, Ronan Le Bras, Jena D. Hwang,\n",
      "Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings of\n",
      "the 2021 Conference on Empirical Methods in Natural Language Processing , pages 698–718, Online\n",
      "and Punta Cana, Dominican Republic. Association\n",
      "for Computational Linguistics.\n",
      "Maxwell Forbes, Jena D Hwang, Vered Shwartz,\n",
      "Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral\n",
      "norms. In EMNLP .\n",
      "Matt Gardner, Joel Grus, Mark Neumann, Oyvind\n",
      "Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. 2018.Allennlp: A deep semantic natural language processing platform. arXiv preprint arXiv:1803.07640 .\n",
      "Pengcheng He, Xiaodong Liu, Jianfeng Gao, and\n",
      "Weizhu Chen. 2020. Deberta: Decoding-enhanced\n",
      "bert with disentangled attention. In International\n",
      "Conference on Learning Representations .\n",
      "Dan Hendrycks, Collin Burns, Steven Basart, Andrew\n",
      "Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n",
      "2021. Aligning {ai} with shared human values. In\n",
      "International Conference on Learning Representations .\n",
      "Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen,\n",
      "Thomas Scialom, Idan Szpektor, Avinatan Hassidim,\n",
      "and Yossi Matias. 2022. TRUE: Re-evaluating factual consistency evaluation. In Proceedings of the\n",
      "2022 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics:\n",
      "Human Language Technologies , pages 3905–3920,\n",
      "Seattle, United States. Association for Computational Linguistics.\n",
      "Or Honovich, Leshem Choshen, Roee Aharoni, Ella\n",
      "Neeman, Idan Szpektor, and Omri Abend. 2021.\n",
      "Q2:: Evaluating factual consistency in knowledgegrounded dialogues via question generation and\n",
      "question answering. In Proceedings of the 2021\n",
      "Conference on Empirical Methods in Natural Language Processing , pages 7856–7870.\n",
      "2301.12729\n",
      "connotation, we propose to utilize the next dialog-act (or responseact) in the response generation task. Formally, we formulate the\n",
      "problem as follows:\n",
      "Given a counseling dialogue containing utterances and their corresponding dialogue-acts as 𝑈∈{𝑢0,𝑢1,...,𝑢𝑡−1,𝑢𝑡}and𝐷𝐴∈\n",
      "{𝑑0,𝑑1,...,𝑑𝑡−1,𝑑𝑡}respectively, where 𝑡is the time step, our twofold\n",
      "jointly-learned tasks are – (a) to predict the response-act 𝑑𝑡+1(auxiliary), and (b) to generate a response 𝑢𝑡+1in the dialogue abiding\n",
      "by the predicted response-act 𝑑𝑡+1(primary).\n",
      "To this end, we propose READER , a novel response-act guided reinforced response generation model. The architecture of READER is\n",
      "presented in Figure 2. READER leans on the joint transfer-reinforcementlearning (TRL) paradigm for generating response-acts and responses.Our method of transformer reinforcement learning takes inspiration from an earlier work [ 37] . Moreover, we train the foundation\n",
      "language model with Proximal Policy Optimization (PPO) [ 23]. We\n",
      "define a vocabulary Σand the foundation language model 𝜃(in our\n",
      "case, GPT2) that defines a probability distribution over sequences\n",
      "of tokens.\n",
      "On top of the foundation language model, we place three taskcum-learning specific heads. First, the language model head (LMHead) is generalized for text generation tasks. Secondly, we introduce a response-act classification head (RAC-Head), an encoderonly model to classify response-acts. At last, we have a value head\n",
      "(V-Head) to compute the reward to send back to the foundation\n",
      "model. Next, we train the model jointly to generate responses from\n",
      "LM-Head and predict response-acts from RAC-Head simultaneously. Subsequently, V-Head computes the reward considering the\n",
      "scores of LM-Head and RAC-Head, which in turn is optimized\n",
      "via PPO. We furnish details related to each head and the reward\n",
      "computation in subsequent sections.\n",
      "RAC-Head. Dialogue-acts play an essential role in articulating dialogue flow. RAC-Head is a transformer-based encoder-only\n",
      "module on top of the foundation language model that learns to\n",
      "predict the future response-act. The head exploits the last hidden\n",
      "representations of the foundation language model. We feed the\n",
      "hidden representations to a GRU to exploit the contextual pattern\n",
      "of the dialog. In parallel, we obtain linear projections of the hidden\n",
      "representation. Next, these contextually-rich representations are\n",
      "passed through a multi-head attention module in which we treat the\n",
      "GRU representations as the query and the linear projections as the\n",
      "keyandvalue . Finally, we apply softmax to classify a response-act.\n",
      "2212.09146\n",
      "Can Retriever-Augmented Language Models Reason? The Blame Game\n",
      "Between the Retriever and the Language Model\n",
      "Parishad BehnamGhader\u0003Santiago MiretySiva Reddy\u0003\n",
      "\u0003McGill University / Mila - Quebec AI;yIntel Labs\n",
      "{parishad.behnamghader, siva.reddy}@mila.quebec\n",
      "santiago.miret@intel.com\n",
      "Abstract\n",
      "The emergence of large pretrained models has\n",
      "enabled language models to achieve superior\n",
      "performance in common NLP tasks, including\n",
      "language modeling and question answering,\n",
      "compared to previous static word representation methods. Augmenting these models with\n",
      "a retriever to retrieve the related text and documents as supporting information has shown\n",
      "promise in effectively solving NLP problems\n",
      "in a more interpretable way given that the additional knowledge is injected explicitly rather\n",
      "than being captured in the models’ parameters. In spite of the recent progress, our analysis on retriever-augmented language models\n",
      "shows that this class of language models still\n",
      "lack reasoning over the retrieved documents.\n",
      "In this paper, we study the strengths and\n",
      "weaknesses of different retriever-augmented\n",
      "language models such as REALM, kNN-LM,\n",
      "FiD, ATLAS, and Flan-T5 in reasoning over\n",
      "the selected documents in different tasks. In\n",
      "particular, we analyze the reasoning failures of\n",
      "each of these models and study how the models’ failures in reasoning are rooted in the retriever module as well as the language model.1\n",
      "1 Introduction\n",
      "Pretrained language models, such as decoderonly transformers, BERT, and T5 are being used\n",
      "in a wide range of tasks with outstanding results (Vaswani et al., 2017; Devlin et al., 2019;\n",
      "Raffel et al., 2020). Additionally, many methods\n",
      "have been proposed to improve language models,\n",
      "including augmenting the models with knowledge\n",
      "retrievers (Guu et al., 2020; Izacard and Grave,\n",
      "2021; Izacard et al., 2022b) or memory components (Zhong et al., 2022; Khandelwal et al., 2020;\n",
      "Verga et al., 2021). The primary goal of using a\n",
      "latent knowledge retriever is to allow the model\n",
      "to capture information from external knowledge\n",
      "1We release our code at https://github.com/\n",
      "McGill-NLP/retriever-lm-reasoning .\n",
      "Phobos is one of two large objects\n",
      "that orbit the planet mars. Because\n",
      "Phobos orbits mars, Phobos should\n",
      "be classified as which type of body?\n",
      "Mars is a kind of planet.  \n",
      "Phobos orbits mars.  \n",
      "Moons orbit planets.FiD moon\n",
      "kNN-LM orbits mars.  \n",
      "Flan-T5 a moon\n",
      "ATLAS Moons orbit planets.REALM Phobos\n",
      "In New Y ork State, the shortest\n",
      "period of daylight occurs during\n",
      "which month?\n",
      "December is during the winter in\n",
      "the northern hemisphere.  \n",
      "New york state is a state located in\n",
      "the united states of america.  \n",
      "Winter has the least sunlight.  \n",
      "United states is located in the\n",
      "northern hemisphere.  FiD winter\n",
      "2212.09736\n",
      "framework, Pangu, where a symbolic agent interacts\n",
      "with the target environment to propose candidate plans,\n",
      "and a neural LM evaluates the plausibility of each plan\n",
      "based on the input utterance. The agent searches in the\n",
      "environment to incrementally construct the plans, and\n",
      "the LM guides the search process.\n",
      "serves as a universal device, powered by LMs, for\n",
      "automated problem solving and interacting with\n",
      "the (computing) world.\n",
      "However, a key missing piece in realizing this\n",
      "future is the connection between LMs and realworld environments, including both digital environments ( e.g., databases, knowledge bases, Excel\n",
      "spreadsheets, software, websites, among others)\n",
      "and physical environments ( e.g., instruction following robots in household settings (Shridhar et al.,\n",
      "2020)). Such environments are where many real\n",
      "problems lie. For example, a biologist may need\n",
      "to ﬁnd all the species of a certain butterﬂy genus\n",
      "and their geographic distribution from a biology\n",
      "knowledge base, a local grocery store owner may\n",
      "want to visualize the historical sales of different\n",
      "item categories in Excel to decide what and how\n",
      "much to restock before the holiday season, and\n",
      "a physician may need to ﬁnd patients in a large\n",
      "database of electronic medical records who exhibited a rare combination of symptoms to inform the\n",
      "current diagnosis. How can LMs enable solving all\n",
      "these problems, which involve seeking information\n",
      "or taking actions in a speciﬁc environment, witharXiv:2212.09736v1  [cs.CL]  19 Dec 2022\n",
      "natural language?\n",
      "Each environment is a unique context for interpreting natural language requests from users.\n",
      "Grounding ,i.e., linking of (natural language) concepts to contexts (Chandu et al., 2021), therefore\n",
      "becomes the fundamental problem. More precisely,\n",
      "we need to produce a plan that can be executed in\n",
      "an environment to achieve the desired effects of\n",
      "the corresponding language request. When a plan\n",
      "is described in a formal language ( e.g., SQL for\n",
      "relational databases (Yu et al., 2018) or APIs for\n",
      "web services (Su et al., 2017; Andreas et al., 2020)),\n",
      "it is also called a program . The unique challenge\n",
      "of such grounded language understanding problems stems from 1) the vast heterogeneity of environments and their planning languages ( e.g., SQL,\n",
      "GraphQL/REST APIs, \u0015-calculus, and robot planning languages), and 2) the vast, oftentimes inﬁnite,\n",
      "number of possible instantiations (or states) of each\n",
      "environment. Some environments can also be dynamic ( e.g., a database that is constantly updated\n",
      "or a physical environment with moving objects).\n",
      "Most existing methods for grounded language\n",
      "understanding follow the popular sequence-tosequence framework (Sutskever et al., 2014; Cho\n",
      "et al., 2014) and generate the plans/programs in\n",
      "a left-to-right autoregressive fashion (Wang et al.,\n",
      "2021; Xie et al., 2022; Ye et al., 2022; Liu et al.,\n",
      "2022a). A core thesis of this paper is that directly\n",
      "generating plans may not be the optimal way of\n",
      "using LMs for grounded language understanding .\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the react framework for reasoning and acting in language models?\"\n",
    "\n",
    "xq = embed([query])[0]\n",
    "xc = index.query(xq, top_k=5)\n",
    "\n",
    "for record in xc['matches']:\n",
    "    key = record['id']\n",
    "    print(kv[key]['doi']+'\\n'+kv[key]['chunk'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I realize that this paper isn't even in the dataset because it's from Oct 2022, oops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212.10403\n",
      "Towards Reasoning in Large Language Models: A Survey\n",
      "Jie Huang Kevin Chen-Chuan Chang\n",
      "Department of Computer Science, University of Illinois at Urbana-Champaign\n",
      "{jeffhj, kcchang}@illinois.edu\n",
      "Abstract\n",
      "Reasoning is a fundamental aspect of human\n",
      "intelligence that plays a crucial role in activities such as problem solving, decision making,\n",
      "and critical thinking. In recent years, large language models (LLMs) have made signiﬁcant\n",
      "progress in natural language processing, and\n",
      "there is observation that these models may exhibit reasoning abilities when they are sufﬁciently large. However, it is not yet clear to\n",
      "what extent LLMs are capable of reasoning.\n",
      "This paper provides a comprehensive overview\n",
      "of the current state of knowledge on reasoning\n",
      "in LLMs, including techniques for improving\n",
      "and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning\n",
      "abilities, ﬁndings and implications of previous\n",
      "research in this ﬁeld, and suggestions on future\n",
      "directions. Our aim is to provide a detailed and\n",
      "up-to-date review of this topic and stimulate\n",
      "meaningful discussion and future work.1\n",
      "1 Introduction\n",
      "Reasoning is a cognitive process that involves using\n",
      "evidence, arguments, and logic to arrive at conclusions or make judgments. It plays a central role in\n",
      "many intellectual activities, such as problem solving, decision making, and critical thinking. The\n",
      "study of reasoning is important in ﬁelds like psychology (Wason and Johnson-Laird, 1972), philosophy (Passmore, 1961), and computer science (Huth\n",
      "and Ryan, 2004), as it helps individuals make decisions, solve problems, and think critically.\n",
      "Recently, large language models (LLMs)\n",
      "(Brown et al., 2020; Chowdhery et al., 2022; Chung\n",
      "et al., 2022; Shoeybi et al., 2019, inter alia ) have\n",
      "made signiﬁcant advancements in natural language\n",
      "processing and related ﬁelds. It has been shown\n",
      "that these models exhibit emergent behaviors, including the ability to “reason”, when they are large\n",
      "1Paperlist is updated at https://github.com/jeffhj/\n",
      "LM-reasoning .enough (Wei et al., 2022a). For example, by providing the models with “ chain of thoughts ”, i.e.,\n",
      "reasoning exemplars, or a simple prompt “ Let’s\n",
      "think step by step ”, these models are able to answer\n",
      "questions with explicit reasoning steps (Wei et al.,\n",
      "2022b; Kojima et al., 2022), e.g., “all whales are\n",
      "mammals, all mammals have kidneys; therefore,\n",
      "all whales have kidneys.” This has sparked considerable interest in the community since reasoning\n",
      "ability is a hallmark of human intelligence that is\n",
      "frequently considered missed in current artiﬁcial\n",
      "intelligence systems (Marcus, 2020; Russin et al.,\n",
      "2020; Mitchell, 2021; Bommasani et al., 2021).\n",
      "However, despite the strong performance of\n",
      "LLMs on certain reasoning tasks, it remains unclear\n",
      "2212.10409\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. Advances in neural information processing\n",
      "systems , 33:1877–1901.\n",
      "Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil\n",
      "Houlsby, and Wei Wang. 2018. Ask the right\n",
      "questions: Active question reformulation with reinforcement learning. In International Conference on\n",
      "Learning Representations .\n",
      "Denis Emelin, Ronan Le Bras, Jena D. Hwang,\n",
      "Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings of\n",
      "the 2021 Conference on Empirical Methods in Natural Language Processing , pages 698–718, Online\n",
      "and Punta Cana, Dominican Republic. Association\n",
      "for Computational Linguistics.\n",
      "Maxwell Forbes, Jena D Hwang, Vered Shwartz,\n",
      "Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral\n",
      "norms. In EMNLP .\n",
      "Matt Gardner, Joel Grus, Mark Neumann, Oyvind\n",
      "Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. 2018.Allennlp: A deep semantic natural language processing platform. arXiv preprint arXiv:1803.07640 .\n",
      "Pengcheng He, Xiaodong Liu, Jianfeng Gao, and\n",
      "Weizhu Chen. 2020. Deberta: Decoding-enhanced\n",
      "bert with disentangled attention. In International\n",
      "Conference on Learning Representations .\n",
      "Dan Hendrycks, Collin Burns, Steven Basart, Andrew\n",
      "Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n",
      "2021. Aligning {ai} with shared human values. In\n",
      "International Conference on Learning Representations .\n",
      "Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen,\n",
      "Thomas Scialom, Idan Szpektor, Avinatan Hassidim,\n",
      "and Yossi Matias. 2022. TRUE: Re-evaluating factual consistency evaluation. In Proceedings of the\n",
      "2022 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics:\n",
      "Human Language Technologies , pages 3905–3920,\n",
      "Seattle, United States. Association for Computational Linguistics.\n",
      "Or Honovich, Leshem Choshen, Roee Aharoni, Ella\n",
      "Neeman, Idan Szpektor, and Omri Abend. 2021.\n",
      "Q2:: Evaluating factual consistency in knowledgegrounded dialogues via question generation and\n",
      "question answering. In Proceedings of the 2021\n",
      "Conference on Empirical Methods in Natural Language Processing , pages 7856–7870.\n",
      "2302.00618\n",
      "reasoning in language models. CoRR , abs/2203.11171,\n",
      "2022a. doi: 10.48550/arXiv.2203.11171. URL https:\n",
      "//doi.org/10.48550/arXiv.2203.11171 .\n",
      "Wang, Y ., Mishra, S., Alipoormolabashi, P., Kordi, Y .,\n",
      "Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran,\n",
      "A. S., Naik, A., Stap, D., Pathak, E., Karamanolakis, G.,\n",
      "Lai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuznia,\n",
      "K., Doshi, K., Patel, M., Pal, K. K., Moradshahi, M., Parmar, M., Purohit, M., Varshney, N., Kaza, P. R., Verma,\n",
      "P., Puri, R. S., Karia, R., Sampat, S. K., Doshi, S., Mishra,\n",
      "S., Reddy, S., Patro, S., Dixit, T., Shen, X., Baral, C.,\n",
      "Choi, Y ., Smith, N. A., Hajishirzi, H., and Khashabi, D.\n",
      "Super-naturalinstructions: Generalization via declarative\n",
      "instructions on 1600+ nlp tasks, 2022b.\n",
      "Wei, J., Bosma, M., Zhao, V . Y ., Guu, K., Yu, A. W., Lester,\n",
      "B., Du, N., Dai, A. M., and Le, Q. V . Finetuned language\n",
      "models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR\n",
      "2022, Virtual Event, April 25-29, 2022 . OpenReview.net,\n",
      "2022a. URL https://openreview.net/forum?\n",
      "id=gEZrGCozdqR .Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi,\n",
      "E. H., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. CoRR ,\n",
      "abs/2201.11903, 2022b. URL https://arxiv.org/\n",
      "abs/2201.11903 .\n",
      "West, P., Bhagavatula, C., Hessel, J., Hwang, J. D., Jiang, L.,\n",
      "Bras, R. L., Lu, X., Welleck, S., and Choi, Y . Symbolic\n",
      "knowledge distillation: from general language models\n",
      "to commonsense models. In Carpuat, M., de Marneffe,\n",
      "M., and Ru ´ız, I. V . M. (eds.), Proceedings of the 2022\n",
      "2212.10403\n",
      "Prajjwal Bhargava and Vincent Ng. 2022. Commonsense knowledge reasoning and generation with pretrained language models: A survey. Proceedings of\n",
      "the AAAI Conference on Artiﬁcial Intelligence .\n",
      "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\n",
      "Altman, Simran Arora, Sydney von Arx, Michael S\n",
      "Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\n",
      "Brunskill, et al. 2021. On the opportunities\n",
      "and risks of foundation models. ArXiv preprint ,\n",
      "abs/2108.07258.\n",
      "Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, and Martin Goedhart. 2020. Logical reasoning in formal\n",
      "and everyday reasoning tasks. International Journal\n",
      "of Science and Mathematics Education , 18(8):1673–\n",
      "1694.\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
      "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
      "Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n",
      "Clemens Winter, Christopher Hesse, Mark Chen,\n",
      "Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n",
      "Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\n",
      "Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing\n",
      "Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n",
      "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\n",
      "Henrique Ponde de Oliveira Pinto, Jared Kaplan,\n",
      "Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\n",
      "Brockman, et al. 2021. Evaluating large language models trained on code. ArXiv preprint ,\n",
      "abs/2107.03374.\n",
      "Wenhu Chen. 2022. Large language models are\n",
      "few (1)-shot table reasoners. ArXiv preprint ,\n",
      "abs/2210.06710.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts,\n",
      "Paul Barham, Hyung Won Chung, Charles Sutton,\n",
      "Sebastian Gehrmann, et al. 2022. Palm: Scaling\n",
      "language modeling with pathways. ArXiv preprint ,\n",
      "abs/2204.02311.\n",
      "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\n",
      "Wang, Mostafa Dehghani, Siddhartha Brahma, et al.\n",
      "2022. Scaling instruction-ﬁnetuned language models.ArXiv preprint , abs/2210.11416.\n",
      "2301.06627\n",
      "models to systematically reason over implicit knowledge. Advances in Neural Information Processing Systems , 33:\n",
      "20227–20237, 2020b.\n",
      "M. K. Tanenhaus, M. J. Spivey-Knowlton, K. M. Eberhard, and J. C. Sedivy. Integration of visual and linguistic\n",
      "information in spoken language comprehension. Science , 268(5217):1632, 1995.\n",
      "Leyla Tarhan and Talia Konkle. Sociality and interaction envelope organize visual action representations. Nature\n",
      "Communications , 11(1):3002, June 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-16846-w. URL https:\n",
      "//www.nature.com/articles/s41467-020-16846-w . Number: 1 Publisher: Nature Publishing Group.\n",
      "Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Computational Linguistics , pages 4593–4601, Florence, Italy, July\n",
      "2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452. URL https://aclanthology.org/\n",
      "P19-1452 .\n",
      "Michael Tomasello. Origins of human communication . MIT press, 2010.\n",
      "41\n",
      "APREPRINT - JANUARY 18, 2023\n",
      "John C. Trueswell and Michael K. Tanenhaus. Toward a lexicalist framework of constraint-based syntactic ambiguity\n",
      "resolution. Lawrence Erlbaum Associates, Inc, 1994.\n",
      "John C. Trueswell, Michael K. Tanenhaus, and Christopher Kello. Verb-speciﬁc constraints in sentence processing:\n",
      "separating effects of lexical preference from garden-paths. Journal of Experimental psychology: Learning, memory,\n",
      "and Cognition , 19(3):528, 1993. Publisher: American Psychological Association.\n",
      "Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A Persson,\n",
      "Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials science\n",
      "literature. Nature , 571(7763):95–98, 2019.\n",
      "Mycal Tucker, Tiwalayo Eisape, Peng Qian, Roger Levy, and Julie Shah. When does syntax mediate neural language\n",
      "model performance? evidence from dropout probes. In Proceedings of the 2022 Conference of the North American\n",
      "Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5393–5408, Seattle,\n",
      "United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.394. URL\n",
      "https://aclanthology.org/2022.naacl-main.394 .\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the the latest research on reasoning and acting in language models?\"\n",
    "\n",
    "xq = embed([query])[0]\n",
    "xc = index.query(xq, top_k=5)\n",
    "\n",
    "for record in xc['matches']:\n",
    "    key = record['id']\n",
    "    print(kv[key]['doi']+'\\n'+kv[key]['chunk'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks interesting. Let's try feeding it into a langchain completion endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first extract retrieved contexts\n",
    "contexts = [\n",
    "    {\"context\": kv[record['id']]['chunk']} for record in xc['matches']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name='text-davinci-003',\n",
    "    openai_api_key=OPENAI_KEY\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "prompt_template = \"\"\"You are a influential educator in the space of machine learning\n",
    "and artificial intelligence. You are known for providing easy to understand explanations\n",
    "to complex concepts in these fields. Given the information contained in the following\n",
    "contexts answer the question below. If the question cannot be answered using the\n",
    "information in the contexts, answer \"I don't know\".\n",
    "\n",
    "Contexts: {contexts}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"contexts\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_str = \"\\n\\n\".join([c['context'] for c in contexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The latest research on reasoning and acting in language models includes techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Additionally, there have been studies on unsupervised word embeddings, syntactic ambiguity resolution, and the evaluation of large language models trained on code.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.run(question=query, contexts=contexts_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a good overview, let's wrap it up into a `arxiv_bot` function so I can ask more questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_bot(query: str):\n",
    "    xq = embed([query])[0]\n",
    "    xc = index.query(xq, top_k=5)\n",
    "    contexts = [\n",
    "        {\"context\": kv[record['id']]['chunk']} for record in xc['matches']\n",
    "    ]\n",
    "    dois = [kv[record['id']]['doi'] for record in xc['matches']]\n",
    "    contexts_str = \"\\n\\n\".join([c['context'] for c in contexts])\n",
    "    print(llm_chain.run(question=query, contexts=contexts_str))\n",
    "    print(dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Emergent behavior\n",
      "['2301.12726', '2301.12726', '2212.10403', '2212.10403', '2212.10071']\n"
     ]
    }
   ],
   "source": [
    "arxiv_bot(\n",
    "    \"what is the term that describes how large language models seem \"+\n",
    "    \"to exhibit reasoning abilities when they get to a certain size?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Emergent abilities of language models is the concept that larger models are more proficient at meta-learning than smaller models and can acquire abilities that are not present in smaller models. This includes tasks such as few-shot prompting, transliteration from the International Phonetic Alphabet, recovering a word from its scrambled letters and Persian question-answering. Furthermore, larger language models can be trained with more data which can potentially lead to miscorrelation between different modalities. To mitigate the risks associated with emergent abilities, researchers are urged to develop up-to-date benchmarks to measure unforeseen behaviors in large language models.\n",
      "['2301.12867', '2212.10755', '2301.06627', '2302.00763', '2301.10095']\n"
     ]
    }
   ],
   "source": [
    "arxiv_bot(\"Tell me about the idea behind 'emergent abilities' in LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chain of thoughts are a technique used to enable complex reasoning and generate explanations with LLMs by forcing models to explicitly verbalize reasoning steps as natural language. This method has improved performance on a variety of tasks and sparked the active development of further refinements.\n",
      "['2212.10403', '2301.11596', '2301.11596', '2301.00303', '2301.13379']\n"
     ]
    }
   ],
   "source": [
    "arxiv_bot(\"what are 'chain of thoughts' in LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zebras are stripey because the stripes are thought to act as a form of camouflage, helping them to blend in with their environment and making it harder for predators to spot them. The stripes also act as a form of social identification, with each zebra having its own unique stripe pattern.\n",
      "['2301.10799', '2301.03559', '2301.08721', '2301.03559', '2301.03559']\n"
     ]
    }
   ],
   "source": [
    "arxiv_bot(\"can you tell me why zebras are stripey?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally I'd rather the model not answer the question if it can't source info."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
